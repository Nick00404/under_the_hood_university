{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b330553",
   "metadata": {},
   "source": [
    "Let's dive into the **heart of BERT** â€” where words hide in masks and context reveals everything.  \n",
    "Time to build a **Masked Language Model (MLM)** from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§ª `08_lab_masked_language_modeling_from_scratch.ipynb`  \n",
    "### ğŸ“ `03_natural_language_processing`  \n",
    "> Pretrain a **mini BERT-style model** on a small text corpus using **masked token prediction**.  \n",
    "No black-box â€” youâ€™ll learn **what BERT learns**, and **how**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Goals\n",
    "\n",
    "- Understand **Masked Language Modeling (MLM)** as used in BERT  \n",
    "- Train a **tiny Transformer encoder** on a small dataset  \n",
    "- Implement training loop, masking logic, loss function  \n",
    "- Visualize token recovery over time\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Runtime Design\n",
    "\n",
    "| Feature         | Spec              |\n",
    "|-----------------|-------------------|\n",
    "| Target device   | âœ… Colab / Laptop (CPU or GPU)  \n",
    "| Dataset size    | âœ… <10k lines (custom text or dataset)  \n",
    "| Model size      | âœ… ~100k params (tiny encoder)  \n",
    "| Epochs          | ğŸ” 5â€“10 fast epochs  \n",
    "| Libraries       | âœ… PyTorch, HuggingFace Tokenizers  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Section 1: Install & Imports\n",
    "\n",
    "```python\n",
    "!pip install transformers datasets\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“„ Section 2: Create Sample Dataset\n",
    "\n",
    "```python\n",
    "# Toy corpus (repeat to expand)\n",
    "text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"All your base are belong to us\",\n",
    "    \"The cake is a lie\",\n",
    "    \"To be or not to be\",\n",
    "    \"I am the one who knocks\"\n",
    "] * 200  # expand for training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Section 3: Tokenizer + Masking\n",
    "\n",
    "```python\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    # Sample tokens to mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "        for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # only compute loss on masked tokens\n",
    "\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Section 4: Dataset + Dataloader\n",
    "\n",
    "```python\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=32):\n",
    "        self.examples = tokenizer(texts, truncation=True, padding='max_length',\n",
    "                                  max_length=max_length, return_tensors='pt')['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.examples[idx].clone()\n",
    "        masked, labels = mask_tokens(input_ids, tokenizer)\n",
    "        return masked, labels\n",
    "\n",
    "dataset = MLMDataset(text_data, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Section 5: Define a Mini BERT-style Encoder\n",
    "\n",
    "```python\n",
    "class MiniBert(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, 64)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=64, nhead=4),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.out = nn.Linear(64, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)  # (B, T, D)\n",
    "        x = self.encoder(x.permute(1, 0, 2))  # (T, B, D)\n",
    "        x = self.out(x.permute(1, 0, 2))  # back to (B, T, V)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Section 6: Train Loop (MLM)\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MiniBert(tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Section 7: Try Masked Prediction\n",
    "\n",
    "```python\n",
    "def predict_masked(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', max_length=32, truncation=True, padding='max_length')\n",
    "    input_ids = tokens['input_ids'].clone()\n",
    "    mask_pos = 5  # for demo\n",
    "    input_ids[0, mask_pos] = tokenizer.mask_token_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids.to(device))\n",
    "    top_pred = output[0, mask_pos].argmax().item()\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Predicted mask:\", tokenizer.decode([top_pred]))\n",
    "\n",
    "predict_masked(\"The quick brown [MASK] jumps over\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Recap\n",
    "\n",
    "| What You Did               | âœ… |\n",
    "|----------------------------|----|\n",
    "| Created a toy corpus       | âœ… |\n",
    "| Built masking logic        | âœ… |\n",
    "| Trained MLM from scratch   | âœ… |\n",
    "| Tested masked recovery     | âœ… |\n",
    "| CPU/GPU friendly           | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What You Learned\n",
    "\n",
    "- How **BERT learns by unmasking tokens**  \n",
    "- Importance of **contextual embeddings**  \n",
    "- Why MLM pretraining captures **deep semantics**  \n",
    "- How to scale this into **serious pretraining pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "Ready for `09_lab_attention_visualization.ipynb` next?  \n",
    "Weâ€™ll visualize **real attention heads** from a transformer â€” and see how models \"look\" at words."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
