{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73787bbf",
   "metadata": {},
   "source": [
    "ðŸ”¥ Weâ€™re entering **real-world territory** now. No more notebooks just for learning â€” now weâ€™re **shipping models**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `07_lab_export_pytorch_to_onnx_and_run.ipynb`  \n",
    "### ðŸ“ `06_deployment_and_scaling`  \n",
    "> Convert a trained **PyTorch model** to the **ONNX format**  \n",
    "â†’ Then run **platform-agnostic inference** using ONNX Runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Export PyTorch model to `.onnx` format  \n",
    "- Understand **what ONNX is & why it matters**  \n",
    "- Load ONNX model with **onnxruntime**  \n",
    "- Run inference and verify consistency  \n",
    "- Prepare for deployment to **Edge, Cloud, or C++ backend**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Spec\n",
    "\n",
    "| Component      | Setting         |\n",
    "|----------------|------------------|\n",
    "| Model          | Trained MLP (MNIST) âœ…  \n",
    "| Export Format  | ONNX `.onnx` âœ…  \n",
    "| Backend        | ONNX Runtime âœ…  \n",
    "| Hardware       | CPU / Colab âœ…  \n",
    "| Use Case       | Fast, cross-platform inference âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Section 1: Install ONNX Tools\n",
    "\n",
    "```bash\n",
    "!pip install onnx onnxruntime\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– Section 2: Define & Train Simple Model\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¾ Section 3: Export to ONNX\n",
    "\n",
    "```python\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "torch.onnx.export(\n",
    "    model, dummy_input, \"mlp_mnist.onnx\",\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    opset_version=11\n",
    ")\n",
    "print(\"âœ… Exported to mlp_mnist.onnx\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 4: Inference with ONNX Runtime\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession(\"mlp_mnist.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Single image\n",
    "x_test = torch.randn(1, 1, 28, 28).numpy().astype(np.float32)\n",
    "outputs = session.run(None, {input_name: x_test})\n",
    "\n",
    "print(\"ONNX Prediction:\", np.argmax(outputs[0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Section 5: Compare with PyTorch Output\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    torch_pred = model(torch.tensor(x_test)).argmax(dim=1).item()\n",
    "print(\"PyTorch Prediction:\", torch_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Summary\n",
    "\n",
    "| Task                             | âœ… |\n",
    "|----------------------------------|----|\n",
    "| Train & save PyTorch model       | âœ… |\n",
    "| Export to ONNX                   | âœ… |\n",
    "| Load & run with ONNX Runtime     | âœ… |\n",
    "| Compare PyTorch vs ONNX outputs  | âœ… |\n",
    "| Fully portable CPU inference     | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- ONNX is a **cross-platform IR** for deep learning models  \n",
    "- Once exported, your model becomes **hardware-agnostic**  \n",
    "- This lab is the **gateway to C++, TensorRT, mobile & edge**\n",
    "\n",
    "---\n",
    "\n",
    "Next mission in deployment dojo:  \n",
    "> ðŸ”¥ `08_lab_dockerize_and_test_flask_model_server.ipynb`  \n",
    "We'll serve this ONNX or PyTorch model as a **REST API** using **Flask + Docker** â€” real production skills.\n",
    "\n",
    "Shall we go?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
