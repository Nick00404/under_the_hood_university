{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“š Table of Contents\n",
                "\n",
                "- [ğŸŒ Introduction to Multilingual NLP](#introduction-to-multilingual-nlp)\n",
                "  - [ğŸŒ Why multilingual models matter](#why-multilingual-models-matter)\n",
                "  - [ğŸ”¤ Challenges in multilingual NLP](#challenges-in-multilingual-nlp)\n",
                "- [ğŸ§  XLM (Cross-lingual Language Model)](#xlm-cross-lingual-language-model)\n",
                "  - [ğŸŒ XLMâ€™s approach to multilingual representation](#xlms-approach-to-multilingual-representation)\n",
                "  - [ğŸŒ Using XLM for translation and classification](#using-xlm-for-translation-and-classification)\n",
                "  - [ğŸ§ª Example: Fine-tuning XLM](#example-fine-tuning-xlm)\n",
                "- [ğŸ“š RoBERTa and mT5 for Multilingual Tasks](#roberta-and-mt5-for-multilingual-tasks)\n",
                "  - [ğŸš€ RoBERTaâ€™s improvements over BERT](#robertas-improvements-over-bert)\n",
                "  - [ğŸ“¤ Using mT5 for multilingual generation and translation](#using-mt5-for-multilingual-generation-and-translation)\n",
                "  - [ğŸ§ª Example: Fine-tuning mT5](#example-fine-tuning-mt5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Multilingual NLP Overview**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Challenges[\"Key Challenges\"]\n",
                "        direction LR\n",
                "        A[Tokenization] --> B[Language Diversity]\n",
                "        B --> C[Data Scarcity]\n",
                "        C --> D[Script Variations]\n",
                "    end\n",
                "    \n",
                "    subgraph Importance[\"Why Multilingual?\"]\n",
                "        direction LR\n",
                "        E[ğŸŒ Global Apps] --> F[ğŸ”„ Cross-Lingual Transfer]\n",
                "        F --> G[ğŸ“ˆ Low-Resource Languages]\n",
                "    end\n",
                "    \n",
                "    Challenges -->|Solve With| Models[Multilingual Models]\n",
                "    Importance --> Models\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. XLM Architecture**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph XLM[\"XLM Workflow\"]\n",
                "        direction TB\n",
                "        LangA[English Text] --> SharedEmb[Shared Embeddings]\n",
                "        LangB[ä¸­æ–‡æ–‡æœ¬] --> SharedEmb\n",
                "        LangC[Texto espaÃ±ol] --> SharedEmb\n",
                "        SharedEmb --> Transformer[Transformer Encoder]\n",
                "        Transformer --> Task[Cross-lingual Tasks]\n",
                "    end\n",
                "    \n",
                "    Task --> CLS[Classification]\n",
                "    Task --> MT[Translation]\n",
                "    Task --> QA[Question Answering]\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. RoBERTa Improvements**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    BERT -->|Enhancements| RoBERTa\n",
                "    subgraph RoBERTa[\"RoBERTa Optimizations\"]\n",
                "        direction TB\n",
                "        A[Larger Dataset] --> B[Longer Training]\n",
                "        B --> C[No NSP Objective]\n",
                "        C --> D[Dynamic Masking]\n",
                "    end\n",
                "    \n",
                "    RoBERTa --> Apps[Better Multilingual Performance]\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **4. mT5 for Multilingual Generation**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph mT5[\"mT5 Architecture\"]\n",
                "        direction LR\n",
                "        Input[\"Multilingual Input\"] --> Encoder\n",
                "        Encoder --> Decoder\n",
                "        Decoder --> Output[\"Multilingual Output\"]\n",
                "    end\n",
                "    \n",
                "    Input -.->|English: \"Hello\"| Output1[\"French: \"Bonjour\"\"]\n",
                "    Input -.->|ä¸­æ–‡: \"ä½ å¥½\"| Output2[\"Spanish: \"Hola\"\"]\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Fine-tuning Workflow**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    Model[Pretrained Model] --> FT[Fine-tuning]\n",
                "    FT -->|Multilingual Dataset| Train[Training]\n",
                "    Train --> Eval[Evaluation]\n",
                "    \n",
                "    subgraph Dataset[\"Example Data\"]\n",
                "        direction LR\n",
                "        D1[\"English: Positive\"] --> Mix\n",
                "        D2[\"ä¸­æ–‡: æ­£é¢\"] --> Mix\n",
                "        D3[\"EspaÃ±ol: Positivo\"] --> Mix\n",
                "    end\n",
                "    \n",
                "    Dataset --> FT\n",
                "```\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"introduction-to-multilingual-nlp\"></a>ğŸŒ Introduction to Multilingual NLP\n",
                "\n",
                "# <a id=\"why-multilingual-models-matter\"></a>ğŸŒ Why multilingual models matter\n",
                "\n",
                "# <a id=\"challenges-in-multilingual-nlp\"></a>ğŸ”¤ Challenges in multilingual NLP\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"xlm-cross-lingual-language-model\"></a>ğŸ§  XLM (Cross-lingual Language Model)\n",
                "\n",
                "# <a id=\"xlms-approach-to-multilingual-representation\"></a>ğŸŒ XLMâ€™s approach to multilingual representation\n",
                "\n",
                "# <a id=\"using-xlm-for-translation-and-classification\"></a>ğŸŒ Using XLM for translation and classification\n",
                "\n",
                "# <a id=\"example-fine-tuning-xlm\"></a>ğŸ§ª Example: Fine-tuning XLM\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"roberta-and-mt5-for-multilingual-tasks\"></a>ğŸ“š RoBERTa and mT5 for Multilingual Tasks\n",
                "\n",
                "# <a id=\"robertas-improvements-over-bert\"></a>ğŸš€ RoBERTaâ€™s improvements over BERT\n",
                "\n",
                "# <a id=\"using-mt5-for-multilingual-generation-and-translation\"></a>ğŸ“¤ Using mT5 for multilingual generation and translation\n",
                "\n",
                "# <a id=\"example-fine-tuning-mt5\"></a>ğŸ§ª Example: Fine-tuning mT5\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
