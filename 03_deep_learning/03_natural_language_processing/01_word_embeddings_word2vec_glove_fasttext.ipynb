{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“š Table of Contents\n",
                "\n",
                "- [ğŸ§¾ Introduction to Word Embeddings](#introduction-to-word-embeddings)\n",
                "  - [â“ What are word embeddings, and why are they used?](#what-are-word-embeddings-and-why-are-they-used)\n",
                "  - [ğŸ§  How do word embeddings capture semantic meaning in text?](#how-do-word-embeddings-capture-semantic-meaning-in-text)\n",
                "- [ğŸ”¤ Word2Vec](#word2vec)\n",
                "  - [ğŸ“š Overview of Word2Vec: CBOW vs. Skip-Gram](#overview-of-word2vec-cbow-vs-skip-gram)\n",
                "  - [ğŸ› ï¸ Training Word2Vec on text data using Gensim](#training-word2vec-on-text-data-using-gensim)\n",
                "  - [ğŸ§­ Example: Visualizing word embeddings with t-SNE](#example-visualizing-word-embeddings-with-t-sne)\n",
                "- [ğŸŒ GloVe (Global Vectors for Word Representation)](#glove-global-vectors-for-word-representation)\n",
                "  - [ğŸ” Difference between Word2Vec and GloVe](#difference-between-word2vec-and-glove)\n",
                "  - [ğŸ§® GloVeâ€™s matrix factorization approach](#gloves-matrix-factorization-approach)\n",
                "  - [ğŸ“¦ Using pre-trained GloVe embeddings in NLP tasks](#using-pre-trained-glove-embeddings-in-nlp-tasks)\n",
                "- [âš¡ FastText](#fasttext)\n",
                "  - [ğŸ”¡ FastTextâ€™s approach to representing words as subword units](#fasttexts-approach-to-representing-words-as-subword-units)\n",
                "  - [ğŸ§³ Handling out-of-vocabulary words with FastText](#handling-out-of-vocabulary-words-with-fasttext)\n",
                "  - [ğŸ§ª Example: Training and using FastText for word vector representation](#example-training-and-using-fasttext-for-word-vector-representation)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âœ… **1. Word2Vec Implementation (Hybrid with Code Tooltip)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph Word2Vec_Models[\"Word2Vec Implementation\"]\n",
                "        direction TB\n",
                "        CBOW[[CBOW Architecture<br/>Predict center word from context]]:::blue\n",
                "        SkipGram[[Skip-Gram<br/>Predict context from center]]:::orange\n",
                "        CBOW --> Visualize1[t-SNE Projection]\n",
                "        SkipGram --> Visualize2[t-SNE Projection]\n",
                "    end\n",
                "\n",
                "    RawText[Raw Text Corpus] --> Preprocess[Tokenization & Cleaning]\n",
                "    Preprocess --> CBOW\n",
                "    Preprocess --> SkipGram\n",
                "\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef orange fill:#ffe6cc,stroke:#ff6600\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>ğŸ§ª Python Code (CBOW / Skip-Gram)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.models import Word2Vec\n",
                "model_cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW\n",
                "model_sg   = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)  # Skip-Gram\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## âœ… **2. Comparison Matrix (with BERT, Emoji & Context Column)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Comparison[\"Embedding Model Comparison\"]\n",
                "        direction LR\n",
                "        header1[Model] --> header2[OOV] --> header3[Subwords] --> header4[Context-Aware] --> header5[Speed]\n",
                "        row1[Word2Vec] --> cell1[âŒ] --> cell2[âŒ] --> cell3[âŒ] --> cell4[ğŸï¸ Fast]\n",
                "        row2[GloVe] --> cell5[âŒ] --> cell6[âŒ] --> cell7[âŒ] --> cell8[ğŸï¸ Fast]\n",
                "        row3[FastText] --> cell9[âœ…] --> cell10[âœ…] --> cell11[âŒ] --> cell12[ğŸš— Medium]\n",
                "        row4[BERT] --> cell13[âœ…] --> cell14[âœ…] --> cell15[âœ…] --> cell16[ğŸ¢ Slow]\n",
                "    end\n",
                "    classDef header fill:#e6f3ff,stroke:#0066cc\n",
                "    class header1,header2,header3,header4,header5 header\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## âœ… **3. FastText OOV Handling (with Linguistic Examples)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    A[FastText<br/>OOV Recovery in Morphological Languages]:::green\n",
                "    A --> B[\"ğŸ‡¹ğŸ‡· Turkish: 'evlerinizden' â†’ ev + ler + iniz + den\"]\n",
                "    A --> C[\"ğŸ‡«ğŸ‡® Finnish: 'taloissani' â†’ talo + issa + ni\"]\n",
                "    A --> D[\"ğŸ‡°ğŸ‡· Korean: 'ê°€ë°©ì—' â†’ ê°€ë°© + ë°©ì—\"]\n",
                "    A --> E[\"Subwords averaged â†’ Word vector\"]\n",
                "\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    class B,C,D,E green\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>ğŸ§ª Python Code (FastText)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.models import FastText\n",
                "model = FastText(sentences, vector_size=100, window=5, min_count=1)\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## âœ… **4. GloVe Mechanics (Cleaned + Objective Function)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    A[Corpus] --> B[Build Co-occurrence Matrix]\n",
                "    B --> C[Factorize Matrix]\n",
                "    C --> D[Word Vectors]\n",
                "    D --> E[Pretrained Vectors] --> F[Fine-tune for Downstream Task]\n",
                "    C -.->|Loss Function| M[[\"J = Î£ f(X_ij)(wáµ¢áµ€wÌƒâ±¼ + báµ¢ + bÌƒâ±¼ âˆ’ log X_ij)Â²\"]]:::math\n",
                "\n",
                "    classDef math fill:#f0e6ff,stroke:#6600cc\n",
                "    class M math\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>ğŸ§ª Load Pretrained GloVe (Gensim)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.scripts.glove2word2vec import glove2word2vec\n",
                "glove2word2vec(\"glove.txt\", \"glove.word2vec.txt\")\n",
                "from gensim.models import KeyedVectors\n",
                "model = KeyedVectors.load_word2vec_format(\"glove.word2vec.txt\")\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## âœ… **5. BERT Contextual Embedding (New Bonus Diagram)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    A[Input Sentence] --> B[WordPiece Tokenization]\n",
                "    B --> C[Transformer Layers]\n",
                "    C --> D[Contextual Embeddings]\n",
                "    D --> E[Fine-tune: NER / QA / Sentiment]\n",
                "\n",
                "    classDef purple fill:#f0e6ff,stroke:#6600cc\n",
                "    class B,C,D,E purple\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>ğŸ§ª Python Code (Transformers â€“ BERT Embeddings)</summary>\n",
                "\n",
                "```python\n",
                "from transformers import BertTokenizer, BertModel\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "model = BertModel.from_pretrained('bert-base-uncased')\n",
                "\n",
                "inputs = tokenizer(\"Paris is the capital of France\", return_tensors=\"pt\")\n",
                "outputs = model(**inputs)\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"introduction-to-word-embeddings\"></a>ğŸ§¾ Introduction to Word Embeddings\n",
                "\n",
                "## <a id=\"what-are-word-embeddings-and-why-are-they-used\"></a>â“ What are word embeddings, and why are they used?\n",
                "\n",
                "## <a id=\"how-do-word-embeddings-capture-semantic-meaning-in-text\"></a>ğŸ§  How do word embeddings capture semantic meaning in text?\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"word2vec\"></a>ğŸ”¤ Word2Vec\n",
                "\n",
                "## <a id=\"overview-of-word2vec-cbow-vs-skip-gram\"></a>ğŸ“š Overview of Word2Vec: CBOW vs. Skip-Gram\n",
                "\n",
                "## <a id=\"training-word2vec-on-text-data-using-gensim\"></a>ğŸ› ï¸ Training Word2Vec on text data using Gensim\n",
                "\n",
                "## <a id=\"example-visualizing-word-embeddings-with-t-sne\"></a>ğŸ§­ Example: Visualizing word embeddings with t-SNE\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"glove-global-vectors-for-word-representation\"></a>ğŸŒ GloVe (Global Vectors for Word Representation)\n",
                "\n",
                "## <a id=\"difference-between-word2vec-and-glove\"></a>ğŸ” Difference between Word2Vec and GloVe\n",
                "\n",
                "## <a id=\"gloves-matrix-factorization-approach\"></a>ğŸ§® GloVeâ€™s matrix factorization approach\n",
                "\n",
                "## <a id=\"using-pre-trained-glove-embeddings-in-nlp-tasks\"></a>ğŸ“¦ Using pre-trained GloVe embeddings in NLP tasks\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"fasttext\"></a>âš¡ FastText\n",
                "\n",
                "## <a id=\"fasttexts-approach-to-representing-words-as-subword-units\"></a>ğŸ”¡ FastTextâ€™s approach to representing words as subword units\n",
                "\n",
                "## <a id=\"handling-out-of-vocabulary-words-with-fasttext\"></a>ğŸ§³ Handling out-of-vocabulary words with FastText\n",
                "\n",
                "## <a id=\"example-training-and-using-fasttext-for-word-vector-representation\"></a>ğŸ§ª Example: Training and using FastText for word vector representation\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
