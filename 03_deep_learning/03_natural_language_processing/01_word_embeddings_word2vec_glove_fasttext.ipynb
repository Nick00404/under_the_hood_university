{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Table of Contents\n",
                "\n",
                "- [üßæ Introduction to Word Embeddings](#introduction-to-word-embeddings)\n",
                "  - [‚ùì What are word embeddings, and why are they used?](#what-are-word-embeddings-and-why-are-they-used)\n",
                "  - [üß† How do word embeddings capture semantic meaning in text?](#how-do-word-embeddings-capture-semantic-meaning-in-text)\n",
                "- [üî§ Word2Vec](#word2vec)\n",
                "  - [üìö Overview of Word2Vec: CBOW vs. Skip-Gram](#overview-of-word2vec-cbow-vs-skip-gram)\n",
                "  - [üõ†Ô∏è Training Word2Vec on text data using Gensim](#training-word2vec-on-text-data-using-gensim)\n",
                "  - [üß≠ Example: Visualizing word embeddings with t-SNE](#example-visualizing-word-embeddings-with-t-sne)\n",
                "- [üåê GloVe (Global Vectors for Word Representation)](#glove-global-vectors-for-word-representation)\n",
                "  - [üîç Difference between Word2Vec and GloVe](#difference-between-word2vec-and-glove)\n",
                "  - [üßÆ GloVe‚Äôs matrix factorization approach](#gloves-matrix-factorization-approach)\n",
                "  - [üì¶ Using pre-trained GloVe embeddings in NLP tasks](#using-pre-trained-glove-embeddings-in-nlp-tasks)\n",
                "- [‚ö° FastText](#fasttext)\n",
                "  - [üî° FastText‚Äôs approach to representing words as subword units](#fasttexts-approach-to-representing-words-as-subword-units)\n",
                "  - [üß≥ Handling out-of-vocabulary words with FastText](#handling-out-of-vocabulary-words-with-fasttext)\n",
                "  - [üß™ Example: Training and using FastText for word vector representation](#example-training-and-using-fasttext-for-word-vector-representation)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ **1. Word2Vec Implementation (Hybrid with Code Tooltip)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph Word2Vec_Models[\"Word2Vec Implementation\"]\n",
                "        direction TB\n",
                "        CBOW[[CBOW Architecture<br/>Predict center word from context]]:::blue\n",
                "        SkipGram[[Skip-Gram<br/>Predict context from center]]:::orange\n",
                "        CBOW --> Visualize1[t-SNE Projection]\n",
                "        SkipGram --> Visualize2[t-SNE Projection]\n",
                "    end\n",
                "\n",
                "    RawText[Raw Text Corpus] --> Preprocess[Tokenization & Cleaning]\n",
                "    Preprocess --> CBOW\n",
                "    Preprocess --> SkipGram\n",
                "\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef orange fill:#ffe6cc,stroke:#ff6600\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>üß™ Python Code (CBOW / Skip-Gram)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.models import Word2Vec\n",
                "model_cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW\n",
                "model_sg   = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)  # Skip-Gram\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## ‚úÖ **2. Comparison Matrix (with BERT, Emoji & Context Column)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Comparison[\"Embedding Model Comparison\"]\n",
                "        direction LR\n",
                "        header1[Model] --> header2[OOV] --> header3[Subwords] --> header4[Context-Aware] --> header5[Speed]\n",
                "        row1[Word2Vec] --> cell1[‚ùå] --> cell2[‚ùå] --> cell3[‚ùå] --> cell4[üèéÔ∏è Fast]\n",
                "        row2[GloVe] --> cell5[‚ùå] --> cell6[‚ùå] --> cell7[‚ùå] --> cell8[üèéÔ∏è Fast]\n",
                "        row3[FastText] --> cell9[‚úÖ] --> cell10[‚úÖ] --> cell11[‚ùå] --> cell12[üöó Medium]\n",
                "        row4[BERT] --> cell13[‚úÖ] --> cell14[‚úÖ] --> cell15[‚úÖ] --> cell16[üê¢ Slow]\n",
                "    end\n",
                "    classDef header fill:#e6f3ff,stroke:#0066cc\n",
                "    class header1,header2,header3,header4,header5 header\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## ‚úÖ **3. FastText OOV Handling (with Linguistic Examples)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    A[FastText<br/>OOV Recovery in Morphological Languages]:::green\n",
                "    A --> B[\"üáπüá∑ Turkish: 'evlerinizden' ‚Üí ev + ler + iniz + den\"]\n",
                "    A --> C[\"üá´üáÆ Finnish: 'taloissani' ‚Üí talo + issa + ni\"]\n",
                "    A --> D[\"üá∞üá∑ Korean: 'Í∞ÄÎ∞©Ïóê' ‚Üí Í∞ÄÎ∞© + Î∞©Ïóê\"]\n",
                "    A --> E[\"Subwords averaged ‚Üí Word vector\"]\n",
                "\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    class B,C,D,E green\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>üß™ Python Code (FastText)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.models import FastText\n",
                "model = FastText(sentences, vector_size=100, window=5, min_count=1)\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## ‚úÖ **4. GloVe Mechanics (Cleaned + Objective Function)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    A[Corpus] --> B[Build Co-occurrence Matrix]\n",
                "    B --> C[Factorize Matrix]\n",
                "    C --> D[Word Vectors]\n",
                "    D --> E[Pretrained Vectors] --> F[Fine-tune for Downstream Task]\n",
                "    C -.->|Loss Function| M[[\"J = Œ£ f(X_ij)(w·µ¢·µÄwÃÉ‚±º + b·µ¢ + bÃÉ‚±º ‚àí log X_ij)¬≤\"]]:::math\n",
                "\n",
                "    classDef math fill:#f0e6ff,stroke:#6600cc\n",
                "    class M math\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>üß™ Load Pretrained GloVe (Gensim)</summary>\n",
                "\n",
                "```python\n",
                "from gensim.scripts.glove2word2vec import glove2word2vec\n",
                "glove2word2vec(\"glove.txt\", \"glove.word2vec.txt\")\n",
                "from gensim.models import KeyedVectors\n",
                "model = KeyedVectors.load_word2vec_format(\"glove.word2vec.txt\")\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "## ‚úÖ **5. BERT Contextual Embedding (New Bonus Diagram)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    A[Input Sentence] --> B[WordPiece Tokenization]\n",
                "    B --> C[Transformer Layers]\n",
                "    C --> D[Contextual Embeddings]\n",
                "    D --> E[Fine-tune: NER / QA / Sentiment]\n",
                "\n",
                "    classDef purple fill:#f0e6ff,stroke:#6600cc\n",
                "    class B,C,D,E purple\n",
                "```\n",
                "\n",
                "<details>\n",
                "<summary>üß™ Python Code (Transformers ‚Äì BERT Embeddings)</summary>\n",
                "\n",
                "```python\n",
                "from transformers import BertTokenizer, BertModel\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "model = BertModel.from_pretrained('bert-base-uncased')\n",
                "\n",
                "inputs = tokenizer(\"Paris is the capital of France\", return_tensors=\"pt\")\n",
                "outputs = model(**inputs)\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"introduction-to-word-embeddings\"></a>üßæ Introduction to Word Embeddings\n",
                "\n",
                "## <a id=\"what-are-word-embeddings-and-why-are-they-used\"></a>‚ùì What are word embeddings, and why are they used?\n",
                "\n",
                "## <a id=\"how-do-word-embeddings-capture-semantic-meaning-in-text\"></a>üß† How do word embeddings capture semantic meaning in text?\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"word2vec\"></a>üî§ Word2Vec\n",
                "\n",
                "## <a id=\"overview-of-word2vec-cbow-vs-skip-gram\"></a>üìö Overview of Word2Vec: CBOW vs. Skip-Gram\n",
                "\n",
                "## <a id=\"training-word2vec-on-text-data-using-gensim\"></a>üõ†Ô∏è Training Word2Vec on text data using Gensim\n",
                "\n",
                "## <a id=\"example-visualizing-word-embeddings-with-t-sne\"></a>üß≠ Example: Visualizing word embeddings with t-SNE\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"glove-global-vectors-for-word-representation\"></a>üåê GloVe (Global Vectors for Word Representation)\n",
                "\n",
                "## <a id=\"difference-between-word2vec-and-glove\"></a>üîç Difference between Word2Vec and GloVe\n",
                "\n",
                "## <a id=\"gloves-matrix-factorization-approach\"></a>üßÆ GloVe‚Äôs matrix factorization approach\n",
                "\n",
                "## <a id=\"using-pre-trained-glove-embeddings-in-nlp-tasks\"></a>üì¶ Using pre-trained GloVe embeddings in NLP tasks\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"fasttext\"></a>‚ö° FastText\n",
                "\n",
                "## <a id=\"fasttexts-approach-to-representing-words-as-subword-units\"></a>üî° FastText‚Äôs approach to representing words as subword units\n",
                "\n",
                "## <a id=\"handling-out-of-vocabulary-words-with-fasttext\"></a>üß≥ Handling out-of-vocabulary words with FastText\n",
                "\n",
                "## <a id=\"example-training-and-using-fasttext-for-word-vector-representation\"></a>üß™ Example: Training and using FastText for word vector representation\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
