{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Table of Contents\n",
                "\n",
                "- [ü§ñ Introduction to Pretrained Transformers](#introduction-to-pretrained-transformers)\n",
                "  - [üìà The rise of transformer-based models in NLP](#the-rise-of-transformer-based-models-in-nlp)\n",
                "  - [üîç Key transformer models: BERT, GPT, and their architecture differences](#key-transformer-models-bert-gpt-and-their-architecture-differences)\n",
                "  - [üí° Why pretraining is effective for NLP tasks](#why-pretraining-is-effective-for-nlp-tasks)\n",
                "- [üß† BERT (Bidirectional Encoder Representations from Transformers)](#bert-bidirectional-encoder-representations-from-transformers)\n",
                "  - [‚ôªÔ∏è BERT's bidirectional attention mechanism](#berts-bidirectional-attention-mechanism)\n",
                "  - [üéØ Fine-tuning BERT for downstream tasks](#fine-tuning-bert-for-downstream-tasks)\n",
                "  - [üß™ Example: Fine-tuning BERT using Hugging Face](#example-fine-tuning-bert-using-hugging-face)\n",
                "- [üìù GPT (Generative Pretrained Transformer)](#gpt-generative-pretrained-transformer)\n",
                "  - [üîÅ Understanding the autoregressive nature of GPT models](#understanding-the-autoregressive-nature-of-gpt-models)\n",
                "  - [üß† How GPT is used for text generation and language modeling](#how-gpt-is-used-for-text-generation-and-language-modeling)\n",
                "  - [üß™ Example: Fine-tuning GPT for specific text tasks](#example-fine-tuning-gpt-for-specific-text-tasks)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Transformer Evolution Timeline (Fixed Syntax)**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    classDef bert fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef gpt fill:#e6ffe6,stroke:#009900\n",
                "    \n",
                "    2017[2017: Original Transformer] --> 2018B[2018: BERT]:::bert\n",
                "    2017 --> 2018G[2018: GPT-1]:::gpt\n",
                "    2018B --> 2019[2019: RoBERTa/XLNet]\n",
                "    2018G --> 2020[2020: GPT-3]\n",
                "    2018G --> 2022[2022: ChatGPT]\n",
                "    \n",
                "    click 2018B \"https://arxiv.org/abs/1810.04805\" _blank\n",
                "    click 2018G \"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\" _blank\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. BERT Architecture & Fine-Tuning (Validated)**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    %% BERT Architecture\n",
                "    subgraph BERT[\"BERT Architecture\"]\n",
                "        direction TB\n",
                "        Input[\"[CLS] The model output [MASK] great [SEP]\"] --> Tokenizer\n",
                "        Tokenizer -->|Token Embeddings| Encoder1[[Transformer Encoder]]\n",
                "        Encoder1 --> Encoder2[[...]] --> Encoder12[[Encoder 12]]\n",
                "    end\n",
                "\n",
                "    %% Fine-Tuning Process\n",
                "    subgraph FineTune[\"Fine-Tuning Process\"]\n",
                "        direction TB\n",
                "        ftCode[\"from transformers import BertForSequenceClassification\n",
                "        model = BertForSequenceClassification.from_pretrained(\n",
                "            'bert-base-uncased', \n",
                "            num_labels=2\n",
                "        )\"]:::code\n",
                "        ftData[Labeled Dataset] --> ftTrainer[[Trainer]] --> ftModel[Fine-Tuned Model]\n",
                "    end\n",
                "\n",
                "    %% Connections\n",
                "    BERT --> FineTune\n",
                "\n",
                "    %% Style Definitions\n",
                "    classDef code fill:#f8f8f8,stroke:#666,font-family:monospace\n",
                "    classDef bert fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef encoder fill:#ffffff,stroke:#999999\n",
                "    class Encoder1,Encoder2,Encoder12 encoder\n",
                "    class ftCode code\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. GPT Autoregressive Generation (Syntax Fixed)**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    Prompt[\"Input: 'The future of AI'\"] --> Tokens\n",
                "    subgraph GPT[\"GPT Decoder Stack\"]\n",
                "        direction TB\n",
                "        T1[Transformer Decoder] --> T2[...] --> Tn[Decoder N]\n",
                "    end\n",
                "    Tokens --> GPT --> NextToken --> Output[\"Output: '...is bright and full'\"]\n",
                "    \n",
                "    style GPT fill:#e6ffe6\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Pretraining Effectiveness (Validated)**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    Why[\"Why Pretraining Works?\"] --> Transfer[\"Transfer Learning\"]\n",
                "    Why --> Scale[\"Scale: 40GB+ Text Data\"]\n",
                "    Why --> Context[\"Deep Context Understanding\"]\n",
                "    Why --> Adapt[\"Adaptability\"]\n",
                "    \n",
                "    Transfer -->|Reuse language patterns| Downstream\n",
                "    Scale -->|Learn rare patterns| Robustness\n",
                "    Context -->|Understand relationships| Accuracy\n",
                "    Adapt -->|Add task layers| Versatility\n",
                "    \n",
                "    classDef concept fill:#fff3d6,stroke:#ffcc00\n",
                "    class Why,Transfer,Scale,Context,Adapt concept\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. BERT vs GPT Comparison Matrix**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Comparison[\"Key Differences\"]\n",
                "        direction LR\n",
                "        BERT[\"BERT<br>‚Ä¢ Bidirectional<br>‚Ä¢ Masked LM<br>‚Ä¢ Encoder-only\"]:::bert\n",
                "        vs[vs]:::hidden\n",
                "        GPT[\"GPT<br>‚Ä¢ Left-to-right<br>‚Ä¢ Causal LM<br>‚Ä¢ Decoder-only\"]:::gpt\n",
                "    end\n",
                "    \n",
                "    classDef bert fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef gpt fill:#e6ffe6,stroke:#009900\n",
                "    classDef hidden fill:#ffffff,stroke:#ffffff\n",
                "```\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"introduction-to-pretrained-transformers\"></a>ü§ñ Introduction to Pretrained Transformers\n",
                "\n",
                "# <a id=\"the-rise-of-transformer-based-models-in-nlp\"></a>üìà The rise of transformer-based models in NLP\n",
                "\n",
                "# <a id=\"key-transformer-models-bert-gpt-and-their-architecture-differences\"></a>üîç Key transformer models: BERT, GPT, and their architecture differences\n",
                "\n",
                "# <a id=\"why-pretraining-is-effective-for-nlp-tasks\"></a>üí° Why pretraining is effective for NLP tasks\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"bert-bidirectional-encoder-representations-from-transformers\"></a>üß† BERT (Bidirectional Encoder Representations from Transformers)\n",
                "\n",
                "# <a id=\"berts-bidirectional-attention-mechanism\"></a>‚ôªÔ∏è BERT's bidirectional attention mechanism\n",
                "\n",
                "# <a id=\"fine-tuning-bert-for-downstream-tasks\"></a>üéØ Fine-tuning BERT for downstream tasks\n",
                "\n",
                "# <a id=\"example-fine-tuning-bert-using-hugging-face\"></a>üß™ Example: Fine-tuning BERT using Hugging Face\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"gpt-generative-pretrained-transformer\"></a>üìù GPT (Generative Pretrained Transformer)\n",
                "\n",
                "# <a id=\"understanding-the-autoregressive-nature-of-gpt-models\"></a>üîÅ Understanding the autoregressive nature of GPT models\n",
                "\n",
                "# <a id=\"how-gpt-is-used-for-text-generation-and-language-modeling\"></a>üß† How GPT is used for text generation and language modeling\n",
                "\n",
                "# <a id=\"example-fine-tuning-gpt-for-specific-text-tasks\"></a>üß™ Example: Fine-tuning GPT for specific text tasks\n",
                "\n",
                "---\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
