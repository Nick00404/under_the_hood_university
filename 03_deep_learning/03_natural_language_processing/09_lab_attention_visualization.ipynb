{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c74dcc",
   "metadata": {},
   "source": [
    "ðŸ”¥ Locked in. Time to peek inside the mind of a transformer.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `09_lab_attention_visualization.ipynb`  \n",
    "### ðŸ“ `03_natural_language_processing`  \n",
    "> Visualize **self-attention heads** in real transformer models like BERT or GPT-2.  \n",
    "See which words attend to which â€” layer-by-layer, head-by-head.  \n",
    "**Intuition meets interpretability** in this lab.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Understand **self-attention weights** as visual heatmaps  \n",
    "- Use tools like **`bertviz`** to see where attention flows  \n",
    "- Compare **different heads/layers**  \n",
    "- Analyze attention patterns: positional, syntactic, semantic\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Design\n",
    "\n",
    "| Feature          | Spec             |\n",
    "|------------------|------------------|\n",
    "| Platform         | âœ… Colab (recommended)  \n",
    "| Model            | âœ… `bert-base-uncased` (or `gpt2`)  \n",
    "| Tooling          | âœ… `bertviz` or `transformer-vis`  \n",
    "| Hardware         | âœ… CPU/GPU (minimal VRAM)  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Section 1: Install & Import\n",
    "\n",
    "```python\n",
    "!pip install transformers bertviz\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from bertviz import head_view, model_view\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Section 2: Load Model & Tokenizer\n",
    "\n",
    "```python\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“„ Section 3: Prepare Input Sentence\n",
    "\n",
    "```python\n",
    "sentence = \"The cat sat on the mat and looked at the dog.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(**inputs).attentions  # Tuple of layers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¬ Section 4: Visualize Attention â€” Model View\n",
    "\n",
    "```python\n",
    "# Full attention structure across layers and heads\n",
    "model_view(attention, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "```\n",
    "\n",
    "> ðŸ§  Shows attention across **all layers + heads**  \n",
    "> Hover over tokens to see which ones they focus on  \n",
    "> Great for analyzing **which layers attend to what**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Section 5: Visualize Specific Head â€” Head View\n",
    "\n",
    "```python\n",
    "head_view(attention, tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "```\n",
    "\n",
    "> ðŸ” Lets you isolate specific **layer/head pairs**  \n",
    "> See if early layers attend **positionally**, while later layers attend **semantically**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Optional: Use GPT-2 Instead\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sentence = \"Transformers are changing the world.\"\n",
    "\n",
    "gpt2_inputs = gpt2_tokenizer(sentence, return_tensors='pt')\n",
    "attn = gpt2(**gpt2_inputs).attentions\n",
    "\n",
    "head_view(attn, gpt2_tokenizer.convert_ids_to_tokens(gpt2_inputs['input_ids'][0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 6: What to Look For\n",
    "\n",
    "| Layer | Pattern            | Meaning |\n",
    "|-------|--------------------|---------|\n",
    "| 0â€“3   | Positional          | Looks left/right like positional encodings  \n",
    "| 4â€“8   | Phrase-based        | Attention spans across phrase boundaries  \n",
    "| 9â€“12  | Semantic & summary  | Focuses on nouns, verbs, sentence ends  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up\n",
    "\n",
    "| What You Did             | âœ… |\n",
    "|--------------------------|----|\n",
    "| Visualized self-attention| âœ… |\n",
    "| Understood head structure| âœ… |\n",
    "| Interpreted patterns     | âœ… |\n",
    "| Used BERT and GPT2       | âœ… |\n",
    "| Colab safe               | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- **Attention = context mapping**  \n",
    "- Each head has its own **linguistic role**  \n",
    "- Transformers **donâ€™t see linearly** â€” they jump, link, and focus dynamically  \n",
    "- You can now **debug models visually**, not just by numbers\n",
    "\n",
    "---\n",
    "\n",
    "âœ… That's the final NLP lab in this batch.\n",
    "\n",
    "Next up: `04_advanced_architectures` â†’  \n",
    "Want to move to `07_lab_gnn_node_classification_with_cora.ipynb` and get graphy with it?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
