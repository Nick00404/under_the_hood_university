{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Table of Contents\n",
                "\n",
                "\n",
                "- [üîÅ Introduction to Recurrent Neural Networks (RNNs)](#introduction-to-recurrent-neural-networks-rnns)\n",
                "  - [üßµ What is a RNN and how does it handle sequential data?](#what-is-a-rnn-and-how-does-it-handle-sequential-data)\n",
                "  - [‚ö†Ô∏è Understanding vanishing and exploding gradient problems in RNNs](#understanding-vanishing-and-exploding-gradient-problems-in-rnns)\n",
                "  - [üß™ Implementing a simple RNN for text classification](#implementing-a-simple-rnn-for-text-classification)\n",
                "- [üß† Long Short-Term Memory (LSTM) Networks](#long-short-term-memory-lstm-networks)\n",
                "  - [üö™ The LSTM architecture: Forget, input, and output gates](#the-lstm-architecture-forget-input-and-output-gates)\n",
                "  - [üß± Solving the vanishing gradient problem with LSTMs](#solving-the-vanishing-gradient-problem-with-lstms)\n",
                "  - [üìà Example: Using LSTMs for sentiment analysis on text data](#example-using-lstms-for-sentiment-analysis-on-text-data)\n",
                "- [üîí Gated Recurrent Units (GRUs)](#gated-recurrent-units-grus)\n",
                "  - [üÜö Differences between GRUs and LSTMs](#differences-between-grus-and-lstms)\n",
                "  - [‚ùì When to choose GRUs over LSTMs](#when-to-choose-grus-over-lstms)\n",
                "  - [üß™ Implementing GRUs for language modeling or sequence prediction tasks](#implementing-grus-for-language-modeling-or-sequence-prediction-tasks)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Fixed Core RNN Architecture (Layered View)**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    %% Basic Structure\n",
                "    subgraph Basic[\"Basic RNN Structure\"]\n",
                "        direction LR\n",
                "        X[Input x<sub>t</sub>] --> H[Hidden h<sub>t</sub>]\n",
                "        H --> Y[Output ≈∑<sub>t</sub>]\n",
                "        H --> HNext[Next h<sub>t+1</sub>]\n",
                "    end\n",
                "\n",
                "    %% Detailed View\n",
                "    subgraph Detail[\"Detailed View\"]\n",
                "        direction LR\n",
                "        xt[Input x<sub>t</sub>] -->|Concat| HMath[[\"h<sub>t</sub> = tanh(W<sub>h</sub>[h<sub>t-1</sub>,x<sub>t</sub>]+b)\"]]\n",
                "        HMath --> yMath[[\"≈∑<sub>t</sub> = œÉ(W<sub>y</sub>h<sub>t</sub>)\"]]\n",
                "        note[[\"Parameters: (300+128)√ó128 + 128 = 38,528\"]]:::yellow\n",
                "    end\n",
                "\n",
                "    %% Connection\n",
                "    Basic --> Detail\n",
                "    \n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "```\n",
                "---\n",
                "\n",
                "### **2. LSTM Architecture with Mathematical Context**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    %% Gate Structure\n",
                "    subgraph Gates[\"LSTM Gates\"]\n",
                "        direction LR\n",
                "        ft[Forget œÉ]:::sigmoid\n",
                "        it[Input œÉ]:::sigmoid\n",
                "        Ct[Cell tanh]:::tanh\n",
                "        ot[Output œÉ]:::sigmoid\n",
                "    end\n",
                "\n",
                "    %% Parameter Context\n",
                "    subgraph Math[\"Mathematical Context\"]\n",
                "        direction TB\n",
                "        Formula[[\"Params = 4√ó(h<sub>in</sub>+h<sub>hid</sub>+1)√óh<sub>hid</sub>\n",
                "        h=128 ‚Üí 4√ó(300+128)√ó128 = 219,648\"]]:::yellow\n",
                "        Gates --> Formula\n",
                "    end\n",
                "\n",
                "    %% Real-world Connection\n",
                "    subgraph Usage[\"Practical Use\"]\n",
                "        Model[[\"LSTM(units=128)<br/>Keras/PyTorch API\"]] --> Perf[[\"~3√ó slower than GRU<br/>Better long-range dep\"]]\n",
                "    end\n",
                "\n",
                "    classDef sigmoid fill:#e6ffe6,stroke:#009900\n",
                "    classDef tanh fill:#ffe6cc,stroke:#ff9900\n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. GRU vs LSTM: Complete Comparison**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Structural[\"Structural Difference\"]\n",
                "        direction LR\n",
                "        LSTM_Struct[[\"LSTM: 3 gates + 2 states\"]] --> GRU_Struct[[\"GRU: 2 gates + 1 state\"]]\n",
                "    end\n",
                "\n",
                "    subgraph Practical[\"Practical Trade-offs\"]\n",
                "        direction LR\n",
                "        Params[[\"Params: LSTM=4h(h+i) vs GRU=3h(h+i)\"]] --> Speed[[\"Speed: GRU ~25% faster\"]]\n",
                "        Accuracy[[\"Accuracy: LSTM better on >100 steps\"]] --> UseCase[[\"Use GRU for:<br/>- Real-time apps<br/>- Short sequences\"]]\n",
                "    end\n",
                "\n",
                "    Structural --> Practical\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Fixed Integrated Training Workflow**\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Workflow[\"End-to-End Training\"]\n",
                "        direction LR\n",
                "        Code[[\"Code:\n",
                "        model = Sequential()\n",
                "        model.add(LSTM(128))\n",
                "        model.add(Dense(2))\"]] --> Data[Data Loading]\n",
                "        Data --> Train[Training Loop]\n",
                "        Train --> Eval[Evaluation]\n",
                "    end\n",
                "\n",
                "    subgraph Theory[\"Theoretical Foundation\"]\n",
                "        direction TB\n",
                "        Loss[[\"Loss = Cross-Entropy\"]] --> Opt[[\"Adam Optimizer\"]]\n",
                "    end\n",
                "\n",
                "    Workflow --> Theory\n",
                "\n",
                "    classDef code fill:#f8f8f8,stroke:#666\n",
                "    class Code code\n",
                "```\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. RNN Architecture & Parameter Insight**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph RNN[\"RNN Cell\"]\n",
                "        direction LR\n",
                "        xt[(\"x<sub>t</sub>\")] -->|Concatenate| H[[\"h<sub>t</sub> = tanh(W<sub>h</sub>[h<sub>t-1</sub>,x<sub>t</sub>] + b)\n",
                "        Params = (h<sub>in</sub>+h<sub>hid</sub>)*h<sub>hid</sub> + h<sub>hid</sub>\"]]\n",
                "        H --> yt[(\"≈∑<sub>t</sub> = œÉ(W<sub>y</sub>h<sub>t</sub>)\")]\n",
                "        H -->|Recurrence| H_prev[(\"h<sub>t-1</sub>\")]\n",
                "    end\n",
                "\n",
                "    subgraph Problems[\"Gradient Issues\"]\n",
                "        direction TB\n",
                "        VG[[\"Vanishing Gradients<br/>(‚àÇLoss/‚àÇh‚ÇÄ ‚âà 0)\"]]:::red\n",
                "        EG[[\"Exploding Gradients<br/>(||‚àÇh‚Çú/‚àÇh‚ÇÄ|| ‚Üí ‚àû)\"]]:::orange\n",
                "    end\n",
                "\n",
                "    classDef tanh fill:#ffe6cc,stroke:#ff9900\n",
                "    classDef sigmoid fill:#e6ffe6,stroke:#009900\n",
                "    class H tanh\n",
                "    class yt sigmoid\n",
                "    classDef red fill:#ffe6e6,stroke:#cc0000\n",
                "    classDef orange fill:#ffebcc,stroke:#ff9900\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. LSTM: Gate Logic + Parameterization**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TB\n",
                "    subgraph LSTM[\"LSTM Cell\"]\n",
                "        direction LR\n",
                "        ft[[\"Forget Gate<br/>œÉ (sigmoid)\"]]:::sigmoid\n",
                "        it[[\"Input Gate<br/>œÉ (sigmoid)\"]]:::sigmoid\n",
                "        Ct[[\"Cell Update<br/>tanh\"]]:::tanh\n",
                "        ot[[\"Output Gate<br/>œÉ (sigmoid)\"]]:::sigmoid\n",
                "        ht[[\"Hidden State<br/>tanh\"]]:::tanh\n",
                "    end\n",
                "\n",
                "    subgraph Params[\"LSTM Param Count\"]\n",
                "        direction TB\n",
                "        Formula[[\"LSTM = 4 √ó (h<sub>in</sub> + h<sub>hid</sub> + 1) √ó h<sub>hid</sub><br/>\n",
                "        Example: h=128, i=300 ‚Üí 4√ó(300+128+1)√ó128 = 219,648\"]]:::yellow\n",
                "    end\n",
                "\n",
                "    classDef sigmoid fill:#e6ffe6,stroke:#009900\n",
                "    classDef tanh fill:#ffe6cc,stroke:#ff9900\n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. GRU vs LSTM vs RNN: Structural & Parametric Comparison**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph Structure[\"Architectural Comparison\"]\n",
                "        direction TB\n",
                "        LSTM_S[[\"LSTM: 3 Gates (Forget, Input, Output) + 2 States (Cell, Hidden)\"]]:::purple\n",
                "        GRU_S[[\"GRU: 2 Gates (Update, Reset) + 1 State (Hidden)\"]]:::green\n",
                "        RNN_S[[\"SimpleRNN: No gates, 1 State (Hidden)\"]]:::orange\n",
                "    end\n",
                "\n",
                "    subgraph Params[\"Parameter Count\"]\n",
                "        direction TB\n",
                "        LSTM_P[[\"LSTM: 4h(h+i+1) = 219,648\"]]:::purple\n",
                "        GRU_P[[\"GRU: 3h(h+i+1) = 164,736\"]]:::green\n",
                "        RNN_P[[\"RNN: h(h+i+1) = 54,912\"]]:::orange\n",
                "    end\n",
                "\n",
                "    classDef purple fill:#f0e6ff,stroke:#6600cc\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef orange fill:#ffe6cc,stroke:#ff9900\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Implementation Examples**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph Keras[\"Keras Example\"]\n",
                "        Input[\"Input(seq_len)\"] --> Embed[\"Embedding(128)\"] --> RNNL[\"SimpleRNN(64)\"] --> Dense[\"Dense(2, softmax)\"]\n",
                "    end\n",
                "\n",
                "    subgraph PyTorch[\"PyTorch GRU\"]\n",
                "        Define[[\"nn.GRU(input_size=300, hidden_size=128, num_layers=2, batch_first=True)\"]]:::code\n",
                "        Forward[[\"out, _ = self.gru(x)\\nout = self.fc(out[:,-1,:])\"]]\n",
                "    end\n",
                "\n",
                "    classDef code fill:#f8f8f8,stroke:#666\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **5. Integrated Training Workflow**\n",
                "\n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph All[\"Training Workflow\"]\n",
                "        direction LR\n",
                "        Data[[\"1. Data Prep\n",
                "        - Tokenize\n",
                "        - Pad\n",
                "        - Embed\"]] --> Model[[\"2. Model Selection\n",
                "        - RNN ‚Üí Short Seqs\n",
                "        - LSTM ‚Üí Long Deps\n",
                "        - GRU ‚Üí Fast Training\"]]\n",
                "        Model --> Train[[\"3. Train Loop\n",
                "        - Forward\n",
                "        - Loss\n",
                "        - BPTT\n",
                "        - Update\"]]\n",
                "        Train --> Eval[[\"4. Evaluation\n",
                "        - Accuracy / Perplexity\n",
                "        - Grad Norm\"]]\n",
                "    end\n",
                "\n",
                "    subgraph Config[\"Training Config\"]\n",
                "        direction TB\n",
                "        Loss[[\"Loss:\n",
                "        - CrossEntropy (LM)\n",
                "        - BCE (Classif)\"]]:::yellow\n",
                "        Opt[[\"Optimizer:\n",
                "        - Adam (lr=3e-4)\n",
                "        - Grad Clipping\"]]:::blue\n",
                "    end\n",
                "\n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"introduction-to-recurrent-neural-networks-rnns\"></a>üîÅ Introduction to Recurrent Neural Networks (RNNs)\n",
                "\n",
                "# <a id=\"what-is-a-rnn-and-how-does-it-handle-sequential-data\"></a>üßµ What is a RNN and how does it handle sequential data?\n",
                "\n",
                "# <a id=\"understanding-vanishing-and-exploding-gradient-problems-in-rnns\"></a>‚ö†Ô∏è Understanding vanishing and exploding gradient problems in RNNs\n",
                "\n",
                "# <a id=\"implementing-a-simple-rnn-for-text-classification\"></a>üß™ Implementing a simple RNN for text classification\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"long-short-term-memory-lstm-networks\"></a>üß† Long Short-Term Memory (LSTM) Networks\n",
                "\n",
                "# <a id=\"the-lstm-architecture-forget-input-and-output-gates\"></a>üö™ The LSTM architecture: Forget, input, and output gates\n",
                "\n",
                "# <a id=\"solving-the-vanishing-gradient-problem-with-lstms\"></a>üß± Solving the vanishing gradient problem with LSTMs\n",
                "\n",
                "# <a id=\"example-using-lstms-for-sentiment-analysis-on-text-data\"></a>üìà Example: Using LSTMs for sentiment analysis on text data\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"gated-recurrent-units-grus\"></a>üîí Gated Recurrent Units (GRUs)\n",
                "\n",
                "# <a id=\"differences-between-grus-and-lstms\"></a>üÜö Differences between GRUs and LSTMs\n",
                "\n",
                "# <a id=\"when-to-choose-grus-over-lstms\"></a>‚ùì When to choose GRUs over LSTMs\n",
                "\n",
                "# <a id=\"implementing-grus-for-language-modeling-or-sequence-prediction-tasks\"></a>üß™ Implementing GRUs for language modeling or sequence prediction tasks\n",
                "\n",
                "---\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
