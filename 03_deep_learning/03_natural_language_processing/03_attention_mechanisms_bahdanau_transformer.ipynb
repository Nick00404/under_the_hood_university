{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“š Table of Contents\n",
                "\n",
                "- [ğŸ¯ Understanding Attention Mechanisms](#understanding-attention-mechanisms)\n",
                "  - [ğŸ‘ï¸ What is attention in the context of NLP?](#what-is-attention-in-the-context-of-nlp)\n",
                "  - [ğŸš€ Why attention mechanisms improve sequence-to-sequence models](#why-attention-mechanisms-improve-sequence-to-sequence-models)\n",
                "  - [ğŸ§ª Example: Implementing Bahdanau attention for sequence-to-sequence tasks](#example-implementing-bahdanau-attention-for-sequence-to-sequence-tasks)\n",
                "- [ğŸ¯ Bahdanau Attention (Additive Attention)](#bahdanau-attention-additive-attention)\n",
                "  - [ğŸ”‘ Key components: Query, Key, and Value](#key-components-query-key-and-value)\n",
                "  - [ğŸ“š How Bahdanau attention works and its application in machine translation](#how-bahdanau-attention-works-and-its-application-in-machine-translation)\n",
                "  - [ğŸ§ª Example: Using Bahdanau attention with an RNN encoder-decoder](#example-using-bahdanau-attention-with-an-rnn-encoder-decoder)\n",
                "- [ğŸ§¬ Transformers and Self-Attention](#transformers-and-self-attention)\n",
                "  - [ğŸ“˜ Introduction to transformers and their self-attention mechanism](#introduction-to-transformers-and-their-self-attention-mechanism)\n",
                "  - [âš¡ How transformers outperform RNNs](#how-transformers-outperform-rnns)\n",
                "  - [ğŸ› ï¸ Example: Building a simple transformer model for text classification](#example-building-a-simple-transformer-model-for-text-classification)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Core Attention Mechanism Overview**\n",
                "**Diagram Type:** Comparative Architecture Flow  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    %% Traditional Seq2Seq vs Attention\n",
                "    subgraph NoAttn[\"Traditional Seq2Seq\"]\n",
                "        direction TB\n",
                "        EncoderRNN -->|Last Hidden State| DecoderRNN\n",
                "    end\n",
                "    \n",
                "    subgraph WithAttn[\"Seq2Seq with Attention\"]\n",
                "        direction TB\n",
                "        EncoderAll[All Encoder States] --> Attention\n",
                "        DecoderStep[Decoder State] --> Attention\n",
                "        Attention --> Context[Context Vector]\n",
                "        Context --> DecoderRNN\n",
                "    end\n",
                "    \n",
                "    %% Key Annotation\n",
                "    note[[\"Attention allows dynamic context weighting<br/>instead of fixed last state\"]]:::yellow\n",
                "    WithAttn ~~~ note\n",
                "    \n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "    linkStyle 0,1,2,3,4,5 stroke:#999,stroke-width:1px\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Bahdanau Attention Mechanics**\n",
                "**Diagram Type:** Step-by-Step Process Flow  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    %% Components\n",
                "    Q[Decoder Query]:::blue --> Score[Attention Scores]\n",
                "    K[Encoder Keys]:::green --> Score\n",
                "    Score --> Weights[Softmax Weights]\n",
                "    V[Encoder Values]:::red --> Context[Weighted Sum]\n",
                "    Weights --> Context\n",
                "\n",
                "    %% Mathematical Details\n",
                "    Score -.-> Formula[\"a_ij = v^T tanh(W1*h_i + W2*s_j)\"]:::formula\n",
                "\n",
                "    %% Implementation Flow\n",
                "    Context --> DecoderStep[Decoder Output]\n",
                "\n",
                "    %% Style Definitions\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef red fill:#ffe6e6,stroke:#cc0000\n",
                "    classDef formula fill:#f0e6ff,stroke:#6600cc,font-size:12px\n",
                "    linkStyle 0,1,2,3,4,5 stroke:#999,stroke-width:1px\n",
                "```\n",
                "---\n",
                "\n",
                "### **3. Transformer Self-Attention**\n",
                "**Diagram Type:** Matrix Operation Diagram  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart LR\n",
                "    %% Input Processing\n",
                "    X[Input Embeddings] --> QKV[Q,K,V Matrices]\n",
                "    QKV -->|Split| Q[Query]:::blue\n",
                "    QKV -->|Split| K[Key]:::green\n",
                "    QKV -->|Split| V[Value]:::red\n",
                "    \n",
                "    %% Attention Calculation\n",
                "    Q --> MatMul[QÃ—K<sup>T</sup>]\n",
                "    K --> MatMul\n",
                "    MatMul --> Scale[\"Scale/âˆšd<sub>k</sub>\"]\n",
                "    Scale --> Mask[Softmax]\n",
                "    Mask --> Attn[Attention Weights]\n",
                "    Attn --> Output[Weighted V]\n",
                "    \n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef red fill:#ffe6e6,stroke:#cc0000\n",
                "    linkStyle 0,1,2,3,4,5 stroke:#999,stroke-width:1px\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **4. Full Transformer Architecture**\n",
                "**Diagram Type:** Layered Block Diagram  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Transformer[\"Transformer Block\"]\n",
                "        direction LR\n",
                "        Input --> PosEnc[Positional Encoding]\n",
                "        PosEnc --> MultiAttn[Multi-Head Attention]\n",
                "        MultiAttn --> AddNorm[Add & Norm]\n",
                "        AddNorm --> FFN[Feed Forward]\n",
                "        FFN --> Output\n",
                "    end\n",
                "    \n",
                "    %% Key Features\n",
                "    note[[\"Parallel processing of all positions<br/>No recurrence â†’ faster training\"]]:::yellow\n",
                "    Transformer ~~~ note\n",
                "    \n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "    linkStyle 0,1,2,3,4,5 stroke:#999,stroke-width:1px\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **Implementation Examples**\n",
                "\n",
                "**1. Bahdanau Attention Code Snippet**  \n",
                "```python\n",
                "class BahdanauAttention(tf.keras.layers.Layer):\n",
                "    def call(self, query, values):\n",
                "        query = tf.expand_dims(query, 1)\n",
                "        score = tf.keras.layers.Dense(units)(tf.nn.tanh(query + values))\n",
                "        weights = tf.nn.softmax(score, axis=1)\n",
                "        return tf.reduce_sum(weights * values, axis=1)\n",
                "```\n",
                "\n",
                "**2. Transformer Text Classification**  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
                "flowchart TD\n",
                "    Input[Text] --> Emb[Embedding]\n",
                "    Emb --> Pos[Positional Encoding]\n",
                "    Pos --> T1[Transformer Block] \n",
                "    T1 --> T2[Transformer Block]\n",
                "    T2 --> Pool[Global Average Pooling]\n",
                "    Pool --> Dense[Classifier]\n",
                "    linkStyle 0,1,2,3,4,5 stroke:#999,stroke-width:1px\n",
                "```\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"understanding-attention-mechanisms\"></a>ğŸ¯ Understanding Attention Mechanisms\n",
                "\n",
                "# <a id=\"what-is-attention-in-the-context-of-nlp\"></a>ğŸ‘ï¸ What is attention in the context of NLP?\n",
                "\n",
                "# <a id=\"why-attention-mechanisms-improve-sequence-to-sequence-models\"></a>ğŸš€ Why attention mechanisms improve sequence-to-sequence models\n",
                "\n",
                "# <a id=\"example-implementing-bahdanau-attention-for-sequence-to-sequence-tasks\"></a>ğŸ§ª Example: Implementing Bahdanau attention for sequence-to-sequence tasks\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"bahdanau-attention-additive-attention\"></a>ğŸ¯ Bahdanau Attention (Additive Attention)\n",
                "\n",
                "# <a id=\"key-components-query-key-and-value\"></a>ğŸ”‘ Key components: Query, Key, and Value\n",
                "\n",
                "# <a id=\"how-bahdanau-attention-works-and-its-application-in-machine-translation\"></a>ğŸ“š How Bahdanau attention works and its application in machine translation\n",
                "\n",
                "# <a id=\"example-using-bahdanau-attention-with-an-rnn-encoder-decoder\"></a>ğŸ§ª Example: Using Bahdanau attention with an RNN encoder-decoder\n",
                "\n",
                "---\n",
                "\n",
                "# <a id=\"transformers-and-self-attention\"></a>ğŸ§¬ Transformers and Self-Attention\n",
                "\n",
                "# <a id=\"introduction-to-transformers-and-their-self-attention-mechanism\"></a>ğŸ“˜ Introduction to transformers and their self-attention mechanism\n",
                "\n",
                "# <a id=\"how-transformers-outperform-rnns\"></a>âš¡ How transformers outperform RNNs\n",
                "\n",
                "# <a id=\"example-building-a-simple-transformer-model-for-text-classification\"></a>ğŸ› ï¸ Example: Building a simple transformer model for text classification\n",
                "\n",
                "---\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
