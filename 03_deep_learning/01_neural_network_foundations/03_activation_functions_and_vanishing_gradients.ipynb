{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Table of Contents\n",
                "\n",
                "- [üßÆ Activation Functions in Neural Networks](#activation-functions-in-neural-networks)\n",
                "  - [üéØ Role of activation functions in deep networks](#role-of-activation-functions-in-deep-networks)\n",
                "  - [üìä Common activation functions: ReLU, Sigmoid, Tanh, Softmax](#common-activation-functions-relu-sigmoid-tanh-softmax)\n",
                "  - [üß† Why activation functions are crucial for learning complex patterns](#why-activation-functions-are-crucial-for-learning-complex-patterns)\n",
                "- [‚ö†Ô∏è Vanishing Gradient Problem](#vanishing-gradient-problem)\n",
                "  - [‚ùì What is vanishing gradients and why does it occur?](#what-is-vanishing-gradients-and-why-does-it-occur)\n",
                "  - [üß± How the vanishing gradient problem affects deep neural networks](#how-the-vanishing-gradient-problem-affects-deep-neural-networks)\n",
                "  - [üõ†Ô∏è Solutions to vanishing gradients (e.g., ReLU, He Initialization)](#solutions-to-vanishing-gradients-eg-relu-he-initialization)\n",
                "- [üöÄ Improving Learning with Activation Functions](#improving-learning-with-activation-functions)\n",
                "  - [üåü Leaky ReLU, ELU, SELU and their advantages over traditional ReLU](#leaky-relu-elu-selu-and-their-advantages-over-traditional-relu)\n",
                "  - [üí• Exploding gradients and gradient clipping](#exploding-gradients-and-gradient-clipping)\n",
                "  - [üß™ Implementing these solutions in both PyTorch and TensorFlow](#implementing-these-solutions-in-both-pytorch-and-tensorflow)\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **1. Activation Functions Diagram**  \n",
                "**Focus:** Types, properties, and roles of activation functions  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Roles[\"Why Activations Matter\"]\n",
                "        direction TB\n",
                "        R1[Introduce Non-Linearity] --> R2[Enable Complex Pattern Learning]\n",
                "        R2 --> R3[Control Output Ranges]\n",
                "    end\n",
                "\n",
                "    subgraph Types[\"Common Activation Functions\"]\n",
                "        direction LR\n",
                "        A1[[\"ReLU<br/>max(0, z)\"]]:::green\n",
                "        A2[[\"Sigmoid<br/>1/(1+e‚Åª·∂ª)\"]]:::orange\n",
                "        A3[[\"Tanh<br/>(e·∂ª - e‚Åª·∂ª)/(e·∂ª + e‚Åª·∂ª)\"]]:::blue\n",
                "        A4[[\"Softmax<br/>e·∂ª/Œ£e·∂ª\"]]:::purple\n",
                "    end\n",
                "\n",
                "    subgraph Properties[\"Key Properties\"]\n",
                "        direction TB\n",
                "        P1[Gradient Preservation] -->|ReLU > Sigmoid| P2[Fights Vanishing Gradients]\n",
                "        P3[Output Range] -->|Sigmoid: 0-1<br/>Tanh: -1-1| P4[Task-Specific Suitability]\n",
                "    end\n",
                "\n",
                "    Roles -->|Enables| Properties\n",
                "    Types -->|Determine| Properties\n",
                "\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef orange fill:#ffebcc,stroke:#ff9900\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef purple fill:#f0e6ff,stroke:#6600cc\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **2. Vanishing Gradient Diagram**  \n",
                "**Focus:** Problem visualization and solutions  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Problem[\"Vanishing Gradient Phenomenon\"]\n",
                "        direction BT\n",
                "        L4[Output Layer] -->|Small Gradient| L3\n",
                "        L3[Layer 3] -->|Diminished| L2\n",
                "        L2[Layer 2] -->|Tiny Gradient| L1[Input Layer]\n",
                "        style L1 stroke:#cc0000\n",
                "        style L2 stroke:#ff6666\n",
                "        style L3 stroke:#ff9999\n",
                "        style L4 stroke:#ffcccc\n",
                "    end\n",
                "\n",
                "    subgraph Solutions[\"Mitigation Strategies\"]\n",
                "        direction LR\n",
                "        S1[[\"ReLU Activation\"]]:::green\n",
                "        S2[[\"He Initialization\"]]:::blue\n",
                "        S3[[\"Residual Connections\"]]:::orange\n",
                "        S1 -->|Non-Zero Gradients| Fix\n",
                "        S2 -->|Proper Weight Scaling| Fix\n",
                "        S3 -->|Alternative Paths| Fix\n",
                "    end\n",
                "\n",
                "    Problem --> Solutions\n",
                "\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef orange fill:#ffebcc,stroke:#ff9900\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Advanced Activation & Gradient Control**  \n",
                "**Focus:** Modern variants and implementation  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'neutral', 'themeVariables': { 'fontSize': '12px'}}}%%\n",
                "flowchart LR\n",
                "    subgraph Activations[\"Improved Activations\"]\n",
                "        direction TB\n",
                "        A1[[\"Leaky ReLU<br/>max(Œ±z, z)\"]]:::green\n",
                "        A2[[\"ELU<br/>Œ±(e·∂ª-1) if z<0\"]]:::blue\n",
                "        A3[[\"SELU<br/>Œª‚ãÖELU(z)\"]]:::purple\n",
                "    end\n",
                "\n",
                "    subgraph Control[\"Gradient Control\"]\n",
                "        direction TB\n",
                "        C1[[\"Gradient Clipping<br/>if ‚Äñg‚Äñ > Œ∏: g = Œ∏g/‚Äñg‚Äñ\"]]:::orange\n",
                "        C2[[\"Weight Regularization<br/>L1/L2 Penalties\"]]:::yellow\n",
                "    end\n",
                "\n",
                "    subgraph Code[\"Implementation\"]\n",
                "        direction LR\n",
                "        P[[\"PyTorch:<br/>nn.LeakyReLU(0.01)\"]]:::pytorch\n",
                "        K[[\"Keras:<br/>tf.keras.layers.ELU()\"]]:::keras\n",
                "    end\n",
                "\n",
                "    Activations --> Control\n",
                "    Control --> Code\n",
                "\n",
                "    classDef green fill:#e6ffe6,stroke:#009900\n",
                "    classDef blue fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef purple fill:#f0e6ff,stroke:#6600cc\n",
                "    classDef orange fill:#ffebcc,stroke:#ff9900\n",
                "    classDef yellow fill:#ffffcc,stroke:#ffcc00\n",
                "    classDef pytorch fill:#ffe6e6,stroke:#cc0000\n",
                "    classDef keras fill:#e6f3ff,stroke:#0066cc\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"activation-functions-in-neural-networks\"></a>üßÆ Activation Functions in Neural Networks\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"role-of-activation-functions-in-deep-networks\"></a>üéØ Role of activation functions in deep networks\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"common-activation-functions-relu-sigmoid-tanh-softmax\"></a>üìä Common activation functions: ReLU, Sigmoid, Tanh, Softmax\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"why-activation-functions-are-crucial-for-learning-complex-patterns\"></a>üß† Why activation functions are crucial for learning complex patterns\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"vanishing-gradient-problem\"></a>‚ö†Ô∏è Vanishing Gradient Problem\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"what-is-vanishing-gradients-and-why-does-it-occur\"></a>‚ùì What is vanishing gradients and why does it occur?\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"how-the-vanishing-gradient-problem-affects-deep-neural-networks\"></a>üß± How the vanishing gradient problem affects deep neural networks\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"solutions-to-vanishing-gradients-eg-relu-he-initialization\"></a>üõ†Ô∏è Solutions to vanishing gradients (e.g., ReLU, He Initialization)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"improving-learning-with-activation-functions\"></a>üöÄ Improving Learning with Activation Functions\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"leaky-relu-elu-selu-and-their-advantages-over-traditional-relu\"></a>üåü Leaky ReLU, ELU, SELU and their advantages over traditional ReLU\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"exploding-gradients-and-gradient-clipping\"></a>üí• Exploding gradients and gradient clipping\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"implementing-these-solutions-in-both-pytorch-and-tensorflow\"></a>üß™ Implementing these solutions in both PyTorch and TensorFlow\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
