{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Table of Contents\n",
                "\n",
                "- [üîÑ Backpropagation Overview](#backpropagation-overview)\n",
                "  - [‚ùì What is backpropagation and how does it work?](#what-is-backpropagation-and-how-does-it-work)\n",
                "  - [üîó Understanding the chain rule in neural networks](#understanding-the-chain-rule-in-neural-networks)\n",
                "  - [üì• Steps in the forward pass and backward pass](#steps-in-the-forward-pass-and-backward-pass)\n",
                "- [üßÆ Autograd in PyTorch](#autograd-in-pytorch)\n",
                "  - [‚öôÔ∏è PyTorch‚Äôs autograd mechanism: how it calculates gradients automatically](#pytorchs-autograd-mechanism-how-it-calculates-gradients-automatically)\n",
                "  - [üìä How `autograd` tracks operations and computes derivatives](#how-autograd-tracks-operations-and-computes-derivatives)\n",
                "  - [üß™ Practical example: Training a network using autograd](#practical-example-training-a-network-using-autograd)\n",
                "- [üîß Custom Gradient Rules](#custom-gradient-rules)\n",
                "  - [üõ†Ô∏è Implementing custom backpropagation rules in PyTorch](#implementing-custom-backpropagation-rules-in-pytorch)\n",
                "  - [üß∞ Using `torch.autograd.Function` for custom gradient computation](#using-torchautogradfunction-for-custom-gradient-computation)\n",
                "  - [üß™ Example of defining custom gradient calculations](#example-of-defining-custom-gradient-calculations)\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### **1. Backpropagation Overview (Fixed)**  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Forward[\"Forward Pass (Data Flow)\"]\n",
                "        direction LR\n",
                "        X[Input] -->|\"Linear: W‚ÇÅ¬∑X + b‚ÇÅ\"| L1[Hidden Layer]\n",
                "        L1 -->|\"ReLU: œÉ(z)\"| A1[Activation]\n",
                "        A1 -->|\"Linear: W‚ÇÇ¬∑A1 + b‚ÇÇ\"| Y[Output]\n",
                "        Y --> Loss[[\"Loss = MSE(Y, Y_true\"]]\n",
                "    end\n",
                "\n",
                "    subgraph Backward[\"Backward Pass (Gradient Flow)\"]\n",
                "        direction RL\n",
                "        Loss -.-|‚àÇLoss/‚àÇY| Y\n",
                "        Y -.-|\"‚àÇLoss/‚àÇW‚ÇÇ = ‚àÇLoss/‚àÇY ¬∑ A‚ÇÅ·µÄ\"| W2[Weight W‚ÇÇ]\n",
                "        Y -.-|\"‚àÇLoss/‚àÇA1 = W‚ÇÇ·µÄ ¬∑ ‚àÇLoss/‚àÇY\"| A1\n",
                "        A1 -.-|\"‚àÇLoss/‚àÇL1 = ‚àÇLoss/‚àÇA1 ¬∑ œÉ'(z)\"| L1\n",
                "        L1 -.-|\"‚àÇLoss/‚àÇW‚ÇÅ = ‚àÇLoss/‚àÇL1 ¬∑ X·µÄ\"| W1[Weight W‚ÇÅ]\n",
                "    end\n",
                "\n",
                "    classDef forward fill:#e6f3ff,stroke:#0066cc\n",
                "    classDef backward fill:#ffe6e6,stroke:#cc0000,stroke-dasharray:5,5\n",
                "    linkStyle 4,5,6,7,8 stroke:#cc0000,stroke-dasharray:5,5\n",
                "```\n",
                "\n",
                "### **2. Autograd in PyTorch (Fixed)**  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Autograd[\"PyTorch Autograd Engine\"]\n",
                "        direction TB\n",
                "        x[(Input Tensor<br/>requires_grad=True)] -->|\"MatMul: W¬∑x\"| z[Pre-activation]\n",
                "        z -->|ReLU| a[Activation]\n",
                "        a --> Loss[[\"Loss = (a - y)¬≤\"]]\n",
                "\n",
                "        %% Gradient Computation\n",
                "        Loss -->|\"loss.backward()\"| Grads[[\"Gradients:<br/>‚àÇLoss/‚àÇW, ‚àÇLoss/‚àÇx\"]]\n",
                "        Grads -->|\"optimizer.step()\"| Update[Weight Update]\n",
                "\n",
                "        style x stroke:#009900\n",
                "        style Grads stroke:#cc0000\n",
                "    end\n",
                "\n",
                "    classDef tensor fill:#f0f0f0,stroke:#666\n",
                "    classDef op fill:#e6ffe6,stroke:#009900\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### **3. Custom Gradient Rules**  \n",
                "**Focus:** Defining custom backward logic with `torch.autograd.Function`  \n",
                "```mermaid\n",
                "%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '12px'}}}%%\n",
                "flowchart TD\n",
                "    subgraph Custom[\"Custom Function Workflow\"]\n",
                "        direction TB\n",
                "        subgraph Function[\"CustomFunction(torch.autograd.Function)\"]\n",
                "            direction LR\n",
                "            Forward[[\"forward(ctx, x):\n",
                "  ctx.save_for_backward(x)\n",
                "  return x * 2\"]] --> Backward[[\"backward(ctx, grad):\n",
                "  x, = ctx.saved_tensors\n",
                "  return grad * 3\"]]\n",
                "        end\n",
                "\n",
                "        Input[Input Tensor] -->|CustomFunction.apply| Output[Output Tensor]\n",
                "        Output --> Loss\n",
                "        Loss -->|Backward| CustomGrad[[\"Custom Gradient: 3 √ó grad\"]]\n",
                "\n",
                "        style Forward fill:#f0f0f0,stroke:#666\n",
                "        style Backward fill:#ffe6e6,stroke:#cc0000\n",
                "    end\n",
                "\n",
                "    classDef code fill:#f0f0f0,stroke:#666\n",
                "    classDef grad fill:#ffe6e6,stroke:#cc0000\n",
                "```\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"backpropagation-overview\"></a>üîÑ Backpropagation Overview\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# <a id=\"what-is-backpropagation-and-how-does-it-work\"></a>‚ùì What is backpropagation and how does it work?\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"understanding-the-chain-rule-in-neural-networks\"></a>üîó Understanding the chain rule in neural networks\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"steps-in-the-forward-pass-and-backward-pass\"></a>üì• Steps in the forward pass and backward pass\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"autograd-in-pytorch\"></a>üßÆ Autograd in PyTorch\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"pytorchs-autograd-mechanism-how-it-calculates-gradients-automatically\"></a>‚öôÔ∏è PyTorch‚Äôs autograd mechanism: how it calculates gradients automatically\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"how-autograd-tracks-operations-and-computes-derivatives\"></a>üìä How `autograd` tracks operations and computes derivatives\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"practical-example-training-a-network-using-autograd\"></a>üß™ Practical example: Training a network using autograd\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"custom-gradient-rules\"></a>üîß Custom Gradient Rules\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"implementing-custom-backpropagation-rules-in-pytorch\"></a>üõ†Ô∏è Implementing custom backpropagation rules in PyTorch\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"using-torchautogradfunction-for-custom-gradient-computation\"></a>üß∞ Using `torch.autograd.Function` for custom gradient computation\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <a id=\"example-of-defining-custom-gradient-calculations\"></a>üß™ Example of defining custom gradient calculations\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
