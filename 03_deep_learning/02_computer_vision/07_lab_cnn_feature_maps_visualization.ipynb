{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933956ed",
   "metadata": {},
   "source": [
    "Locked in. Next lab is from:  \n",
    "### ðŸ“ `02_computer_vision`  \n",
    "Letâ€™s go deep and visual ðŸ‘ï¸\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  `07_lab_cnn_feature_maps_visualization.ipynb`  \n",
    "### ðŸ§ª Purpose:  \n",
    "> **Visualize how CNNs \"see\"** â€” layer-by-layer.  \n",
    "Inspect learned filters and activation maps to gain intuition on what convolutional layers extract at different depths.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Target Setup\n",
    "\n",
    "| Requirement        | Designed For     |\n",
    "|--------------------|------------------|\n",
    "| Device             | CPU / Colab GPU (T4) âœ…  \n",
    "| RAM                | < 2GB âœ…  \n",
    "| Libs               | PyTorch, matplotlib âœ…  \n",
    "| Dataset            | CIFAR-10 (built-in) âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Step-by-Step Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Section 1: Imports & Setup\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ–¼ï¸ Section 2: Load CIFAR-10 Sample\n",
    "\n",
    "```python\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "classes = testset.classes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Section 3: Load Pretrained CNN (e.g., ResNet18)\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¬ Section 4: Hook to Extract Feature Maps\n",
    "\n",
    "```python\n",
    "feature_maps = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_maps[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook on early conv layer\n",
    "model.layer1[0].conv1.register_forward_hook(get_activation(\"layer1.0.conv1\"))\n",
    "model.layer2[0].conv1.register_forward_hook(get_activation(\"layer2.0.conv1\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ–¼ï¸ Section 5: Visualize Input Image\n",
    "\n",
    "```python\n",
    "# Load sample\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Unnormalize for display\n",
    "img = images[0].permute(1, 2, 0) * 0.5 + 0.5\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Input Image: {classes[labels[0]]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¡ Section 6: Forward Pass & Capture\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    _ = model(images)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¨ Section 7: Visualize Feature Maps\n",
    "\n",
    "```python\n",
    "def show_feature_maps(layer_name, num_channels=6):\n",
    "    fmap = feature_maps[layer_name][0]  # [C, H, W]\n",
    "    fig, axes = plt.subplots(1, num_channels, figsize=(15, 5))\n",
    "    for i in range(num_channels):\n",
    "        axes[i].imshow(fmap[i].cpu(), cmap='viridis')\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Filter {i}\")\n",
    "    plt.suptitle(f\"Feature Maps - {layer_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Try:\n",
    "show_feature_maps(\"layer1.0.conv1\", num_channels=6)\n",
    "show_feature_maps(\"layer2.0.conv1\", num_channels=6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Bonus: Visualize Filters Themselves (Weights)\n",
    "\n",
    "```python\n",
    "# Extract filters from conv1\n",
    "filters = model.conv1.weight.data.clone()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i in range(16):\n",
    "    f = filters[i]\n",
    "    f_min, f_max = f.min(), f.max()\n",
    "    f_img = (f - f_min) / (f_max - f_min)\n",
    "    axes[i // 4, i % 4].imshow(f_img.permute(1, 2, 0).cpu())\n",
    "    axes[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"First Conv Layer Filters\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Checklist\n",
    "\n",
    "| Concept                  | Covered |\n",
    "|--------------------------|---------|\n",
    "| Hooking into layers      | âœ…       |\n",
    "| Forward-pass inspection  | âœ…       |\n",
    "| Visualizing activations  | âœ…       |\n",
    "| Understanding filters    | âœ…       |\n",
    "| No GPU dependency        | âœ…       |\n",
    "| Colab compatible         | âœ…       |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- CNNs extract **low-level patterns** in early layers (edges, colors)  \n",
    "- Deeper layers encode **shapes, textures, semantics**  \n",
    "- Hooks allow **peeking inside the model brain**\n",
    "\n",
    "---\n",
    "\n",
    "Ready for `08_lab_data_augmentation_comparison.ipynb` next?  \n",
    "We'll test flips, cutout, mixup â€” and actually **track how accuracy shifts**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
