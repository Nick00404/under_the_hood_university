# 04 Advanced Architectures

- [01 graph neural networks with pyg dgl](./01_graph_neural_networks_with_pyg_dgl.ipynb)
- [02 memory augmented nets neural turing machines](./02_memory_augmented_nets_neural_turing_machines.ipynb)
- [03 meta learning maml prototypical nets](./03_meta_learning_maml_prototypical_nets.ipynb)
- [04 attention free architectures mlp mixer](./04_attention_free_architectures_mlp_mixer.ipynb)
- [05 spiking neural nets surrogate gradients](./05_spiking_neural_nets_surrogate_gradients.ipynb)
- [06 diffusion models for generation](./06_diffusion_models_for_generation.ipynb)
- [ 07 lab gnn_node classification with cora.ipynb ](./07_lab_gnn_node_classification_with_cora.ipynb)  
- [ 08 lab memory augmented net tiny tasks.ipynb ](./08_lab_memory_augmented_net_tiny_tasks.ipynb)  
- [ 09 lab diffusion model toy image gen.ipynb ](./09_lab_diffusion_model_toy_image_gen.ipynb)  
---

## ğŸ“˜ **Advanced Deep Learning â€“ Structured Index**

---

### ğŸ§© **01. Graph Neural Networks (GNNs) with PyG and DGL**

#### ğŸ“Œ **Subtopics:**
- **Introduction to Graph Neural Networks (GNNs)**
  - Understanding the concept of graphs in deep learning
  - Applications of GNNs: Social networks, recommendation systems, molecular graphs, etc.
- **PyTorch Geometric (PyG) and Deep Graph Library (DGL)**
  - Overview of PyG and DGL for building GNNs
  - How to implement basic GNN models using PyG and DGL
  - Example: Graph classification and node classification with PyG/DGL
- **Graph Convolutional Networks (GCNs)**
  - Convolution operations on graph structures
  - The spectral and spatial perspectives of GCNs
  - Example: Implementing a simple GCN for node classification

---

### ğŸ§© **02. Memory-Augmented Networks and Neural Turing Machines**

#### ğŸ“Œ **Subtopics:**
- **Memory-Augmented Networks (MANNs)**
  - What are memory-augmented networks, and why do they require external memory?
  - Applications: Tasks that require reasoning and memory beyond traditional neural networks
- **Neural Turing Machines (NTMs)**
  - Architecture of NTMs: Neural networks with a differentiable memory matrix
  - How NTMs solve problems like algorithmic tasks and memory-based reasoning
  - Example: Implementing a simple Neural Turing Machine using PyTorch
- **Memory Networks and Their Applications**
  - Overview of memory networks and how they store and retrieve information
  - How they perform well in NLP and question answering tasks
  - Example: Implementing a memory network for QA systems

---

### ğŸ§© **03. Meta-Learning: MAML and Prototypical Networks**

#### ğŸ“Œ **Subtopics:**
- **Introduction to Meta-Learning**
  - What is meta-learning, and why is it important for few-shot learning?
  - Applications of meta-learning in real-world tasks like robotics, computer vision, and NLP
- **Model-Agnostic Meta-Learning (MAML)**
  - Overview of MAML for few-shot learning
  - How MAML trains models to adapt quickly to new tasks with minimal data
  - Example: Implementing MAML for few-shot classification in PyTorch
- **Prototypical Networks for Few-Shot Learning**
  - Overview of prototypical networks and how they learn representations for few-shot learning
  - How prototypical networks compare to other meta-learning methods
  - Example: Implementing a Prototypical Network for few-shot classification tasks

---

### ğŸ§© **04. Attention-Free Architectures: MLP-Mixer**

#### ğŸ“Œ **Subtopics:**
- **Introduction to Attention-Free Architectures**
  - The need for attention mechanisms in traditional architectures (transformers) vs. attention-free models
  - How attention-free models aim to reduce computational complexity
- **MLP-Mixer Architecture**
  - Overview of MLP-Mixer and its key components: Token-mixing and channel-mixing layers
  - Comparison to transformer-based models in terms of performance and efficiency
  - Example: Implementing MLP-Mixer for image classification
- **Alternatives to Attention Mechanisms**
  - How other attention-free methods (e.g., ConvNeXt) compare to traditional attention-based models
  - Pros and cons of attention-free models for NLP and computer vision tasks

---

### ğŸ§© **05. Spiking Neural Networks (SNNs) and Surrogate Gradients**

#### ğŸ“Œ **Subtopics:**
- **Introduction to Spiking Neural Networks (SNNs)**
  - The biological inspiration behind SNNs
  - How SNNs differ from traditional neural networks in terms of information processing (spikes and time-based events)
  - Applications of SNNs in neuroscience and neuromorphic computing
- **Surrogate Gradients for SNN Training**
  - Challenges of training SNNs with traditional backpropagation
  - Introduction to surrogate gradients and how they allow gradient-based learning for SNNs
  - Example: Implementing SNNs with surrogate gradients for simple tasks like classification
- **Neuromorphic Computing**
  - Overview of neuromorphic computing and its role in AI
  - How SNNs are used in edge devices and low-power systems
  - The future of SNNs and their role in AI research

---

### ğŸ§© **06. Diffusion Models for Image Generation**

#### ğŸ“Œ **Subtopics:**
- **Introduction to Diffusion Models**
  - What are diffusion models, and how do they generate data (e.g., images)?
  - Comparison to other generative models like GANs and VAEs
  - How diffusion models iteratively denoise noisy data to produce high-quality outputs
- **Training and Sampling with Diffusion Models**
  - Overview of the training process: noise schedules, forward and reverse diffusion
  - Sampling from diffusion models using various techniques (e.g., Langevin dynamics)
  - Example: Implementing a basic diffusion model for image generation
- **Applications of Diffusion Models**
  - Use cases: Image generation, inpainting, and super-resolution
  - How diffusion models have outperformed GANs in some tasks
  - Example: Comparing diffusion models with GANs on image generation tasks

---

### ğŸ§  **Bonus:**
- **Emerging Trends in Deep Learning**
  - Explore how advanced architectures like Graph Neural Networks, Memory-Augmented Networks, and Spiking Neural Networks are evolving.
  - Dive deeper into cutting-edge topics in AI, such as meta-learning, attention-free models, and diffusion models.
- **Real-World Applications and Research Directions**
  - Use cases in fields like robotics, self-driving cars, healthcare, and personalized AI.
  - The future of AI: From neuromorphic computing to scalable, energy-efficient models.

---



















You're absolutely on a roll with this final piece of the deep learning puzzle. Here's your fully polished **Table of Contents with anchor links** and **section headers with HTML anchor IDs** for **Advanced Deep Learning â€“ Structured Index** â€” Jupyter-ready and loaded with clarity and clean navigation.

---

## âœ… Table of Contents â€“ Advanced Deep Learning

```markdown
## ğŸ§­ Table of Contents â€“ Advanced Deep Learning

### ğŸ§© [01. Graph Neural Networks (GNNs) with PyG and DGL](#gnns)
- ğŸ”— [Introduction to GNNs](#gnn-intro)
- ğŸ› ï¸ [PyG and DGL for GNNs](#pyg-dgl)
- ğŸŒ [Graph Convolutional Networks (GCNs)](#gcns)

### ğŸ§© [02. Memory-Augmented Networks and Neural Turing Machines](#manns)
- ğŸ§  [Memory-Augmented Networks (MANNs)](#manns-intro)
- ğŸ’¾ [Neural Turing Machines (NTMs)](#ntms)
- ğŸ§­ [Memory Networks](#memory-networks)

### ğŸ§© [03. Meta-Learning: MAML and Prototypical Networks](#meta-learning)
- ğŸ§  [Introduction to Meta-Learning](#meta-intro)
- ğŸ” [MAML](#maml)
- ğŸ§© [Prototypical Networks](#proto-nets)

### ğŸ§© [04. Attention-Free Architectures: MLP-Mixer](#mlp-mixer)
- ğŸ‘ï¸â€ğŸ—¨ï¸ [Intro to Attention-Free Architectures](#attention-free)
- ğŸ§± [MLP-Mixer Architecture](#mlp-mixer-arch)
- ğŸ§ª [Alternatives to Attention](#attention-alt)

### ğŸ§© [05. Spiking Neural Networks (SNNs) and Surrogate Gradients](#snns)
- ğŸ§  [Intro to Spiking Neural Networks](#snns-intro)
- ğŸ“‰ [Surrogate Gradients](#surrogate)
- âš¡ [Neuromorphic Computing](#neuromorphic)

### ğŸ§© [06. Diffusion Models for Image Generation](#diffusion)
- ğŸŒ«ï¸ [Intro to Diffusion Models](#diffusion-intro)
- ğŸ”„ [Training and Sampling](#diffusion-train)
- ğŸ–¼ï¸ [Applications of Diffusion Models](#diffusion-apps)
```

---

## ğŸ§© Section Headers with Anchor Tags

```markdown
### ğŸ§© <a id="gnns"></a>01. Graph Neural Networks (GNNs) with PyG and DGL

#### <a id="gnn-intro"></a>ğŸ”— Introduction to Graph Neural Networks  
- Graphs in deep learning  
- GNN applications  

#### <a id="pyg-dgl"></a>ğŸ› ï¸ PyTorch Geometric (PyG) and Deep Graph Library (DGL)  
- Using PyG and DGL  
- Graph/node classification examples  

#### <a id="gcns"></a>ğŸŒ Graph Convolutional Networks (GCNs)  
- Convolutions on graphs  
- Spectral vs spatial methods  

---

### ğŸ§© <a id="manns"></a>02. Memory-Augmented Networks and Neural Turing Machines

#### <a id="manns-intro"></a>ğŸ§  Memory-Augmented Networks (MANNs)  
- Reasoning with external memory  
- Applications in complex tasks  

#### <a id="ntms"></a>ğŸ’¾ Neural Turing Machines (NTMs)  
- Differentiable memory  
- Algorithmic tasks  

#### <a id="memory-networks"></a>ğŸ§­ Memory Networks  
- QA systems  
- Info retrieval  

---

### ğŸ§© <a id="meta-learning"></a>03. Meta-Learning: MAML and Prototypical Networks

#### <a id="meta-intro"></a>ğŸ§  Introduction to Meta-Learning  
- Few-shot learning  
- Real-world applications  

#### <a id="maml"></a>ğŸ” Model-Agnostic Meta-Learning (MAML)  
- Fast adaptation  
- PyTorch implementation  

#### <a id="proto-nets"></a>ğŸ§© Prototypical Networks  
- Embedding-based learning  
- Few-shot classification  

---

### ğŸ§© <a id="mlp-mixer"></a>04. Attention-Free Architectures: MLP-Mixer

#### <a id="attention-free"></a>ğŸ‘ï¸â€ğŸ—¨ï¸ Intro to Attention-Free Architectures  
- Why avoid attention  
- Simplicity & speed  

#### <a id="mlp-mixer-arch"></a>ğŸ§± MLP-Mixer Architecture  
- Token-mixing  
- Channel-mixing  
- Image classification  

#### <a id="attention-alt"></a>ğŸ§ª Alternatives to Attention  
- ConvNeXt  
- Use in vision & NLP  

---

### ğŸ§© <a id="snns"></a>05. Spiking Neural Networks (SNNs) and Surrogate Gradients

#### <a id="snns-intro"></a>ğŸ§  Introduction to SNNs  
- Spikes & temporal dynamics  
- Applications in edge AI  

#### <a id="surrogate"></a>ğŸ“‰ Surrogate Gradients for SNN Training  
- Differentiability workaround  
- PyTorch example  

#### <a id="neuromorphic"></a>âš¡ Neuromorphic Computing  
- AI on silicon  
- Low-power inference  

---

### ğŸ§© <a id="diffusion"></a>06. Diffusion Models for Image Generation

#### <a id="diffusion-intro"></a>ğŸŒ«ï¸ Introduction to Diffusion Models  
- Iterative denoising  
- Diffusion vs GANs/VAEs  

#### <a id="diffusion-train"></a>ğŸ”„ Training and Sampling  
- Noise schedule  
- Sampling strategies  

#### <a id="diffusion-apps"></a>ğŸ–¼ï¸ Applications of Diffusion Models  
- Image generation  
- Inpainting & super-resolution  
```

---
