{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8dd5aa0",
   "metadata": {},
   "source": [
    "ðŸ•’ Time to **track time** like a sniper.  \n",
    "If your model starts lagging â€” even by a second â€” it could wreck **user experience, SLAs, or your production pipeline**.  \n",
    "Letâ€™s measure every millisecond. Letâ€™s set traps for latency spikes. Letâ€™s roll.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `10_lab_monitor_model_latency_with_prometheus.ipynb`  \n",
    "### ðŸ“ `06_mlops/05_model_monitoring`  \n",
    "> Instrument your modelâ€™s **inference latency** using **Prometheus Histograms**,  \n",
    "then trigger alerts when **response time exceeds SLA**.  \n",
    "This lab is **response time surveillance on steroids**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Track **inference latency** using Prometheus  \n",
    "- Use **Histogram buckets** for percentile analysis  \n",
    "- Simulate a **lag spike** and catch it live  \n",
    "- Prep for alerting based on **95th percentile thresholds**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Setup\n",
    "\n",
    "| Component     | Spec                |\n",
    "|---------------|---------------------|\n",
    "| API           | Flask âœ…  \n",
    "| Monitoring    | Prometheus âœ…  \n",
    "| Metrics       | `Histogram` for latency âœ…  \n",
    "| Visuals       | Dashboard or curl logs âœ…  \n",
    "| Platform      | Localhost / Docker âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Section 1: Flask API with Latency Metric\n",
    "\n",
    "```python\n",
    "from flask import Flask, jsonify\n",
    "from prometheus_client import start_http_server, Histogram\n",
    "import time, random\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define latency metric\n",
    "latency_hist = Histogram(\n",
    "    \"model_latency_seconds\",\n",
    "    \"Inference latency (s)\",\n",
    "    buckets=[0.1, 0.3, 0.5, 0.7, 1, 2, 5]\n",
    ")\n",
    "\n",
    "@app.route(\"/predict\")\n",
    "@latency_hist.time()\n",
    "def predict():\n",
    "    delay = random.choice([0.2, 0.5, 2.0])  # simulate variable latency\n",
    "    time.sleep(delay)\n",
    "    return jsonify({\"latency\": delay})\n",
    "\n",
    "# Start Prometheus metric endpoint on port 8000\n",
    "start_http_server(8000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Section 2: Prometheus Scrape Config\n",
    "\n",
    "```yaml\n",
    "scrape_configs:\n",
    "  - job_name: 'model_latency'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:8000']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¨ Section 3: Alerting Rule (95th Percentile)\n",
    "\n",
    "```yaml\n",
    "groups:\n",
    "- name: latency_alerts\n",
    "  rules:\n",
    "  - alert: HighLatency95thPercentile\n",
    "    expr: histogram_quantile(0.95, rate(model_latency_seconds_bucket[1m])) > 1\n",
    "    for: 30s\n",
    "    labels:\n",
    "      severity: warning\n",
    "    annotations:\n",
    "      summary: \"ðŸš¨ 95th percentile latency > 1s\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 4: View Metrics Live\n",
    "\n",
    "Visit:\n",
    "```\n",
    "http://localhost:8000/metrics\n",
    "```\n",
    "\n",
    "Query live in Prometheus UI:\n",
    "```prometheus\n",
    "histogram_quantile(0.95, rate(model_latency_seconds_bucket[1m]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Section 5: Simulate Spike and Alert\n",
    "\n",
    "1. Make repeated requests:\n",
    "```bash\n",
    "curl http://localhost:5000/predict\n",
    "```\n",
    "\n",
    "2. Watch latency climb and alert fire (if Alertmanager hooked).\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Recap\n",
    "\n",
    "| Feature                          | âœ… |\n",
    "|----------------------------------|----|\n",
    "| Latency histogram implemented    | âœ… |\n",
    "| Prometheus query for P95 latency| âœ… |\n",
    "| Alert fired on SLA breach        | âœ… |\n",
    "| Fully local + portable setup     | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- Histograms = perfect tool for latency buckets  \n",
    "- You can **monitor response time distributions**, not just averages  \n",
    "- Prometheus + Flask = **production-grade latency tracking**  \n",
    "- Combine this with Grafana, and you're Netflix-tier ðŸ‘‘\n",
    "\n",
    "---\n",
    "\n",
    "Next lab is a beast:  \n",
    "> `11_lab_concurrent_traffic_with_locust.ipynb`  \n",
    "Simulate **hundreds of users hitting your model server at once**  \n",
    "â†’ See how latency, failure rates, and throughput hold up under fire.  \n",
    "Shall we scale-test like legends?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
