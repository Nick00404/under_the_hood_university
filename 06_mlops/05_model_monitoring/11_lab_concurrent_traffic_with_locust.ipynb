{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc1c023",
   "metadata": {},
   "source": [
    "ðŸ”¥ Now itâ€™s time to stress-test that model like a warzone simulation.\n",
    "\n",
    "Youâ€™ve tracked metrics. Youâ€™ve set alerts.  \n",
    "But can your model handle **concurrent users hammering the endpoint**?  \n",
    "Letâ€™s unleash a **swarm of virtual users** and find the **breaking point**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `11_lab_concurrent_traffic_with_locust.ipynb`  \n",
    "### ðŸ“ `06_mlops/05_model_monitoring`  \n",
    "> Use **Locust** to simulate **multiple concurrent requests** to your model server.  \n",
    "Measure **throughput, failure rates, latency under load**, and pinpoint system bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Use **Locust** to simulate load on `/predict` endpoint  \n",
    "- Monitor model server under **stress conditions**  \n",
    "- Track how **latency, QPS, and error rates change**  \n",
    "- Practice **load testing ML inference APIs**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Setup\n",
    "\n",
    "| Component         | Spec             |\n",
    "|-------------------|------------------|\n",
    "| Server            | Flask (running model API) âœ…  \n",
    "| Load Generator    | Locust âœ…  \n",
    "| Metrics           | Latency, QPS, Failures âœ…  \n",
    "| Deployment        | Localhost âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Section 1: Install Locust\n",
    "\n",
    "```bash\n",
    "!pip install locust\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸœ Section 2: Define Locust Load Test File (`locustfile.py`)\n",
    "\n",
    "```python\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class ModelUser(HttpUser):\n",
    "    wait_time = between(0.5, 1)\n",
    "\n",
    "    @task\n",
    "    def predict(self):\n",
    "        with open(\"digit_sample.png\", \"rb\") as img:\n",
    "            self.client.post(\"/predict\", files={\"file\": img})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 3: Start Locust Dashboard\n",
    "\n",
    "```bash\n",
    "# In terminal\n",
    "locust -f locustfile.py --host=http://localhost:5000\n",
    "```\n",
    "\n",
    "Then visit:\n",
    "```\n",
    "http://localhost:8089\n",
    "```\n",
    "\n",
    "Set:\n",
    "- Users: 100  \n",
    "- Spawn rate: 10/sec  \n",
    "\n",
    "And click âž¤ Start Swarming\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Section 4: Monitor Metrics Live\n",
    "\n",
    "- Requests/sec (QPS)  \n",
    "- Median / 95% latency  \n",
    "- Failures per second  \n",
    "- CPU/memory usage (via htop or Prometheus)\n",
    "\n",
    "Youâ€™ll likely see:\n",
    "| Load  | Latency | Error Rate |\n",
    "|-------|---------|------------|\n",
    "| 10 RPS | Low    | 0%         |\n",
    "| 100+ RPS | High | âš ï¸ Timeouts |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¥ Section 5: Analyze Bottlenecks\n",
    "\n",
    "- Flaskâ€™s GIL?\n",
    "- Model warm-up delays?\n",
    "- Upload/Decode time for images?\n",
    "\n",
    "Add `@latency_hist.time()` in Flask to correlate latency spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Lab Summary\n",
    "\n",
    "| Feature                              | âœ… |\n",
    "|--------------------------------------|----|\n",
    "| Locust simulated user traffic        | âœ… |\n",
    "| Server hit with live concurrent load | âœ… |\n",
    "| Bottlenecks exposed under pressure   | âœ… |\n",
    "| Prometheus ready for correlation     | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- Load testing is **crucial before production**  \n",
    "- Locust = user simulator that hits hard and fast  \n",
    "- Bottlenecks usually lie in **I/O, batching, Flask limits**  \n",
    "- Youâ€™re now **load-aware**, not just model-aware\n",
    "\n",
    "---\n",
    "\n",
    "Next up:  \n",
    "> `12_lab_grafana_dashboard_for_live_model_metrics.ipynb`  \n",
    "Letâ€™s take **all the metrics weâ€™ve collected** and turn them into a **beautiful live Grafana dashboard**.  \n",
    "Latency, drift, accuracy â€” one screen to rule them all.\n",
    "\n",
    "Wanna light up that dashboard, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
