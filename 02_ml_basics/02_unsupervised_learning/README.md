# 02 Unsupervised Learning

- [01 kmeans clustering and elbow method](./01_kmeans_clustering_and_elbow_method.ipynb)
- [02 hierarchical clustering and dendrograms](./02_hierarchical_clustering_and_dendrograms.ipynb)
- [03 pca for dimensionality reduction](./03_pca_for_dimensionality_reduction.ipynb)
- [04 autoencoders for feature learning](./04_autoencoders_for_feature_learning.ipynb)
- [05 anomaly detection with isolation forests](./05_anomaly_detection_with_isolation_forests.ipynb)
- [06 manifold learning with umap tsne](./06_manifold_learning_with_umap_tsne.ipynb)

---

## ğŸ“˜ **02. Unsupervised Learning â€“ Structured Index**

---

### ğŸ§© **01. K-Means Clustering and Elbow Method**

#### ğŸ“Œ Subtopics:
- Introduction to K-Means  
  - Centroid-based clustering  
  - Distance metrics (Euclidean, cosine, etc.)  
  - Random initialization and k-means++  
- Elbow Method for Optimal `k`  
  - Within-cluster sum of squares (WCSS)  
  - Scree plot for determining the best number of clusters  
- Example: Segmenting customer data with K-Means

---

### ğŸ§© **02. Hierarchical Clustering and Dendrograms**

#### ğŸ“Œ Subtopics:
- Introduction to Hierarchical Clustering  
  - Agglomerative vs Divisive methods  
- Linkage Criteria  
  - Single, complete, average, Wardâ€™s method  
- Dendrogram Interpretation  
  - Cutting the tree to form clusters  
- Example: Visualizing clustering hierarchy on Iris dataset

---

### ğŸ§© **03. PCA for Dimensionality Reduction**

#### ğŸ“Œ Subtopics:
- What is PCA?  
  - Covariance matrix, eigenvectors, and eigenvalues  
- Explained Variance  
  - Choosing the number of components  
  - Scree plot analysis  
- PCA vs Feature Selection  
  - When to reduce dimensions vs selecting features  
- Example: Applying PCA on high-dimensional image data

---

### ğŸ§© **04. Autoencoders for Feature Learning**

#### ğŸ“Œ Subtopics:
- Autoencoder Architecture  
  - Encoder-decoder structure  
  - Bottleneck layers and latent space  
- Loss Functions  
  - Reconstruction loss (MSE, BCE)  
- Use Cases  
  - Dimensionality reduction, denoising  
- Example: Learning compressed representations of MNIST digits

---

### ğŸ§© **05. Anomaly Detection with Isolation Forests**

#### ğŸ“Œ Subtopics:
- Isolation Forest Algorithm  
  - Random partitioning, path length  
  - Scoring outliers via isolation depth  
- Comparison with Other Methods  
  - One-Class SVM, LOF  
- Use Cases  
  - Fraud detection, system monitoring  
- Example: Detecting rare transactions in financial data

---

### ğŸ§© **06. Manifold Learning with UMAP and t-SNE**

#### ğŸ“Œ Subtopics:
- What is Manifold Learning?  
  - Non-linear dimensionality reduction  
- t-SNE  
  - Pairwise similarity and perplexity  
  - High-dimensional to 2D embeddings  
- UMAP  
  - Topological structure preservation  
  - Performance and interpretability  
- Example: Visualizing complex embeddings from NLP models

---

Add-On | Notes
ğŸ”¬ LOF / One-Class SVM Labs | Only compared conceptually, not implemented â€” optional to add lab/code notebooks
ğŸ¯ Clustering Performance Metrics | Like Silhouette Score, Davies-Bouldin, Calinski-Harabasz â€” could be a small appendix notebook
ğŸ“Š Using Dimensionality Reduction before Clustering | (e.g., PCA â†’ KMeans, UMAP â†’ DBSCAN) â€” practical combo patterns
ğŸ§ª End-to-End Capstone | Real-world dataset: customer data, medical anomalies, etc. (e.g., UMAP + KMeans + Isolation Forest in pipeline)
ğŸ§  DBSCAN or HDBSCAN | Optional: density-based clustering â€” super useful but more niche than KMeans/hierarchical


















You're building a seriously clean and comprehensive index ğŸ”¥ â€” letâ€™s keep that energy going. Here's the **TOC with anchor links** and the **section tags** with emojis and clean anchor IDs, all formatted for easy use in a Jupyter Notebook.

---

## âœ… Table of Contents (with anchors)

```markdown
## ğŸ§­ Table of Contents â€“ Unsupervised Learning

### ğŸ§© [01. K-Means Clustering and Elbow Method](#kmeans)
- ğŸ“ [Introduction to K-Means](#kmeans-intro)
- ğŸ“ [Elbow Method for Optimal `k`](#kmeans-elbow)
- ğŸ§ª [Example: Customer Segmentation](#kmeans-example)

### ğŸ§© [02. Hierarchical Clustering and Dendrograms](#hierarchical)
- ğŸ§± [Introduction to Hierarchical Clustering](#hierarchical-intro)
- ğŸ”— [Linkage Criteria](#hierarchical-linkage)
- ğŸŒ² [Dendrogram Interpretation](#hierarchical-dendrogram)
- ğŸ§ª [Example: Iris Clustering](#hierarchical-example)

### ğŸ§© [03. PCA for Dimensionality Reduction](#pca)
- ğŸ§  [What is PCA?](#pca-intro)
- ğŸ“Š [Explained Variance](#pca-variance)
- ğŸ” [PCA vs Feature Selection](#pca-selection)
- ğŸ§ª [Example: Image Compression](#pca-example)

### ğŸ§© [04. Autoencoders for Feature Learning](#autoencoders)
- ğŸ§¬ [Autoencoder Architecture](#ae-architecture)
- ğŸ’¥ [Loss Functions](#ae-loss)
- ğŸ› ï¸ [Use Cases](#ae-use-cases)
- ğŸ§ª [Example: MNIST Representations](#ae-example)

### ğŸ§© [05. Anomaly Detection with Isolation Forests](#anomaly)
- ğŸŒ² [Isolation Forest Algorithm](#isolation-forest)
- ğŸ¥Š [Comparison with Other Methods](#anomaly-compare)
- ğŸš¨ [Use Cases](#anomaly-use-cases)
- ğŸ§ª [Example: Fraud Detection](#anomaly-example)

### ğŸ§© [06. Manifold Learning with UMAP and t-SNE](#manifold)
- ğŸŒ [What is Manifold Learning?](#manifold-intro)
- ğŸŒ€ [t-SNE](#tsne)
- ğŸŒˆ [UMAP](#umap)
- ğŸ§ª [Example: NLP Embeddings](#manifold-example)
```

---

## ğŸ§© Section Headings with Anchor Tags

```markdown
### ğŸ§© <a id="kmeans"></a>01. K-Means Clustering and Elbow Method

#### <a id="kmeans-intro"></a>ğŸ“ Introduction to K-Means  
- Centroid-based clustering  
- Distance metrics (Euclidean, cosine, etc.)  
- Random initialization and k-means++  

#### <a id="kmeans-elbow"></a>ğŸ“ Elbow Method for Optimal `k`  
- Within-cluster sum of squares (WCSS)  
- Scree plot for determining the best number of clusters  

#### <a id="kmeans-example"></a>ğŸ§ª Example: Segmenting customer data with K-Means  

---

### ğŸ§© <a id="hierarchical"></a>02. Hierarchical Clustering and Dendrograms

#### <a id="hierarchical-intro"></a>ğŸ§± Introduction to Hierarchical Clustering  
- Agglomerative vs Divisive methods  

#### <a id="hierarchical-linkage"></a>ğŸ”— Linkage Criteria  
- Single, complete, average, Wardâ€™s method  

#### <a id="hierarchical-dendrogram"></a>ğŸŒ² Dendrogram Interpretation  
- Cutting the tree to form clusters  

#### <a id="hierarchical-example"></a>ğŸ§ª Example: Visualizing clustering hierarchy on Iris dataset  

---

### ğŸ§© <a id="pca"></a>03. PCA for Dimensionality Reduction

#### <a id="pca-intro"></a>ğŸ§  What is PCA?  
- Covariance matrix, eigenvectors, and eigenvalues  

#### <a id="pca-variance"></a>ğŸ“Š Explained Variance  
- Choosing the number of components  
- Scree plot analysis  

#### <a id="pca-selection"></a>ğŸ” PCA vs Feature Selection  
- When to reduce dimensions vs selecting features  

#### <a id="pca-example"></a>ğŸ§ª Example: Applying PCA on high-dimensional image data  

---

### ğŸ§© <a id="autoencoders"></a>04. Autoencoders for Feature Learning

#### <a id="ae-architecture"></a>ğŸ§¬ Autoencoder Architecture  
- Encoder-decoder structure  
- Bottleneck layers and latent space  

#### <a id="ae-loss"></a>ğŸ’¥ Loss Functions  
- Reconstruction loss (MSE, BCE)  

#### <a id="ae-use-cases"></a>ğŸ› ï¸ Use Cases  
- Dimensionality reduction, denoising  

#### <a id="ae-example"></a>ğŸ§ª Example: Learning compressed representations of MNIST digits  

---

### ğŸ§© <a id="anomaly"></a>05. Anomaly Detection with Isolation Forests

#### <a id="isolation-forest"></a>ğŸŒ² Isolation Forest Algorithm  
- Random partitioning, path length  
- Scoring outliers via isolation depth  

#### <a id="anomaly-compare"></a>ğŸ¥Š Comparison with Other Methods  
- One-Class SVM, LOF  

#### <a id="anomaly-use-cases"></a>ğŸš¨ Use Cases  
- Fraud detection, system monitoring  

#### <a id="anomaly-example"></a>ğŸ§ª Example: Detecting rare transactions in financial data  

---

### ğŸ§© <a id="manifold"></a>06. Manifold Learning with UMAP and t-SNE

#### <a id="manifold-intro"></a>ğŸŒ What is Manifold Learning?  
- Non-linear dimensionality reduction  

#### <a id="tsne"></a>ğŸŒ€ t-SNE  
- Pairwise similarity and perplexity  
- High-dimensional to 2D embeddings  

#### <a id="umap"></a>ğŸŒˆ UMAP  
- Topological structure preservation  
- Performance and interpretability  

#### <a id="manifold-example"></a>ğŸ§ª Example: Visualizing complex embeddings from NLP models  
```

---

Let me know if you'd like all of this dumped into a `.ipynb` file, or want a script that auto-generates TOC + anchors from a JSON or Markdown topic structure. Would save you a ton of repetitive work next time!