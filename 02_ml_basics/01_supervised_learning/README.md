# 01 Supervised Learning

- [01 linear regression and cost functions](./01_linear_regression_and_cost_functions.ipynb)
- [02 logistic regression and classification metrics](./02_logistic_regression_and_classification_metrics.ipynb)
- [03 decision trees and ensemble methods](./03_decision_trees_and_ensemble_methods.ipynb)
- [04 svm and kernel tricks for nonlinear data](./04_svm_and_kernel_tricks_for_nonlinear_data.ipynb)
- [05 regularization l1 l2 elasticnet](./05_regularization_l1_l2_elasticnet.ipynb)
- [06 bayesian models and naive bayes](./06_bayesian_models_and_naive_bayes.ipynb)



## ğŸ“˜ `01_linear_regression_and_cost_functions.ipynb`

### ğŸ§© **1. Building Blocks of Linear Regression**
- Hypothesis Function
- Line Fitting (Geometric intuition)
- Assumptions of Linear Models

### ğŸ§© **2. Cost Function & Optimization**
- Squared Error / MSE
- Gradient Descent (Single variable)
- Gradient Descent (Multivariable)
- Vectorization for Speedup

### ğŸ§© **3. Evaluation & Interpretation**
- RÂ² Score
- Underfitting & Model Diagnostics
- Visualizing Cost Surface

---

## ğŸ“˜ `02_logistic_regression_and_classification_metrics.ipynb`

### ğŸ§© **1. Understanding Logistic Regression**
- Binary Classification Motivation
- Sigmoid Function & Probability Output
- Decision Boundary Interpretation

### ğŸ§© **2. Training the Model**
- Cost Function for Logistic Regression
- Gradient Descent for Logistic Regression
- Feature Scaling

### ğŸ§© **3. Evaluation & Performance**
- Accuracy, Precision, Recall, F1
- Confusion Matrix
- ROC Curve & AUC
- Overfitting in Classification Models

---

## ğŸ“˜ `03_decision_trees_and_ensemble_methods.ipynb`

### ğŸ§© **1. Decision Trees Explained**
- Splitting Criteria: Gini vs Entropy
- Tree Depth & Pruning
- Overfitting in Trees

### ğŸ§© **2. Ensemble Techniques**
- Bagging & Random Forests
- Feature Importance
- Boosting Basics (GBM, XGBoost)

### ğŸ§© **3. Model Tuning & Comparison**
- Hyperparameters for Forests & Boosters
- When to Use Trees vs Linear Models
- Bias-Variance Tradeoff Visualized

---

## ğŸ“˜ `04_svm_and_kernel_tricks_for_nonlinear_data.ipynb`

### ğŸ§© **1. Core Concepts of SVM**
- Max-Margin Intuition
- Hard vs Soft Margins
- Hinge Loss Function

### ğŸ§© **2. Going Nonlinear with Kernels**
- Polynomial & RBF Kernels
- Visualizing Transformations
- Kernelized Decision Boundaries

### ğŸ§© **3. Practical Usage**
- Parameter Tuning: C and Gamma
- Linear vs Non-linear SVM
- Comparison with Logistic Regression

---

## ğŸ“˜ `05_regularization_l1_l2_elasticnet.ipynb`

### ğŸ§© **1. Motivation & Math of Regularization**
- Overfitting Intuition
- Regularized Cost Functions
- Effect of Î» on Loss

### ğŸ§© **2. Types of Regularization**
- L2 (Ridge)
- L1 (Lasso)
- ElasticNet (Combining L1 + L2)

### ğŸ§© **3. Practical Model Fitting**
- Regularization in Scikit-Learn
- Cross-Validation for Î» Selection
- Visual Demos: Shrinkage of Weights

---

## ğŸ“˜ `06_bayesian_models_and_naive_bayes.ipynb`

### ğŸ§© **1. Foundations of Bayesian Thinking**
- Bayes Theorem Refresher
- Likelihood vs Prior vs Posterior
- Probabilistic Classification Intuition

### ğŸ§© **2. Naive Bayes Classifiers**
- Gaussian, Multinomial, Bernoulli
- Conditional Independence Assumption
- When Naive Bayes Works Well

### ğŸ§© **3. Evaluation & Usage**
- Use Cases (Spam, Sentiment, etc.)
- Comparison to Logistic Regression
- Performance on Imbalanced Data

---





















## ğŸ§­ Master Table of Contents



---

### ğŸ“˜ [03 Decision Trees & Ensemble Methods](#trees-explained)
- ğŸŒ³ [Decision Trees Explained](#trees-explained)
  - ğŸ§ª [Splitting Criteria: Gini vs Entropy](#splitting-criteria)
  - âœ‚ï¸ [Tree Depth & Pruning](#pruning)
  - ğŸš¨ [Overfitting in Trees](#overfitting-trees)
- ğŸŒ² [Ensemble Techniques](#ensemble-techniques)
  - ğŸ§º [Bagging & Random Forests](#bagging-forests)
  - â­ [Feature Importance](#feature-importance)
  - ğŸš€ [Boosting Basics (GBM, XGBoost)](#boosting)
- ğŸ§ª [Model Tuning & Comparison](#model-tuning)
  - âš™ï¸ [Hyperparameters for Forests & Boosters](#hyperparameters)
  - ğŸ” [When to Use Trees vs Linear Models](#trees-vs-linear)
  - ğŸ“‰ [Bias-Variance Tradeoff Visualized](#bias-variance)

---

### ğŸ“˜ [04 SVM & Kernel Tricks for Nonlinear Data](#svm-core)
- ğŸ§­ [Core Concepts of SVM](#svm-core)
  - ğŸ“ [Max-Margin Intuition](#max-margin)
  - ğŸ§Š [Hard vs Soft Margins](#hard-soft)
  - ğŸ“ [Hinge Loss Function](#hinge-loss)
- ğŸŒŒ [Going Nonlinear with Kernels](#svm-kernels)
  - ğŸ§® [Polynomial & RBF Kernels](#kernel-types)
  - ğŸ” [Visualizing Transformations](#kernel-visual)
  - ğŸš§ [Kernelized Decision Boundaries](#kernel-boundaries)
- ğŸ§° [Practical Usage](#svm-practical)
  - âš™ï¸ [Parameter Tuning: C and Gamma](#svm-tuning)
  - ğŸ”€ [Linear vs Non-linear SVM](#svm-linear-vs-nonlinear)
  - ğŸ”„ [Comparison with Logistic Regression](#svm-vs-logistic)

---

### ğŸ“˜ [05 Regularization (L1, L2, ElasticNet)](#regularization-motivation)
- ğŸ“‰ [Motivation & Math of Regularization](#regularization-motivation)
  - ğŸ§  [Overfitting Intuition](#overfitting-intuition)
  - ğŸ“Š [Regularized Cost Functions](#regularized-cost)
  - ğŸ“‰ [Effect of Î» on Loss](#lambda-effect)
- ğŸ§® [Types of Regularization](#regularization-types)
  - ğŸ”ï¸ [L2 (Ridge)](#ridge)
  - ğŸ¯ [L1 (Lasso)](#lasso)
  - ğŸ§· [ElasticNet (Combining L1 + L2)](#elasticnet)
- ğŸ› ï¸ [Practical Model Fitting](#regularization-practical)
  - ğŸ§° [Regularization in Scikit-Learn](#sklearn-regularization)
  - ğŸ” [Cross-Validation for Î» Selection](#cv-lambda)
  - ğŸ¨ [Visual Demos: Shrinkage of Weights](#shrinkage-visuals)

---

### ğŸ“˜ [06 Bayesian Models & Naive Bayes](#bayesian-foundations)
- ğŸ§  [Foundations of Bayesian Thinking](#bayesian-foundations)
  - ğŸ“š [Bayes Theorem Refresher](#bayes-theorem)
  - ğŸ§ª [Likelihood vs Prior vs Posterior](#likelihood-prior)
  - ğŸ² [Probabilistic Classification Intuition](#probabilistic-intuition)
- ğŸ¦ [Naive Bayes Classifiers](#naive-bayes)
  - ğŸ“Š [Gaussian, Multinomial, Bernoulli](#nb-types)
  - ğŸ”— [Conditional Independence Assumption](#independence-assumption)
  - âœ… [When Naive Bayes Works Well](#nb-use-cases)
- ğŸ“ˆ [Evaluation & Usage](#bayesian-evaluation)
  - âœ‰ï¸ [Use Cases (Spam, Sentiment, etc.)](#nb-applications)
  - âš”ï¸ [Comparison to Logistic Regression](#nb-vs-logistic)
  - âš–ï¸ [Performance on Imbalanced Data](#imbalanced-performance)




















---

## ğŸ“˜ `03_decision_trees_and_ensemble_methods.ipynb`

### ğŸŒ³ <a id="trees-explained"></a>**1. Decision Trees Explained**
#### <a id="splitting-criteria"></a>ğŸ§ª Splitting Criteria: Gini vs Entropy  
#### <a id="pruning"></a>âœ‚ï¸ Tree Depth & Pruning  
#### <a id="overfitting-trees"></a>ğŸš¨ Overfitting in Trees  

### ğŸŒ² <a id="ensemble-techniques"></a>**2. Ensemble Techniques**
#### <a id="bagging-forests"></a>ğŸ§º Bagging & Random Forests  
#### <a id="feature-importance"></a>â­ Feature Importance  
#### <a id="boosting"></a>ğŸš€ Boosting Basics (GBM, XGBoost)  

### ğŸ§ª <a id="model-tuning"></a>**3. Model Tuning & Comparison**
#### <a id="hyperparameters"></a>âš™ï¸ Hyperparameters for Forests & Boosters  
#### <a id="trees-vs-linear"></a>ğŸ” When to Use Trees vs Linear Models  
#### <a id="bias-variance"></a>ğŸ“‰ Bias-Variance Tradeoff Visualized  

---

## ğŸ“˜ `04_svm_and_kernel_tricks_for_nonlinear_data.ipynb`

### ğŸ§­ <a id="svm-core"></a>**1. Core Concepts of SVM**
#### <a id="max-margin"></a>ğŸ“ Max-Margin Intuition  
#### <a id="hard-soft"></a>ğŸ§Š Hard vs Soft Margins  
#### <a id="hinge-loss"></a>ğŸ“ Hinge Loss Function  

### ğŸŒŒ <a id="svm-kernels"></a>**2. Going Nonlinear with Kernels**
#### <a id="kernel-types"></a>ğŸ§® Polynomial & RBF Kernels  
#### <a id="kernel-visual"></a>ğŸ” Visualizing Transformations  
#### <a id="kernel-boundaries"></a>ğŸš§ Kernelized Decision Boundaries  

### ğŸ§° <a id="svm-practical"></a>**3. Practical Usage**
#### <a id="svm-tuning"></a>âš™ï¸ Parameter Tuning: C and Gamma  
#### <a id="svm-linear-vs-nonlinear"></a>ğŸ”€ Linear vs Non-linear SVM  
#### <a id="svm-vs-logistic"></a>ğŸ”„ Comparison with Logistic Regression  

---

## ğŸ“˜ `05_regularization_l1_l2_elasticnet.ipynb`

### ğŸ“‰ <a id="regularization-motivation"></a>**1. Motivation & Math of Regularization**
#### <a id="overfitting-intuition"></a>ğŸ§  Overfitting Intuition  
#### <a id="regularized-cost"></a>ğŸ“Š Regularized Cost Functions  
#### <a id="lambda-effect"></a>ğŸ“‰ Effect of Î» on Loss  

### ğŸ§® <a id="regularization-types"></a>**2. Types of Regularization**
#### <a id="ridge"></a>ğŸ”ï¸ L2 (Ridge)  
#### <a id="lasso"></a>ğŸ¯ L1 (Lasso)  
#### <a id="elasticnet"></a>ğŸ§· ElasticNet (Combining L1 + L2)  

### ğŸ› ï¸ <a id="regularization-practical"></a>**3. Practical Model Fitting**
#### <a id="sklearn-regularization"></a>ğŸ§° Regularization in Scikit-Learn  
#### <a id="cv-lambda"></a>ğŸ” Cross-Validation for Î» Selection  
#### <a id="shrinkage-visuals"></a>ğŸ¨ Visual Demos: Shrinkage of Weights  

---

## ğŸ“˜ `06_bayesian_models_and_naive_bayes.ipynb`

### ğŸ§  <a id="bayesian-foundations"></a>**1. Foundations of Bayesian Thinking**
#### <a id="bayes-theorem"></a>ğŸ“š Bayes Theorem Refresher  
#### <a id="likelihood-prior"></a>ğŸ§ª Likelihood vs Prior vs Posterior  
#### <a id="probabilistic-intuition"></a>ğŸ² Probabilistic Classification Intuition  

### ğŸ¦ <a id="naive-bayes"></a>**2. Naive Bayes Classifiers**
#### <a id="nb-types"></a>ğŸ“Š Gaussian, Multinomial, Bernoulli  
#### <a id="independence-assumption"></a>ğŸ”— Conditional Independence Assumption  
#### <a id="nb-use-cases"></a>âœ… When Naive Bayes Works Well  

### ğŸ“ˆ <a id="bayesian-evaluation"></a>**3. Evaluation & Usage**
#### <a id="nb-applications"></a>âœ‰ï¸ Use Cases (Spam, Sentiment, etc.)  
#### <a id="nb-vs-logistic"></a>âš”ï¸ Comparison to Logistic Regression  
#### <a id="imbalanced-performance"></a>âš–ï¸ Performance on Imbalanced Data  
---

