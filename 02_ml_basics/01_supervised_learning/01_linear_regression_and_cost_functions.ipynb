{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2542a86",
   "metadata": {},
   "source": [
    "## üß≠ Master Table of Contents\n",
    "\n",
    "- üß© [Building Blocks of Linear Regression](#linear-regression-core)\n",
    "  - üß† [Hypothesis Function](#hypothesis-function)\n",
    "  - üìâ [Line Fitting (Geometric Intuition)](#line-fitting)\n",
    "  - üßæ [Assumptions of Linear Models](#assumptions-linear)\n",
    "- ‚öôÔ∏è [Cost Function & Optimization](#cost-optimization)\n",
    "  - üí• [Squared Error / MSE](#squared-error)\n",
    "  - üîÅ [Gradient Descent (Single Variable)](#gd-single)\n",
    "  - üßÆ [Gradient Descent (Multivariable)](#gd-multivariable)\n",
    "  - ‚ö° [Vectorization for Speedup](#vectorization)\n",
    "- üìä [Evaluation & Interpretation](#evaluation-interpretation)\n",
    "  - üìà [R¬≤ Score](#r2-score)\n",
    "  - ü©∫ [Underfitting & Model Diagnostics](#underfitting-diagnostics)\n",
    "  - üåÑ [Visualizing Cost Surface](#cost-surface)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cf7fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß© <a id=\"linear-regression-core\"></a>**1. Building Blocks of Linear Regression**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa1845",
   "metadata": {},
   "source": [
    "# <a id=\"hypothesis-function\"></a>üß† Hypothesis Function \n",
    "\n",
    "> *The Hypothesis Function is the machine‚Äôs \"best guess formula\" mapping inputs to outputs, like a spring extending proportionally when you pull it ‚Äî simple, direct, mechanical.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**  \n",
    "- **ML**: Basis for all predictive modeling.\n",
    "- **DL**: Linear layer foundation before activations.\n",
    "- **LLMs**: Core in embedding transformations.\n",
    "- **AGI**: Primitive form of function approximation.\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine a **spring attached to a moving cart**.  \n",
    "The spring stretches based on how hard you pull ‚Äî no fancy behavior yet, just simple, **direct proportionality**. That‚Äôs the hypothesis: *input* ‚Üí *simple mechanical response*.\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Hastie et al., 2021 ‚Äî *\"Statistical Learning with Sparsity\"*\n",
    "- Montgomery et al., 2020 ‚Äî *\"Introduction to Linear Regression Analysis\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Hypothesis ($h_\\theta(x)$)**: Predicts output from input. *Analogous to a spring's stretch.*  \n",
    "‚Ä¢ **Parameters ($\\theta$)**: Weights tuning prediction. *Analogous to spring stiffness.*  \n",
    "‚Ä¢ **Input ($x$)**: Features provided to model. *Analogous to force applied to spring.*  \n",
    "‚Ä¢ **Output ($y$)**: Target value. *Analogous to spring's final length.*  \n",
    "‚Ä¢ **Prediction Error**: Difference between prediction and reality. *Analogous to spring overshoot.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "1. **Purpose**\n",
    "- Forecast house prices from area.\n",
    "- Predict stock prices from trends.\n",
    "- Classify spam emails by keywords.\n",
    "\n",
    "2. **When to Avoid**\n",
    "- When relationships are clearly nonlinear.\n",
    "- When data has heavy interaction effects.\n",
    "\n",
    "3. **Origin Story**  \n",
    "Linear modeling dates back to **Gauss (early 1800s)** solving astronomical prediction errors ‚Äî laying the seeds for today‚Äôs ML giants. üå±\n",
    "\n",
    "4. **ASCII Flow Diagram**\n",
    "\n",
    "```plaintext\n",
    "Input (x)\n",
    "  ‚Üì\n",
    "Apply Parameters (Œ∏)\n",
    "  ‚Üì\n",
    "Linear Combination\n",
    "  ‚Üì\n",
    "Predicted Output (hŒ∏(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Defines a linear mapping between spaces |  \n",
    "| ML | Models the base prediction |  \n",
    "| DL | Forms basic layer operations |  \n",
    "| LLM | Projects embeddings linearly before transformations |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "- $\\theta_0$: Intercept (bias)\n",
    "- $\\theta_1$: Slope (weight)\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- $\\theta_1 = 0$: Flat line ‚Äî no sensitivity to input.\n",
    "- $\\theta_1 \\to \\infty$: Extreme sensitivity ‚Äî unstable model.\n",
    "- $\\theta_0 = 0$: Line passes through origin ‚Äî no offset.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like changing the spring's stiffness and attachment point.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $\\theta_0$ | Constant shift | Anchor offset | $\\theta_0 = 0$: origin pass |  \n",
    "| $\\theta_1$ | Linear scaling | Spring stiffness | $\\theta_1 = 0$: no motion |  \n",
    "| $x$ | Input variable | Applied force | $x=0$: no effect |  \n",
    "| $h_\\theta(x)$ | Predicted value | Spring extension | Diverges if unstable |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small $\\theta$ | Small gradients | Slow learning |  \n",
    "| Large $\\theta$ | Steep gradients | Instability risk |  \n",
    "| Zero $\\theta$ | Zero gradient | No learning |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Linearity | Ensures correct mapping | Polynomial trends in data |  \n",
    "| No multicollinearity | Separates feature effects | Redundant input features |  \n",
    "| Homoscedasticity | Stable prediction variance | Heteroskedastic financial data |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Linearity | Model misfit | Wrong trend capture | Use polynomials |  \n",
    "| Multicollinearity | Unstable parameters | Transformer redundant heads | Regularization |  \n",
    "| Homoscedasticity | Biased errors | Financial volatility | Weighted loss |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Residual | $y - h_\\theta(x)$ | Check prediction gap | \"Spring missed target\" |  \n",
    "| Mean Error | $\\frac{1}{n}\\sum (y - h_\\theta(x))$ | Bias check | \"Spring always too short/long\" |  \n",
    "| MSE | $\\frac{1}{n}\\sum (y - h_\\theta(x))^2$ | Penalize large errors | \"Spring violently off\" |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Evaluate $h_\\theta(x)$ | $O(n)$ | $O(n)$ | Linear in inputs |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def hypothesis(theta, x):\n",
    "    \"\"\"\n",
    "    Simple linear hypothesis function.\n",
    "    \n",
    "    Args:\n",
    "    theta (np.ndarray): Shape (2,) [theta_0, theta_1]\n",
    "    x (np.ndarray): Shape (n,) input features\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Predicted outputs, shape (n,)\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1 and theta.shape[0] == 2\n",
    "    assert x.ndim == 1\n",
    "    return theta[0] + theta[1] * x\n",
    "```\n",
    "## PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def hypothesis_function(theta, x):\n",
    "    \"\"\"\n",
    "    Linear hypothesis function.\n",
    "    \n",
    "    Args:\n",
    "        theta (torch.Tensor): Shape (2,), [theta_0, theta_1]\n",
    "        x (torch.Tensor): Shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Predictions, shape (n,)\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1 and theta.shape[0] == 2\n",
    "    assert x.ndim == 1\n",
    "    return theta[0] + theta[1] * x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def hypothesis_function(theta, x):\n",
    "    \"\"\"\n",
    "    Linear hypothesis function.\n",
    "\n",
    "    Args:\n",
    "        theta (tf.Tensor): Shape (2,)\n",
    "        x (tf.Tensor): Shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Predictions, shape (n,)\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(theta, 1)\n",
    "    tf.debugging.assert_rank(x, 1)\n",
    "    return theta[0] + theta[1] * x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Debug & Fix Examples**\n",
    "\n",
    "| Symptom | Root Cause | Fix |  \n",
    "|:--------|:-----------|:----|  \n",
    "| Always same output | $\\theta_1=0$ | Check initialization |  \n",
    "| Output explosion | Large $\\theta$ | Regularize or normalize inputs |  \n",
    "| Shape mismatch error | Wrong dimensions | Assert input shapes |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- $\\theta_0 = 2$\n",
    "- $\\theta_1 = 3$\n",
    "- $x = 4$\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Multiply | $\\theta_1 \\times x = 3 \\times 4$ | 12 |  \n",
    "| 2 | Add | $\\theta_0 + 12 = 2 + 12$ | 14 |  \n",
    "| Final | Output | Predicted $h_\\theta(x)$ | 14 |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q:** What happens if $\\theta_1=0$?  \n",
    "**A:** Model ignores input, outputs only $\\theta_0$ (fixed line).\n",
    "\n",
    "**Q:** Why can't we blindly trust high $\\theta$?  \n",
    "**A:** Too high makes model super-sensitive ‚Äî tiny input changes cause wild predictions.\n",
    "\n",
    "**Q:** How does hypothesis affect learning rate choice?  \n",
    "**A:** Steep $\\theta$ needs smaller learning rate to avoid divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Realm Mapping Directive**\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Linear mappings |  \n",
    "| ML | Linear Regression model |  \n",
    "| DL | Dense layer pre-activation |  \n",
    "| LLMs | Attention linear projections (Q, K, V) |  \n",
    "| Research/AGI | Function approximation for reward models |  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Hypothesis Function**\n",
    "\n",
    "**Scenario:**  \n",
    "You‚Äôre working with a simple linear regression model using the **hypothesis function** $h_\\theta(x) = \\theta_0 + \\theta_1x$ with $\\theta_1=0$.  \n",
    "Observed behavior: **Outputs are constant regardless of inputs.**\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**UNDERFITTING** ‚Üí Model cannot capture any variance from inputs.\n",
    "\n",
    "2. **Action:**  \n",
    "**Increase $\\theta_1$ via learning or re-initialize.**  \n",
    "*Tradeoff*: Larger $\\theta_1$ can cause instability if overshooting correct values.\n",
    "\n",
    "3. **Calculation:**  \n",
    "If $\\theta_1$ is updated by $\\Delta\\theta_1 = +0.5$,  \n",
    "the slope of the line increases ‚Üí predicted $h_\\theta(x)$ will vary with $x$ accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **Hypothesis Function** | Linear prediction | $\\theta_1=0$ | Constant output (no learning) |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Underfitting** ‚Üí Model output ignores input variation.  \n",
    "2. **Increase $\\theta_1$** ‚Üí Risk: overshoot causing oscillations.  \n",
    "3. **Output becomes sensitive to input x** ‚Üí $h_\\theta(x)$ now depends on x.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "**For \"Hypothesis Functions\" in LLMs:**  \n",
    "\n",
    "**Scenario:**  \n",
    "Your embedding linear projection has near-zero weights after initialization.\n",
    "\n",
    "1. **Diagnosis:** Underfitting / Stuck embeddings (no signal propagation).\n",
    "2. **Action:** Re-initialize weights with variance scaling (e.g., Xavier init).\n",
    "3. **Calculation:** Activations will scale proportionally with input vectors.\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Underfitting** ‚Üí Embedding layer outputs collapse.  \n",
    "2. **Variance-scaling initialization** ‚Üí Prevents dead layers.  \n",
    "3. **Activations** ‚Üí $\\sim \\text{input} \\times \\text{scale factor}$.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Gauss (1809) | Least Squares Estimation | Foundations of linear regression |  \n",
    "| Goodfellow et al. (2016) | Initialization strategies | Prevent dead neurons from bad $\\theta$ |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular regression | Flat predictions | Same house price output | Underfitting, missed trends |  \n",
    "| NLP (embeddings) | Collapsed vectors | Poor semantic separation | Bad initialization |  \n",
    "| CV (image regression) | Uniform pixel prediction | No image-specific adjustments | Model dead zone |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Increase $\\theta_1$ randomly | Induce variance | Output standard deviation | Should rise from 0 |  \n",
    "| Initialize $\\theta_0=0$ | No bias | Mean output vs mean target | Mean error nonzero |  \n",
    "| Add noise to $x$ | Check robustness | MSE | Should increase slightly |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How to best initialize linear models for noisy datasets?**  \n",
    "  *Why hard: Noisy variance hard to predict upfront.*\n",
    "\n",
    "- **Optimal $\\theta$ adjustment schedule for online learning?**  \n",
    "  *Why hard: Data distribution shifts mid-stream.*\n",
    "\n",
    "- **Best $\\theta$ setting heuristics for multi-modal embeddings?**  \n",
    "  *Why hard: Conflicting structure across modalities.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Poor initialization locks models into biased predictions.  \n",
    "  *Mitigation: Use variance-scaled initialization.*\n",
    "\n",
    "‚Ä¢ **Risk**: Simple linear models misrepresent complex relationships.  \n",
    "  *Mitigation: Validate assumptions before deployment.*\n",
    "\n",
    "‚Ä¢ **Risk**: Oversimplified mappings hide minority subgroup behaviors.  \n",
    "  *Mitigation: Perform disaggregated error analysis.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Argue whether linear hypotheses are ethical in high-stakes fields like healthcare, given their inability to capture nonlinear complexity.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**:  \n",
    "  TF2 linear layers sometimes auto-add bias even if not declared ‚Äî always double-check.  \n",
    "\n",
    "- **Scaling Limits**:  \n",
    "  Hypothesis functions struggle with high-dimensional, nonlinear manifolds (e.g., NLP embeddings).\n",
    "\n",
    "- **Production Fixes**:  \n",
    "  If outputs seem 'dead', immediately check $\\theta$ magnitudes before debugging deeper.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Control Systems | Proportional control law | Linear mapping input‚Üíoutput |  \n",
    "| Economics | Demand prediction curve | Simple linear fit |  \n",
    "| Robotics | Force feedback models | Spring-like linear relation |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```\n",
    "1800s: Least Squares for Astronomy\n",
    "‚Üí 1900s: Statistical Regression in Economics\n",
    "‚Üí 2000s: ML Model Base Hypotheses\n",
    "‚Üí 2020s: LLMs embedding projections\n",
    "‚Üí 2030+: Neuromorphic analog signal modeling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Adaptive Hypothesis Functions** (e.g., dynamic parameter adjustment during training)  \n",
    "- **Energy-Based Hypotheses** (mapping inputs based on physical laws)  \n",
    "- **Multi-Manifold Linear Approximations** (handling complex domains via local linear patches)\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606e4215",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# üì¶ Setup: Toy dataset + Hypothesis Function Simulator with ipywidgets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\pyplot.py:70\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _docstring\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     69\u001b[39m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Figure, FigureBase, figaspect\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgridspec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\figure.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\projections\\__init__.py:55\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m axes, _docstring\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\matplotlib\\axes\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _base\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_axes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Axes\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Backcompat.\u001b[39;00m\n\u001b[32m      5\u001b[39m Subplot = Axes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1528\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1502\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1601\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üì¶ Setup: Toy dataset + Hypothesis Function Simulator with ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üé≤ Generate synthetic linear dataset: y = 3x + noise\n",
    "def generate_data(n_samples=50, noise_std=1.0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(-10, 10, n_samples)\n",
    "    noise = np.random.normal(0, noise_std, size=n_samples)\n",
    "    y = 3 * x + noise\n",
    "    return x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "\n",
    "# üß† Linear Hypothesis Function\n",
    "def apply_concept(X, y, config):\n",
    "    lr = config['learning_rate']\n",
    "    epochs = config['epochs']\n",
    "    verbose = config['verbose']\n",
    "    \n",
    "    # Add bias term: x_0 = 1 (intercept)\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # (N, 2)\n",
    "    \n",
    "    # Init theta randomly\n",
    "    theta = np.random.randn(2, 1)\n",
    "    \n",
    "    loss_log = []\n",
    "    \n",
    "    # üöÄ Step-by-step Gradient Descent\n",
    "    for i in range(epochs):\n",
    "        # Forward pass: h(x) = X_b @ theta\n",
    "        predictions = X_b @ theta\n",
    "        \n",
    "        # Compute error & MSE loss\n",
    "        errors = predictions - y\n",
    "        loss = np.mean(errors**2)\n",
    "        loss_log.append(loss)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = 2 / len(X_b) * X_b.T @ errors\n",
    "        \n",
    "        # Update rule: Œ∏ := Œ∏ - Œ± * gradient\n",
    "        theta -= lr * gradients\n",
    "        \n",
    "        if verbose and i % (epochs // 5) == 0:\n",
    "            print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    return theta, loss_log\n",
    "\n",
    "# üé® Visualization function\n",
    "def visualize_hypothesis(noise_std=1.0, learning_rate=0.01, epochs=100, verbose=False):\n",
    "    X, y = generate_data(noise_std=noise_std)\n",
    "    config = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "        'verbose': verbose\n",
    "    }\n",
    "    theta, loss_log = apply_concept(X, y, config)\n",
    "    \n",
    "    # üìâ Plot 1: Loss Curve\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_log)\n",
    "    plt.title(\"Training Loss vs Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "    # üìç Plot 2: Data + Hypothesis line\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X, y, color='blue', label=\"Data Points\")\n",
    "    \n",
    "    # Predicted line\n",
    "    x_range = np.linspace(-10, 10, 100).reshape(-1, 1)\n",
    "    x_range_b = np.c_[np.ones((100, 1)), x_range]\n",
    "    y_pred_line = x_range_b @ theta\n",
    "    \n",
    "    plt.plot(x_range, y_pred_line, color='red', label=\"Learned Hypothesis\")\n",
    "    plt.title(\"Linear Fit via Hypothesis Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è ipywidgets Sliders\n",
    "noise_slider = widgets.FloatSlider(value=1.0, min=0.0, max=10.0, step=0.1, description='Noise:')\n",
    "lr_slider = widgets.FloatLogSlider(value=0.01, base=10, min=-4, max=0, step=0.1, description='Learning Rate:')\n",
    "epochs_slider = widgets.IntSlider(value=100, min=10, max=1000, step=10, description='Epochs:')\n",
    "verbose_toggle = widgets.Checkbox(value=False, description='Verbose Logging')\n",
    "\n",
    "ui = widgets.VBox([noise_slider, lr_slider, epochs_slider, verbose_toggle])\n",
    "out = widgets.interactive_output(\n",
    "    visualize_hypothesis,\n",
    "    {\n",
    "        'noise_std': noise_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'epochs': epochs_slider,\n",
    "        'verbose': verbose_toggle\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faaf893",
   "metadata": {},
   "source": [
    "\n",
    "# <a id=\"line-fitting\"></a>üìâ Line Fitting (Geometric Intuition)  \n",
    "---\n",
    "\n",
    "> *Line fitting is the process of drawing the best straight line through a cloud of data points ‚Äî like pulling a tight string between scattered nails.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Captures basic patterns from noisy data.\n",
    "- **DL**: Foundation for linear transformations inside neurons.\n",
    "- **LLMs**: Projection layers between embeddings.\n",
    "- **AGI**: Learning minimal mappings between input-output signals.\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine tiny **magnets (data points)** scattered on a table, and you're holding a **metal rod (line)** above them.  \n",
    "The rod *wants to align* with the magnets ‚Äî pulled softly this way or that, trying to **balance** itself through the center of their invisible forces.  \n",
    "That final *perfect tension* is your fitted line. üéØ\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Tibshirani, 2021 ‚Äî *\"Elements of Statistical Learning (New Edition)\"*  \n",
    "- James et al., 2022 ‚Äî *\"An Introduction to Statistical Learning with Applications in Python\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Best Fit Line**: Minimizes distance to points. *Analogous to tight string through nails.*  \n",
    "‚Ä¢ **Residuals**: Differences between prediction and actual. *Analogous to string slack.*  \n",
    "‚Ä¢ **Mean Squared Error (MSE)**: Average of squared residuals. *Analogous to total tension energy.*  \n",
    "‚Ä¢ **Overfitting**: Line too wiggly to fit every nail perfectly. *Analogous to tangled wire.*  \n",
    "‚Ä¢ **Underfitting**: Line too stiff, ignoring nail positions. *Analogous to rigid rod.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### 1. **Purpose**\n",
    "- Predict house prices based on size.\n",
    "- Estimate temperature trends across years.\n",
    "- Forecast sales from marketing spend.\n",
    "\n",
    "### 2. **When to Avoid**\n",
    "- Highly nonlinear underlying patterns.\n",
    "- Strong feature interactions not capturable by a single line.\n",
    "\n",
    "### 3. **Origin Story**  \n",
    "In **1805**, **Adrien-Marie Legendre** first proposed minimizing residuals (vertical gaps) to find the best line, leading to **Gauss's least squares** method ‚Äî the birth of modern regression.\n",
    "\n",
    "### 4. **ASCII Flow Diagram**\n",
    "\n",
    "```plaintext\n",
    "Data Points\n",
    "  ‚Üì\n",
    "Measure Residuals\n",
    "  ‚Üì\n",
    "Minimize Total Residuals (Sum of Squares)\n",
    "  ‚Üì\n",
    "Compute Best Fit Line\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Find line minimizing squared deviations |  \n",
    "| ML | Learn simple data relationship |  \n",
    "| DL | Project features linearly |  \n",
    "| LLM | Embed tokens into vector space |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $h_\\theta(x)$ = predicted output\n",
    "- $\\theta_0$ = intercept (bias term)\n",
    "- $\\theta_1$ = slope (weight)\n",
    "\n",
    "We aim to **minimize**:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- $\\theta_1 = 0$ ‚Üí Horizontal line (ignores input $x$).  \n",
    "- $\\theta_1 \\to \\infty$ ‚Üí Vertical line (undefined, division by zero).  \n",
    "- $\\theta_0 = 0$ ‚Üí Line forced through origin.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like adjusting a laser pointer to perfectly skim across scattered marbles.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $\\theta_0$ | Shift line vertically | Rod's height adjustment | $\\theta_0 = 0$: anchored at origin |  \n",
    "| $\\theta_1$ | Tilt line | Rod's tilt angle | $\\theta_1 = 0$: flat rod |  \n",
    "| $x$ | Input value | Table surface | Extreme $x$: rod tilts more |  \n",
    "| $h_\\theta(x)$ | Prediction | Rod's shadow on x-axis | Out of domain for huge tilt |  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small $\\theta$ | Small gradients | Slow adjustment |  \n",
    "| Large $\\theta$ | Large gradients | Risk overshooting |  \n",
    "| Zero gradients | At minimum | Optimal fitted line |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Linearity | Fitting line assumes relation is linear | Sinusoidal data trend |  \n",
    "| Homoscedasticity | Equal variance across x | Funnel-shaped residuals |  \n",
    "| Independence | Data points affect only themselves | Time-series autocorrelation |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Linearity | Wrong trend captured | Nonlinear hidden layers | Feature engineering |  \n",
    "| Homoscedasticity | Biased weights | Financial risk modeling | Weighted loss |  \n",
    "| Independence | Biased standard errors | Autoregressive modeling | Time series methods |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Residual | $e^{(i)} = y^{(i)} - h_\\theta(x^{(i)})$ | Pointwise fit error | Slack for each magnet |  \n",
    "| Sum of Squared Residuals (SSR) | $\\sum (e^{(i)})^2$ | Total error to minimize | Total tension |  \n",
    "| MSE | $\\frac{1}{m}\\sum (e^{(i)})^2$ | Average fit error | Average slack energy |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Evaluate $h_\\theta(X)$ for all $X$ | $O(m)$ | $O(m)$ | Linear in data size |  \n",
    "| Compute $J(\\theta)$ | $O(m)$ | $O(m)$ | Quadratic loss increases linearly |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy (PEP8 + Vectorized)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_predictions(theta, X):\n",
    "    \"\"\"\n",
    "    Compute the hypothesis for input features.\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): Parameters, shape (2,)\n",
    "        X (np.ndarray): Input features, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted outputs, shape (m,)\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1 and theta.shape[0] == 2\n",
    "    assert X.ndim == 1\n",
    "    return theta[0] + theta[1] * X\n",
    "\n",
    "def compute_loss(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error.\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): Parameters, shape (2,)\n",
    "        X (np.ndarray): Input features, shape (m,)\n",
    "        y (np.ndarray): Target outputs, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        float: Mean squared error value.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    predictions = compute_predictions(theta, X)\n",
    "    errors = predictions - y\n",
    "    return (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "```\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def compute_loss(theta, X, y):\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss for linear fit.\n",
    "\n",
    "    Args:\n",
    "        theta (torch.Tensor): Shape (2,)\n",
    "        X (torch.Tensor): Input features, shape (m,)\n",
    "        y (torch.Tensor): Targets, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar loss value\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1 and theta.shape[0] == 2\n",
    "    assert X.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    predictions = theta[0] + theta[1] * X\n",
    "    errors = predictions - y\n",
    "    mse = (errors ** 2).mean()\n",
    "    return mse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(theta, X, y):\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss for linear fit.\n",
    "\n",
    "    Args:\n",
    "        theta (tf.Tensor): Shape (2,)\n",
    "        X (tf.Tensor): Input features, shape (m,)\n",
    "        y (tf.Tensor): Targets, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Scalar loss value\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(theta, 1)\n",
    "    tf.debugging.assert_rank(X, 1)\n",
    "    tf.debugging.assert_rank(y, 1)\n",
    "    predictions = theta[0] + theta[1] * X\n",
    "    errors = predictions - y\n",
    "    mse = tf.reduce_mean(tf.square(errors))\n",
    "    return mse\n",
    "```\n",
    "---\n",
    "\n",
    "## üîß **Debug & Fix Examples**\n",
    "\n",
    "| Symptom | Root Cause | Fix |  \n",
    "|:--------|:-----------|:----|  \n",
    "| Loss not decreasing | Learning rate too high | Lower learning rate |  \n",
    "| Nan outputs | Overflow in large gradients | Gradient clipping |  \n",
    "| Divergent line tilt | Wrong theta update direction | Check sign of gradient |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- $\\theta_0 = 1.5$\n",
    "- $\\theta_1 = 2$\n",
    "- $X = [1, 2, 3]$\n",
    "- $y = [3, 5, 7]$\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Predict $h_\\theta(1)$ | $1.5 + 2(1)$ | 3.5 |  \n",
    "| 2 | Predict $h_\\theta(2)$ | $1.5 + 2(2)$ | 5.5 |  \n",
    "| 3 | Predict $h_\\theta(3)$ | $1.5 + 2(3)$ | 7.5 |  \n",
    "| 4 | Error 1 | $3.5 - 3$ | 0.5 |  \n",
    "| 5 | Error 2 | $5.5 - 5$ | 0.5 |  \n",
    "| 6 | Error 3 | $7.5 - 7$ | 0.5 |  \n",
    "| 7 | Squared Errors | $[0.5^2, 0.5^2, 0.5^2]$ | [0.25, 0.25, 0.25] |  \n",
    "| 8 | Sum of Squared Errors | $0.25 + 0.25 + 0.25$ | 0.75 |  \n",
    "| 9 | Mean Squared Error | $\\frac{0.75}{3}$ | 0.25 |  \n",
    "| 10 | Final Loss | $\\frac{1}{2} \\times 0.25$ | 0.125 |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# üî• **Theory Deepening**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q1:** What happens if the line fitting assumes linearity, but the data is highly nonlinear?\n",
    "\n",
    "**A1:** The fitted line will systematically mispredict, leading to large residuals especially at data extremes.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2:** Why is minimizing squared error preferred instead of minimizing absolute error in line fitting?\n",
    "\n",
    "**A2:** Squaring magnifies larger errors, making the fitted line more sensitive to outliers and easier to differentiate mathematically.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3:** How would correlated residuals (non-independent errors) affect model interpretation?\n",
    "\n",
    "**A3:** Standard errors of the estimated parameters become biased, misleading confidence intervals and predictions.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Line Fitting**\n",
    "\n",
    "**Scenario:**  \n",
    "You are training a simple linear regression model with MSE loss. Observed behavior: Loss initially decreases, then plateaus while residuals remain large.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**UNDERFITTING** ‚Üí Model lacks complexity to capture pattern.\n",
    "\n",
    "2. **Action:**  \n",
    "**Add more features or transform inputs (e.g., $x^2$ terms).**  \n",
    "*Tradeoff*: Risk of overfitting if too many terms are added without regularization.\n",
    "\n",
    "3. **Calculation:**  \n",
    "Adding $x^2$ creates a new feature $x^{(2)} = x^2$, fitting a parabola instead of a line.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **Line Fitting** | Simple regression | Model with no feature engineering | Loss plateaus, residuals large |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Underfitting** ‚Üí Linear model fails to capture complexity.  \n",
    "2. **Add features** ‚Üí Risk increasing model variance.  \n",
    "3. **Prediction** ‚Üí Gains curvature to fit trend better.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "**For \"Line Fitting\" in Transformers:**  \n",
    "\n",
    "**Scenario:**  \n",
    "Projection matrices in attention heads use simple linear layers. Observed: Poor attention quality for complex sequences.\n",
    "\n",
    "1. **Diagnosis:** Linear projections insufficient for complex token relations.\n",
    "2. **Action:** Add nonlinearity (e.g., use MLP after attention).\n",
    "3. **Calculation:** Model moves from $Wq$ to $MLP(Wq)$, introducing nonlinearity.\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Modeling limitation** ‚Üí Simple linear mapping too weak.  \n",
    "2. **Action** ‚Üí Insert nonlinear transformations.  \n",
    "3. **Impact** ‚Üí Ability to capture complex interactions.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Vapnik, 1995 | Statistical Learning Theory | Emphasized simplicity vs. complexity balance |  \n",
    "| Goodfellow et al., 2016 | Deep Learning | Linear mappings as layers in deep nets |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular (Housing) | Underfit prices | Wrong price trends | Model too simple |  \n",
    "| NLP (Sentiment Analysis) | Misclassified reviews | Cannot capture negation | Missing interactions |  \n",
    "| CV (Edge Detection) | Blurry edges | Fails high-frequency patterns | Linearity too weak |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Add polynomial terms | Better fit | MSE | Decrease |  \n",
    "| Increase data size | Better generalization | Validation loss | Decrease |  \n",
    "| Introduce noise | Model robustness | Train/val loss gap | Increase |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How to detect nonlinearity early during linear fitting?**  \n",
    "  *Why hard: Data may appear linear at small scales.*\n",
    "\n",
    "- **How to regularize polynomial extensions without overfitting?**  \n",
    "  *Why hard: Balancing bias and variance tightly.*\n",
    "\n",
    "- **Can line fitting ideas extend to ultra-high dimensional latent spaces (e.g., LLM embeddings)?**  \n",
    "  *Why hard: Curse of dimensionality.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Linear fits ignore minority trends.  \n",
    "  *Mitigation: Validate fairness metrics post-modeling.*\n",
    "\n",
    "‚Ä¢ **Risk**: Oversimplification hides critical correlations (e.g., in healthcare).  \n",
    "  *Mitigation: Perform residual analysis across subgroups.*\n",
    "\n",
    "‚Ä¢ **Risk**: Misinterpretation of model outputs as causality.  \n",
    "  *Mitigation: Communicate model limits explicitly.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Argue whether simple linear fitting is ethically sufficient in high-stakes fields like criminal justice risk modeling.\"*\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**  \n",
    "  TensorFlow linear layers sometimes silently include bias even when disabled ‚Äî check configs carefully!\n",
    "\n",
    "- **Scaling Limits**  \n",
    "  Simple line fitting scales well ($O(m)$ time), but fails beyond 2-3 key features without transformations.\n",
    "\n",
    "- **Production Fixes**  \n",
    "  Always visualize residuals before productionizing ‚Äî constant error patterns mean hidden issues!\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Robotics | Trajectory prediction | Linear relationship position/time |  \n",
    "| Finance | Trend estimation | Linear return approximations |  \n",
    "| Bioinformatics | Gene expression analysis | Linear mRNA vs phenotype |  \n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```\n",
    "1805: Least Squares (Legendre) \n",
    "‚Üí 1900s: Generalized Linear Models (GLMs) \n",
    "‚Üí 2010s: Deep Learning with Linear Layers \n",
    "‚Üí 2020s: Embedding Linear Projections (Transformers)\n",
    "‚Üí 2030+: Adaptive Local Linear Models in AGI\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Adaptive Linearizations** ‚Üí Locally adjust line fit in manifold regions.\n",
    "- **Energy-Aware Fitting** ‚Üí Minimize model energy footprint.\n",
    "- **Multi-Modal Line Fitting** ‚Üí Fit simultaneously across different sensory domains.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üíª Interactive Simulation: Line Fitting (Geometric Intuition)\n",
    "\n",
    "Simulates fitting a straight or polynomial line to noisy data,\n",
    "with interactivity over learning rate, regularization strength (alpha),\n",
    "and polynomial degree (rank).\n",
    "\n",
    "Concept Target: Visualize how the line fitting adapts to data and hyperparameters.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Step 1: Generate synthetic data\n",
    "def generate_data(n=30, noise=1.0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    x = np.sort(np.random.rand(n) * 2 - 1)  # X ‚àà [-1, 1]\n",
    "    y = 3 * x + np.random.normal(0, noise, size=n)  # Linear with noise\n",
    "    return x.reshape(-1, 1), y\n",
    "\n",
    "# üîÅ Core Function: Apply Concept\n",
    "def apply_concept(x, y, config):\n",
    "    degree = config['rank']            # Polynomial degree = model complexity\n",
    "    alpha = config['alpha']            # Regularization strength (Ridge)\n",
    "    learning_rate = config['learning_rate']  # Learning rate (for reference; sklearn is closed form)\n",
    "\n",
    "    # Step 1: Build polynomial model with ridge regularization\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        Ridge(alpha=alpha, solver='auto')\n",
    "    )\n",
    "\n",
    "    # Step 2: Fit model to data\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Step 3: Predict across a smooth range for visualization\n",
    "    x_plot = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "    y_plot = model.predict(x_plot)\n",
    "\n",
    "    return model, x_plot, y_plot\n",
    "\n",
    "# üìä Visualization\n",
    "def visualize_fit(rank=1, alpha=0.1, learning_rate=0.01, noise=1.0):\n",
    "    x, y = generate_data(noise=noise)\n",
    "\n",
    "    config = {\n",
    "        'rank': rank,\n",
    "        'alpha': alpha,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "\n",
    "    model, x_plot, y_plot = apply_concept(x, y, config)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(x, y, color='blue', label='Data')\n",
    "    plt.plot(x_plot, y_plot, color='red', linewidth=2, label=f'Fit (degree={rank})')\n",
    "    plt.title(\"Line Fitting Visualization\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Controls\n",
    "rank_slider = widgets.IntSlider(value=1, min=1, max=10, step=1, description='Rank (degree)')\n",
    "alpha_slider = widgets.FloatLogSlider(value=0.1, base=10, min=-4, max=1, step=0.1, description='Alpha (L2 Œª)')\n",
    "lr_slider = widgets.FloatLogSlider(value=0.01, base=10, min=-4, max=0, step=0.1, description='Learning Rate')\n",
    "noise_slider = widgets.FloatSlider(value=1.0, min=0.0, max=5.0, step=0.1, description='Noise')\n",
    "\n",
    "ui = widgets.VBox([rank_slider, alpha_slider, lr_slider, noise_slider])\n",
    "out = widgets.interactive_output(\n",
    "    visualize_fit,\n",
    "    {\n",
    "        'rank': rank_slider,\n",
    "        'alpha': alpha_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'noise': noise_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fd502",
   "metadata": {},
   "source": [
    "# <a id=\"assumptions-linear\"></a>üßæ Assumptions of Linear Models \n",
    "\n",
    "> *Linear models are like carefully built bridges ‚Äî if any supporting beam (assumption) is weak, the entire structure can collapse.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Ensures trustworthiness of model inferences.\n",
    "- **DL**: Provides sanity checks on linear layers' behavior.\n",
    "- **LLMs**: Validates embedding projections before heavy transformations.\n",
    "- **AGI**: Critical for reliable, interpretable low-level signal processing.\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine building a **bridge (model)** across a river.  \n",
    "If you **assume** the ground is firm (homoscedasticity), the cables are strong (linearity), and the traffic is random (independence), the bridge will stand.  \n",
    "But if **any assumption breaks** ‚Äî unstable soil, faulty materials, or synchronized traffic waves ‚Äî the entire bridge risks catastrophic failure. üåâ\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Kuhn & Johnson, 2021 ‚Äî *\"Applied Predictive Modeling (Updated Edition)\"*  \n",
    "- Harrell, 2022 ‚Äî *\"Regression Modeling Strategies\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Linearity**: Relation between input and output is linear. *Analogous to straight tension cables.*  \n",
    "‚Ä¢ **Independence**: Errors are not related. *Analogous to random traffic on bridge.*  \n",
    "‚Ä¢ **Homoscedasticity**: Constant error variance. *Analogous to steady ground firmness.*  \n",
    "‚Ä¢ **Normality**: Errors follow normal distribution. *Analogous to regular, predictable traffic fluctuations.*  \n",
    "‚Ä¢ **No Multicollinearity**: Inputs are not redundant. *Analogous to independent bridge supports.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### 1. **Purpose**\n",
    "- Enable accurate prediction intervals.\n",
    "- Trust feature importance rankings.\n",
    "- Avoid unstable or biased model weights.\n",
    "\n",
    "### 2. **When to Avoid**\n",
    "- Highly nonlinear or chaotic systems.\n",
    "- Deep hierarchical data (like text sequences or video frames).\n",
    "\n",
    "### 3. **Origin Story**  \n",
    "Born alongside Gauss's **normal equations** in early 1800s astronomy work ‚Äî assumptions like independence and normality were not \"luxuries\" but necessities for making planetary predictions feasible.\n",
    "\n",
    "### 4. **ASCII Flow Diagram**\n",
    "\n",
    "```plsintext\n",
    "Assume Linearity\n",
    "  ‚Üì\n",
    "Assume Independence\n",
    "  ‚Üì\n",
    "Assume Homoscedasticity\n",
    "  ‚Üì\n",
    "Assume Normality (optional for prediction)\n",
    "  ‚Üì\n",
    "Fit Model and Validate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Justify least-squares optimality |  \n",
    "| ML | Ensure stable, interpretable models |  \n",
    "| DL | Validate shallow layers before depth |  \n",
    "| LLM | Embed initial projections correctly |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "General form for residuals ($e^{(i)}$):\n",
    "\n",
    "$$\n",
    "e^{(i)} = y^{(i)} - h_\\theta(x^{(i)})\n",
    "$$\n",
    "\n",
    "**Assumptions over $e^{(i)}$**:\n",
    "\n",
    "| Assumption | Mathematical Expression |  \n",
    "|:-----------|:-------------------------|  \n",
    "| Linearity | $y^{(i)} = \\theta^\\top x^{(i)} + \\epsilon^{(i)}$ |  \n",
    "| Independence | $Cov(\\epsilon^{(i)}, \\epsilon^{(j)}) = 0$ for $i \\neq j$ |  \n",
    "| Homoscedasticity | $Var(\\epsilon^{(i)}) = \\sigma^2$ constant |  \n",
    "| Normality | $\\epsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$ |  \n",
    "| No Multicollinearity | $X^\\top X$ is invertible (no perfect correlations) |\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- **Perfect Multicollinearity** ‚Üí Matrix $X^\\top X$ non-invertible; no unique solution.\n",
    "- **Heteroscedasticity** ‚Üí Residual variance grows with $x$.\n",
    "- **Serial Correlation** ‚Üí Residuals of close points are similar, biasing variance estimates.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like bridge cables snapping if weights vibrate together in rhythm.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| Linearity | Straight mapping | Straight cable tension | Breaks with nonlinear inputs |  \n",
    "| Independence | No error autocorrelation | Random car arrivals | Traffic waves destabilize bridge |  \n",
    "| Homoscedasticity | Constant noise | Steady ground pressure | Weak ground collapses part |  \n",
    "| Normality | Predictable error spread | Regular traffic patterns | Rare jams destabilize |  \n",
    "| No Multicollinearity | Invertibility of $X^\\top X$ | Distinct bridge supports | Weak redundancy collapses structure |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Perfect independence | Stable, steady updates | Fast, accurate convergence |  \n",
    "| High correlation in residuals | Biased updates | Slow, misleading convergence |  \n",
    "| Severe multicollinearity | Exploding gradients | No convergence without regularization |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Linearity | Model matches real world trend | Nonlinear real-world process |  \n",
    "| Independence | Valid parameter uncertainty estimates | Sequential time data |  \n",
    "| Homoscedasticity | Fair treatment across input range | Income prediction with growing variance |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Linearity | Wrong predictions | Missing deep layers | Add polynomial features |  \n",
    "| Independence | Wrong standard errors | Autoregressive tokens | Model residual autocorrelation |  \n",
    "| Homoscedasticity | Unreliable confidence bounds | Volatility in markets | Heteroscedastic loss modeling |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Residual ($e^{(i)}$) | $y^{(i)} - h_\\theta(x^{(i)})$ | Check fit per point | Error slack at one point |  \n",
    "| Mean Residual | $\\frac{1}{m}\\sum e^{(i)}$ | Bias detection | Is bridge pulling left/right? |  \n",
    "| Variance of Residuals | $\\frac{1}{m-1}\\sum (e^{(i)} - \\bar{e})^2$ | Homoscedasticity check | Ground pressure evenness |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Residual computation | $O(m)$ | $O(m)$ | Linear |  \n",
    "| Variance computation | $O(m)$ | $O(m)$ | Linear |  \n",
    "| Inverse $(X^\\top X)^{-1}$ | $O(n^3)$ | $O(n^2)$ | Problematic if $n$ very large |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy (PEP8 + Greek Vectorization)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_residuals(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute residuals between predictions and actual targets.\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): Parameters, shape (n,)\n",
    "        X (np.ndarray): Feature matrix, shape (m, n)\n",
    "        y (np.ndarray): Target vector, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Residuals vector, shape (m,)\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    predictions = X @ theta\n",
    "    residuals = y - predictions\n",
    "    return residuals\n",
    "```\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def compute_residuals(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute residuals for linear predictions.\n",
    "\n",
    "    Args:\n",
    "        theta (torch.Tensor): Shape (n,)\n",
    "        X (torch.Tensor): Input features, shape (m, n)\n",
    "        y (torch.Tensor): Targets, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Residuals, shape (m,)\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    predictions = X @ theta\n",
    "    residuals = y - predictions\n",
    "    return residuals\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_residuals(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute residuals for linear predictions.\n",
    "\n",
    "    Args:\n",
    "        theta (tf.Tensor): Shape (n,)\n",
    "        X (tf.Tensor): Input features, shape (m, n)\n",
    "        y (tf.Tensor): Targets, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Residuals, shape (m,)\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(theta, 1)\n",
    "    tf.debugging.assert_rank(X, 2)\n",
    "    tf.debugging.assert_rank(y, 1)\n",
    "    predictions = tf.linalg.matvec(X, theta)\n",
    "    residuals = y - predictions\n",
    "    return residuals\n",
    "```\n",
    "---\n",
    "\n",
    "## üîß **Debug & Fix Examples**\n",
    "\n",
    "| Symptom | Root Cause | Fix |  \n",
    "|:--------|:-----------|:----|  \n",
    "| Residuals have pattern | Independence broken | Model autocorrelation |  \n",
    "| Increasing residual variance | Heteroscedasticity | Weighted loss or transforms |  \n",
    "| Parameter instability | Multicollinearity | Regularization (e.g., Ridge) |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "Given:\n",
    "\n",
    "- $\\theta = [2, 3]^\\top$\n",
    "- $X = \\begin{bmatrix}1 & 1\\\\1 & 2\\\\1 & 3\\end{bmatrix}$\n",
    "- $y = [5, 8, 11]^\\top$\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Predict 1st | $2 + 3(1)$ | 5 |  \n",
    "| 2 | Predict 2nd | $2 + 3(2)$ | 8 |  \n",
    "| 3 | Predict 3rd | $2 + 3(3)$ | 11 |  \n",
    "| 4 | Residual 1 | $5 - 5$ | 0 |  \n",
    "| 5 | Residual 2 | $8 - 8$ | 0 |  \n",
    "| 6 | Residual 3 | $11 - 11$ | 0 |  \n",
    "| 7 | Mean Residual | $(0 + 0 + 0)/3$ | 0 |  \n",
    "| 8 | Residual Variance | $(0+0+0)/2$ | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "# üî• **Theory Deepening**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q1:** What happens if residuals are correlated?\n",
    "\n",
    "**A1:** Estimated variances of coefficients will be biased, making confidence intervals and hypothesis tests invalid.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2:** Why is homoscedasticity important for prediction accuracy?\n",
    "\n",
    "**A2:** If error variance is not constant, predictions become unreliable ‚Äî large errors can dominate small ones unpredictably.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3:** How would multicollinearity make parameter estimates unstable?\n",
    "\n",
    "**A3:** Highly correlated features cause huge swings in $\\theta$ estimates for tiny data changes, making the model brittle.\n",
    "\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Assumptions**\n",
    "\n",
    "**Scenario:**  \n",
    "You're fitting a linear model, but the variance of residuals increases sharply with $x$ values. Loss is still decreasing.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**Heteroscedasticity** ‚Üí Error variance not constant.\n",
    "\n",
    "2. **Action:**  \n",
    "**Use weighted least squares** or **transform inputs** (e.g., log-scaling).\n",
    "\n",
    "3. **Calculation:**  \n",
    "If $Var(\\epsilon|x)$ grows with $x$, applying $y' = \\log(y)$ reduces heteroscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **Assumptions** | Homoscedasticity | Residual variance | Variance grows with $x$ |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Heteroscedasticity** ‚Üí Model underestimates variance at high $x$.  \n",
    "2. **Weighted loss or log-transform** ‚Üí Emphasizes stable variance.  \n",
    "3. **Reduced residual spread** ‚Üí Better calibrated predictions.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "**For \"Assumptions\" in Transformer Layers:**  \n",
    "\n",
    "**Scenario:**  \n",
    "During training, attention weights show autocorrelation across heads.\n",
    "\n",
    "1. **Diagnosis:** Loss of independence between attention heads.\n",
    "\n",
    "2. **Action:** Decorrelate attention heads or introduce head dropout.\n",
    "\n",
    "3. **Calculation:** Expected attention variance $\\sim \\frac{1}{h}$, lower for decorrelated heads.\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Dependence detected** ‚Üí Redundant attention patterns.  \n",
    "2. **Action** ‚Üí Prune or decorrelate heads.  \n",
    "3. **Impact** ‚Üí Higher model expressiveness, less redundancy.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Breiman, 1996 | Bias-variance decomposition | Shows why stable assumptions reduce error |  \n",
    "| Montgomery et al., 2020 | Regression Analysis | Practical violations and their effects |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular | Biased coefficients | Wrong predictions in finance | Heteroscedasticity |  \n",
    "| NLP | Overconfident attention | Poor sequence modeling | Residual autocorrelation |  \n",
    "| CV | Unstable filters | Blurry feature maps | Poor data independence |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Add noise to $x$ | Test stability | MSE | Increase |  \n",
    "| Remove correlated features | Improve stability | Variance of $\\theta$ | Decrease |  \n",
    "| Model residual autocorrelation | Improve errors | Residual ACF | Decrease |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **Can early residual autocorrelation detection guide model architecture?**  \n",
    "  *Why hard: Residuals often subtle in early training.*\n",
    "\n",
    "- **What's the optimal feature selection under high multicollinearity?**  \n",
    "  *Why hard: Feature importance becomes unstable.*\n",
    "\n",
    "- **How does assumption violation affect generalization in LLM fine-tuning?**  \n",
    "  *Why hard: Fine-tuning involves unknown distributions.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Heteroscedastic errors create unfair prediction intervals.  \n",
    "  *Mitigation: Weight errors during model fitting.*\n",
    "\n",
    "‚Ä¢ **Risk**: Correlated residuals lead to spurious feature importance.  \n",
    "  *Mitigation: Residual decorrelation analysis.*\n",
    "\n",
    "‚Ä¢ **Risk**: Multicollinearity hides causal relationships.  \n",
    "  *Mitigation: Careful feature selection or regularization.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Is it ethical to deploy linear models in medical risk scoring if assumptions are clearly violated?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**  \n",
    "  sklearn `LinearRegression` silently assumes independence ‚Äî user must check residual plots manually.\n",
    "\n",
    "- **Scaling Limits**  \n",
    "  Linear model assumptions degrade badly in high dimensions ($d \\gg m$).\n",
    "\n",
    "- **Production Fixes**  \n",
    "  Always run diagnostic plots (residuals vs. fitted) before model deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Stress-strain linearity | Validate material assumptions |  \n",
    "| Genomics | Gene expression regression | Predict phenotype |  \n",
    "| Robotics | Force modeling | Linearity assumption of response |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```\n",
    "1800s: Classical least squares assumptions \n",
    "‚Üí 1950s: Gauss-Markov Theorem formalizes conditions \n",
    "‚Üí 2000s: Regression diagnostics rise \n",
    "‚Üí 2020s: Assumption-aware ML pipelines \n",
    "‚Üí 2030+: Dynamic real-time assumption checking models\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Online residual monitoring** ‚Üí Continual assumption checking during inference.  \n",
    "- **Adaptive error weighting** ‚Üí Dynamically adjust for heteroscedasticity.  \n",
    "- **Residual-based self-correcting systems** ‚Üí Models re-train on detected violation zones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interactive Visualization: Linear Model Assumptions\n",
    "Simulates multicollinearity, noise, and regularization effects.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üìä Generate correlated features and target with noise\n",
    "def generate_data(n_samples=100, noise=1.0, collinearity=0.0):\n",
    "    np.random.seed(0)\n",
    "    x1 = np.random.rand(n_samples)\n",
    "    x2 = x1 * collinearity + np.random.rand(n_samples) * (1 - collinearity)  # Higher collinearity = more similar\n",
    "    y = 3 * x1 + 2 * x2 + np.random.randn(n_samples) * noise  # Linear relationship with added noise\n",
    "    return np.column_stack((x1, x2)), y\n",
    "\n",
    "# üß† Apply linear model and visualize assumptions\n",
    "def visualize_assumptions(collinearity=0.0, noise=1.0, alpha=0.0):\n",
    "    X, y = generate_data(noise=noise, collinearity=collinearity)\n",
    "    \n",
    "    model = Ridge(alpha=alpha)  # Ridge helps reduce variance under multicollinearity\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # üìâ Residual plot to check homoscedasticity & normality\n",
    "    plt.subplot(1, 2, 1)\n",
    "    residuals = y - y_pred\n",
    "    plt.scatter(range(len(y)), residuals)\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Residual (y - ≈∑)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # üîç Feature correlation plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[:, 0], X[:, 1])\n",
    "    plt.title(\"x‚ÇÅ vs x‚ÇÇ (Collinearity)\")\n",
    "    plt.xlabel(\"x‚ÇÅ\")\n",
    "    plt.ylabel(\"x‚ÇÇ\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è User Interactivity Sliders\n",
    "col_slider = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.1, description=\"Collinearity\")\n",
    "noise_slider = widgets.FloatSlider(value=1.0, min=0.0, max=5.0, step=0.1, description=\"Noise\")\n",
    "alpha_slider = widgets.FloatLogSlider(value=0.01, base=10, min=-4, max=2, step=0.1, description=\"Alpha\")\n",
    "\n",
    "ui = widgets.VBox([col_slider, noise_slider, alpha_slider])\n",
    "out = widgets.interactive_output(\n",
    "    visualize_assumptions,\n",
    "    {'collinearity': col_slider, 'noise': noise_slider, 'alpha': alpha_slider}\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403538cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è <a id=\"cost-optimization\"></a>**2. Cost Function & Optimization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94fee3",
   "metadata": {},
   "source": [
    "# <a id=\"squared-error\"></a>üí• Squared Error / MSE \n",
    " \n",
    "> *MSE measures the average squared distance between predictions and actuals ‚Äî like how much energy a stretched spring \"stores\" when it's pulled out of place.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Standard loss function for regression tasks.\n",
    "- **DL**: Guides neural network weight updates (especially in early layers).\n",
    "- **LLMs**: Loss component during pre-training (before cross-entropy).\n",
    "- **AGI**: Fundamental metric for stable environment prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine a **network of springs** connected between predicted and true values.  \n",
    "Each spring pulls harder the more wrong you are ‚Äî  \n",
    "and **the energy stored** in all these springs (summed together) is **your MSE**. üå∏  \n",
    "*Lower energy = model tension relaxing = better predictions.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Bishop, 2021 ‚Äî *\"Pattern Recognition and Machine Learning (New Edition)\"*  \n",
    "- G√©ron, 2022 ‚Äî *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (3rd Edition)\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Squared Error**: Square of prediction error. *Analogous to spring energy.*  \n",
    "‚Ä¢ **Mean Squared Error (MSE)**: Average of squared errors. *Analogous to total network tension.*  \n",
    "‚Ä¢ **Loss Function**: Objective being minimized. *Analogous to energy minimization.*  \n",
    "‚Ä¢ **Prediction ($h_\\theta(x)$)**: Model's guess. *Analogous to spring extension.*  \n",
    "‚Ä¢ **Target ($y$)**: Ground truth. *Analogous to anchor point.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### 1. **Purpose**\n",
    "- Penalize large prediction mistakes more severely.\n",
    "- Create differentiable loss for gradient descent.\n",
    "- Compare model performance quantitatively.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **When to Avoid**\n",
    "- Highly outlier-prone datasets (MSE is sensitive to outliers).\n",
    "- Non-Gaussian error distributions (consider MAE or Huber loss).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Origin Story**  \n",
    "MSE arises naturally from the **Gauss-Markov Theorem** in statistics ‚Äî  \n",
    "showing that minimizing squared errors yields the **best linear unbiased estimator (BLUE)** under classical assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ASCII Flow Diagram**\n",
    "\n",
    "```plaintext\n",
    "Predictions hŒ∏(x)\n",
    "  ‚Üì\n",
    "Compute Error: (hŒ∏(x) - y)\n",
    "  ‚Üì\n",
    "Square Error: (hŒ∏(x) - y)^2\n",
    "  ‚Üì\n",
    "Sum All Squared Errors\n",
    "  ‚Üì\n",
    "Divide by Number of Samples\n",
    "  ‚Üì\n",
    "Mean Squared Error (MSE)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Minimize quadratic loss |  \n",
    "| ML | Main regression objective |  \n",
    "| DL | Guides gradient flow in early layers |  \n",
    "| LLM | Part of pretraining embeddings smoothing |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "For dataset $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h_\\theta(x^{(i)})$ = prediction\n",
    "- $y^{(i)}$ = target\n",
    "- $m$ = number of examples\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- $h_\\theta(x) = y$ ‚Üí MSE = 0 (perfect predictions).  \n",
    "- Large errors ‚Üí MSE explodes quadratically.  \n",
    "- Few large outliers ‚Üí Dominate total loss.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like one massively overstretched spring pulling the entire network tight and tense.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $h_\\theta(x^{(i)})$ | Model prediction | Spring's stretched position | $h_\\theta(x) = y$: no tension |  \n",
    "| $y^{(i)}$ | True value | Spring's rest position | Outliers: huge gap |  \n",
    "| Error $(h_\\theta(x) - y)$ | Deviation | Spring's extension | Larger = higher force |  \n",
    "| Squared Error | Energy | Spring's stored energy | Quadratic growth |  \n",
    "| Mean | Normalize by $m$ | Averaging total tension | Scales energy per spring |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small errors | Small gradients | Slow, stable updates |  \n",
    "| Large errors | Huge gradients | Unstable or fast corrections |  \n",
    "| Zero error | Zero gradient | Model stops updating |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Errors are Gaussian | Justifies squaring | Heavy-tailed data |  \n",
    "| Outliers are rare | Stabilizes training | Outlier-dominated datasets |  \n",
    "| Equal error variance | Avoids error weighting issues | Heteroscedastic targets |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Gaussian Errors | Overpenalizes large errors | Financial data | Huber Loss |  \n",
    "| Rare Outliers | MSE explodes | Noisy sensor readings | Robust loss functions |  \n",
    "| Equal Variance | MSE under/over-weights groups | Income prediction | Weighted MSE |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Single Squared Error | $(h_\\theta(x) - y)^2$ | Local energy | Single spring tension |  \n",
    "| Sum of Squared Errors (SSE) | $\\sum (h_\\theta(x) - y)^2$ | Total network energy | All springs summed |  \n",
    "| Mean Squared Error (MSE) | $\\frac{1}{m}\\sum (h_\\theta(x) - y)^2$ | Average energy | Normalize spring energy |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Compute predictions | $O(m)$ | $O(m)$ | Linear |  \n",
    "| Compute errors | $O(m)$ | $O(m)$ | Linear |  \n",
    "| Compute loss | $O(m)$ | $O(1)$ | Linear |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy (PEP8 + Vectorized)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_mse(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE).\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): Shape (n,)\n",
    "        X (np.ndarray): Feature matrix, shape (m, n)\n",
    "        y (np.ndarray): Target vector, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        float: MSE value\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    predictions = X @ theta\n",
    "    errors = predictions - y\n",
    "    mse = np.mean(errors ** 2)\n",
    "    return mse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def compute_mse(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        theta (torch.Tensor): Shape (n,)\n",
    "        X (torch.Tensor): Feature matrix, shape (m, n)\n",
    "        y (torch.Tensor): Target vector, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar loss\n",
    "    \"\"\"\n",
    "    assert theta.ndim == 1\n",
    "    assert X.ndim == 2\n",
    "    assert y.ndim == 1\n",
    "    predictions = X @ theta\n",
    "    errors = predictions - y\n",
    "    mse = torch.mean(errors ** 2)\n",
    "    return mse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_mse(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) in TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        theta (tf.Tensor): Shape (n,)\n",
    "        X (tf.Tensor): Feature matrix, shape (m, n)\n",
    "        y (tf.Tensor): Target vector, shape (m,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Scalar loss\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(theta, 1)\n",
    "    tf.debugging.assert_rank(X, 2)\n",
    "    tf.debugging.assert_rank(y, 1)\n",
    "    predictions = tf.linalg.matvec(X, theta)\n",
    "    errors = predictions - y\n",
    "    mse = tf.reduce_mean(tf.square(errors))\n",
    "    return mse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üî¢ **Step-by-Step Numerical Example: Squared Error / MSE**\n",
    "\n",
    "Given:\n",
    "\n",
    "- $\\theta = [1, 2]^\\top$\n",
    "- $X = \\begin{bmatrix}1 & 2\\\\1 & 3\\\\1 & 4\\end{bmatrix}$\n",
    "- $y = [5, 7, 9]^\\top$\n",
    "\n",
    "We will **brutally atomize** the full MSE calculation into micro-steps ‚Äî  \n",
    "**one physical operation per row**, **no jumps**, *no hidden math.*\n",
    "\n",
    "---\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Predict 1st | $1 + 2(2)$ | $5$ |  \n",
    "| 2 | Predict 2nd | $1 + 2(3)$ | $7$ |  \n",
    "| 3 | Predict 3rd | $1 + 2(4)$ | $9$ |  \n",
    "| 4 | Error 1 | $5 - 5$ | $0$ |  \n",
    "| 5 | Error 2 | $7 - 7$ | $0$ |  \n",
    "| 6 | Error 3 | $9 - 9$ | $0$ |  \n",
    "| 7 | Squared Error 1 | $0^2$ | $0$ |  \n",
    "| 8 | Squared Error 2 | $0^2$ | $0$ |  \n",
    "| 9 | Squared Error 3 | $0^2$ | $0$ |  \n",
    "| 10 | Sum Squared Errors | $0 + 0 + 0$ | $0$ |  \n",
    "| 11 | Mean Squared Error | $\\frac{0}{3}$ | $0$ |\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Final Result:  \n",
    "The **MSE = 0.0**  \n",
    "(*because our model predictions perfectly match targets ‚Äî zero spring tension left, complete energy relaxation.*)\n",
    " \n",
    "---\n",
    "\n",
    "# üî• **Theory Deepening**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q1:** Why does MSE punish large errors more severely than small ones?\n",
    "\n",
    "**A1:** Squaring the errors amplifies bigger mistakes disproportionately, making the model extremely sensitive to large deviations ‚Äî like a spring pulled very far, storing more and more energy.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2:** What breaks if data has many extreme outliers?\n",
    "\n",
    "**A2:** MSE becomes dominated by these few points, causing the model to optimize badly for the majority of \"normal\" data.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3:** Why is MSE preferred over Mean Absolute Error (MAE) in gradient-based learning?\n",
    "\n",
    "**A3:** MSE has smooth, continuous derivatives everywhere, allowing efficient gradient descent ‚Äî MAE has a kink at zero (non-differentiable point).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Squared Error / MSE**\n",
    "\n",
    "**Scenario:**  \n",
    "You are training a regression model minimizing MSE. Observed behavior: Model's loss is heavily influenced by just a few very large residuals.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**Outlier Sensitivity** ‚Üí MSE dominated by outliers.\n",
    "\n",
    "2. **Action:**  \n",
    "**Switch to Huber Loss** or **perform robust outlier removal**.\n",
    "\n",
    "3. **Calculation:**  \n",
    "Switching to Huber loss means quadratic behavior for small errors, linear behavior for large ones, stabilizing updates.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **MSE** | Squared loss | Large residuals | Loss dominated by few points |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Outlier Sensitivity** ‚Üí Loss explodes due to few samples.  \n",
    "2. **Use Huber Loss** ‚Üí Balances between L2 (MSE) and L1 (MAE) behavior.  \n",
    "3. **Reduced impact of huge residuals** ‚Üí More stable optimization.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "**For \"MSE\" in LLMs:**  \n",
    "\n",
    "**Scenario:**  \n",
    "During early embedding pre-training, large random initializations cause massive token prediction errors.\n",
    "\n",
    "1. **Diagnosis:** High variance loss dominated by initial extreme predictions.\n",
    "\n",
    "2. **Action:** Reduce initialization variance (Xavier, He initialization).\n",
    "\n",
    "3. **Calculation:** Scaling initial weights by $1/\\sqrt{n}$ smooths early MSE.\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Loss dominated by bad early guesses** ‚Üí Initial instability.  \n",
    "2. **Adjust initialization** ‚Üí Prevent wild gradients.  \n",
    "3. **Impact** ‚Üí Smooth, stable MSE decrease.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Bishop, 2021 | MSE derivation from Gaussian assumptions | Validates why squared errors arise naturally |  \n",
    "| Huber, 1964 | Robust loss functions | Alternative when MSE overreacts to outliers |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular | Loss jumps wildly | Few huge outlier errors | MSE instability |  \n",
    "| NLP | Early loss spikes | Poor embedding predictions | Initialization variance |  \n",
    "| CV | Blurry predictions | Overpenalized large pixel errors | Outlier pixel effects |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Add large outliers | Test MSE robustness | Final loss value | Large increase |  \n",
    "| Remove top 5% largest errors | Stabilize training | Loss variance | Decrease |  \n",
    "| Switch to Huber Loss | Compare convergence | Validation loss | Smoother descent |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **When exactly should MSE be replaced with hybrid losses during dynamic training?**  \n",
    "  *Why hard: Requires on-the-fly distribution analysis.*\n",
    "\n",
    "- **How to design MSE variants that are still quadratic but resist outliers?**  \n",
    "  *Why hard: Quadratic curvature inherently amplifies extremes.*\n",
    "\n",
    "- **Can self-correcting MSE penalties emerge during unsupervised pretraining?**  \n",
    "  *Why hard: Requires dynamic, unsupervised feedback on error distributions.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Outlier groups dominate training focus.  \n",
    "  *Mitigation: Balance sample weighting.*\n",
    "\n",
    "‚Ä¢ **Risk**: Important minority patterns ignored when minimizing average loss.  \n",
    "  *Mitigation: Stratified validation and analysis.*\n",
    "\n",
    "‚Ä¢ **Risk**: Loss reporting hides skewness effects.  \n",
    "  *Mitigation: Always report loss distributions, not just means.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"In datasets where outliers represent real marginalized groups, should we still minimize MSE?\"*\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**  \n",
    "  TensorFlow/Keras's `mean_squared_error` computes elementwise loss; careful when switching batch sizes!\n",
    "\n",
    "- **Scaling Limits**  \n",
    "  MSE loss on massive datasets (billion+ points) requires **streamed aggregation**, not full memory load.\n",
    "\n",
    "- **Production Fixes**  \n",
    "  Always check distribution of residuals ‚Äî mean loss alone hides critical outlier behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Control signal error | Minimize actuator deviation |  \n",
    "| Biology | Predict protein folding errors | Minimize spatial distortions |  \n",
    "| Astronomy | Star position regression | Fit celestial trajectories |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1800s: Early squared error principles (Gauss)\n",
    "‚Üí 1950s: Formalized Least Squares Optimization\n",
    "‚Üí 2000s: MSE dominant in ML regressions\n",
    "‚Üí 2010s: Deep Learning MSE-based pretraining\n",
    "‚Üí 2020s: Robust variants of MSE for noisy environments\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Robust MSE variants** ‚Üí Less sensitive to rare catastrophic errors.  \n",
    "- **Uncertainty-aware MSE** ‚Üí Penalize errors based on input uncertainty.  \n",
    "- **Dynamic-loss switching** ‚Üí Move between MSE, Huber, MAE automatically based on training phase.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45af879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Fully Vectorized MSE Simulation with ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üî¨ Generate toy data for linear regression: y = Œ∏‚ÇÅ¬∑x + Œ∏‚ÇÄ + Œµ\n",
    "def generate_data(m=50, true_theta=np.array([[3.0], [2.0]]), noise_std=1.0):\n",
    "    x = np.random.rand(m, 1) * 10  # shape: (m, 1)\n",
    "    X = np.hstack([np.ones((m, 1)), x])  # Add bias term, shape: (m, 2)\n",
    "    noise = np.random.randn(m, 1) * noise_std\n",
    "    y = X @ true_theta + noise  # shape: (m, 1)\n",
    "    return X, y\n",
    "\n",
    "# üéØ Apply MSE: J(Œ∏) = 1/(2m) * ||XŒ∏ - y||¬≤, optimized via GD\n",
    "def apply_concept(X, y, learning_rate=0.01, epochs=50):\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1)  # Œ∏ ‚àà ‚Ñù‚ÅøÀ£¬π\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = X @ theta  # h_Œ∏(x)\n",
    "        error = y_hat - y  # residuals\n",
    "        mse = (1 / (2 * m)) * np.sum(error ** 2)\n",
    "        losses.append(mse)\n",
    "\n",
    "        grad = (1 / m) * (X.T @ error)  # ‚àáJ(Œ∏)\n",
    "        theta -= learning_rate * grad  # update Œ∏\n",
    "\n",
    "    return theta, losses\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(X, y, theta, losses):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # üîπ Left: Fitted line\n",
    "    axs[0].scatter(X[:, 1], y, label='Data')\n",
    "    x_line = np.linspace(X[:, 1].min(), X[:, 1].max(), 100).reshape(-1, 1)\n",
    "    X_line = np.hstack([np.ones_like(x_line), x_line])\n",
    "    y_line = X_line @ theta\n",
    "    axs[0].plot(x_line, y_line, 'r-', label='Prediction')\n",
    "    axs[0].set_title(\" Vectorized Linear Fit (MSE)\")\n",
    "    axs[0].set_xlabel(\"x\")\n",
    "    axs[0].set_ylabel(\"y\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # üîπ Right: MSE Loss over Epochs\n",
    "    axs[1].plot(losses, marker='o')\n",
    "    axs[1].set_title(\" MSE Loss Curve\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üß© Interactive controller\n",
    "def interactive_sim(noise_level, learning_rate, epochs):\n",
    "    X, y = generate_data(noise_std=noise_level)\n",
    "    theta, losses = apply_concept(X, y, learning_rate, epochs)\n",
    "    plot_results(X, y, theta, losses)\n",
    "\n",
    "# üïπÔ∏è Sliders and input widgets\n",
    "noise_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=0.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Noise Std:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "lr_slider = widgets.FloatSlider(\n",
    "    value=0.01,\n",
    "    min=0.001,\n",
    "    max=0.1,\n",
    "    step=0.001,\n",
    "    description='Learning Rate:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "epoch_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=200,\n",
    "    step=10,\n",
    "    description='Epochs:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI to simulation\n",
    "ui = widgets.VBox([noise_slider, lr_slider, epoch_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'noise_level': noise_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'epochs': epoch_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41fe3c",
   "metadata": {},
   "source": [
    "# <a id=\"gd-single\"></a>üîÅ Gradient Descent (Single Variable)  \n",
    "\n",
    "> *Gradient Descent is a method where the model \"feels\" the slope beneath it and steps downhill to minimize loss ‚Äî just like a marble rolling down a tilted surface by following the steepest path.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Foundation for almost all model optimization.\n",
    "- **DL**: Powers backpropagation updates for deep networks.\n",
    "- **LLMs**: Trains massive embedding and attention parameter matrices.\n",
    "- **AGI**: Enables continuous learning by local loss minimization.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine a **marble on a hilly surface (loss landscape)**.  \n",
    "The marble rolls in the direction of steepest descent ‚Äî moving faster on steep slopes and slower on flat areas.  \n",
    "**Each move is guided only by local tilt (gradient), not a global map.**  \n",
    " *Step by tiny, careful step... until it nestles into the valley of minimal energy.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Ruder, 2016 ‚Äî *\"An overview of Gradient Descent Optimization Algorithms\"*  \n",
    "- Goodfellow et al., 2016 ‚Äî *\"Deep Learning\"* (canonical textbook, optimization chapter)\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Gradient**: Rate of change of loss. *Analogous to slope under marble.*  \n",
    "‚Ä¢ **Learning Rate ($\\alpha$)**: Step size. *Analogous to marble's sensitivity.*  \n",
    "‚Ä¢ **Loss Function ($J(\\theta)$)**: Energy landscape. *Analogous to hilly terrain.*  \n",
    "‚Ä¢ **Update Rule**: Adjustment based on gradient. *Analogous to marble's shift.*  \n",
    "‚Ä¢ **Convergence**: Reaching the lowest point. *Analogous to marble resting in valley.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### 1. **Purpose**\n",
    "- Iteratively minimize loss function without needing exact solution.\n",
    "- Handle complex, high-dimensional optimization.\n",
    "- Enable real-time learning from incoming data streams.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **When to Avoid**\n",
    "- Ultra-flat loss surfaces (gradient vanishes ‚Üí no movement).\n",
    "- Highly chaotic loss surfaces (risk of getting stuck in local minima).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Origin Story**  \n",
    "First formalized by **Cauchy (1847)**, gradient descent evolved from early ideas in *numerical optimization* ‚Äî  \n",
    "It became critical in **machine learning** once exact closed-form solutions became computationally infeasible for massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ASCII Flow Diagram**\n",
    "\n",
    "```plaintext\n",
    "Initialize Œ∏ randomly\n",
    "  ‚Üì\n",
    "Compute Gradient ‚àáJ(Œ∏)\n",
    "  ‚Üì\n",
    "Update Œ∏ ‚Üê Œ∏ - Œ± ‚àáJ(Œ∏)\n",
    "  ‚Üì\n",
    "Evaluate New Loss\n",
    "  ‚Üì\n",
    "Repeat until convergence\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Minimize functions by iterative updates |  \n",
    "| ML | Train models by minimizing error |  \n",
    "| DL | Update millions of parameters via gradients |  \n",
    "| LLM | Fine-tune giant models across epochs |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "Update rule for a single parameter $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\frac{d}{d\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\frac{d}{d\\theta} J(\\theta)$ = gradient of loss with respect to $\\theta$\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- $\\alpha \\to 0$ ‚Üí Model barely moves ‚Üí Extremely slow convergence.  \n",
    "- $\\alpha$ too large ‚Üí Model overshoots minimum ‚Üí Possible divergence.  \n",
    "- Gradient = 0 ‚Üí Model stops updating ‚Üí Reached local extremum.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like a marble moving cautiously (small $\\alpha$) or chaotically bouncing (large $\\alpha$) depending on how sensitive it is to the hill's slope.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $\\theta$ | Parameter to optimize | Marble's position | Fixed if no gradient |  \n",
    "| $\\alpha$ | Step size | Marble's responsiveness | Overshoots if too large |  \n",
    "| $\\frac{d}{d\\theta}J(\\theta)$ | Local slope | Steepness beneath marble | No slope ‚Üí no move |  \n",
    "| Update rule | Position adjustment | Marble's next hop | Depends on slope and $\\alpha$ |  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small gradient | Tiny updates | Slow convergence |  \n",
    "| Large gradient | Big updates | Risk of overshooting |  \n",
    "| Zero gradient | No updates | Convergence point reached |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Loss surface is smooth | Gradient exists everywhere | Piecewise loss functions |  \n",
    "| Learning rate tuned | Guarantees convergence | Bad hyperparameters |  \n",
    "| Loss bounded below | Prevents infinite descent | Non-convex chaotic loss |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Smoothness | Nonexistent gradients | ReLU activation kinks | Subgradients |  \n",
    "| Learning Rate | Divergence | High $\\alpha$ in SGD | Scheduler |  \n",
    "| Bounded Loss | Infinite updates | Adversarial loss | Gradient clipping |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Instantaneous loss | $J(\\theta)$ | Evaluate current position | Marble's energy height |  \n",
    "| Gradient magnitude | $\\left|\\frac{d}{d\\theta}J(\\theta)\\right|$ | Check movement force | Slope steepness |  \n",
    "| Loss difference | $J(\\theta_{\\text{old}}) - J(\\theta_{\\text{new}})$ | Progress per step | Energy drop |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Gradient computation | $O(1)$ | $O(1)$ | Very fast for single variable |  \n",
    "| Parameter update | $O(1)$ | $O(1)$ | No scaling issues |  \n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy (PEP8 + Vectorized)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def single_var_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Single variable gradient descent.\n",
    "\n",
    "    Args:\n",
    "        theta_init (float): Initial parameter.\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function to compute gradient.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        float: Optimized parameter value.\n",
    "    \"\"\"\n",
    "    theta = theta_init\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta -= alpha * grad\n",
    "    return theta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def single_var_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Single variable gradient descent using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        theta_init (float): Initial parameter.\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function to compute gradient.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        float: Optimized parameter value.\n",
    "    \"\"\"\n",
    "    theta = torch.tensor(theta_init, dtype=torch.float32, requires_grad=False)\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta = theta - alpha * grad\n",
    "    return theta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def single_var_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Single variable gradient descent using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        theta_init (float): Initial parameter.\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function to compute gradient.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Optimized parameter value.\n",
    "    \"\"\"\n",
    "    theta = tf.Variable(theta_init, dtype=tf.float32)\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta.assign_sub(alpha * grad)\n",
    "    return theta\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example: Gradient Descent (Single Variable)** \n",
    "\n",
    "We will **brutalize** every tiny calculation,  \n",
    "no skipped logic, no hidden assumptions,  \n",
    "like tracing the marble's every tiny roll down the hill.  \n",
    "\n",
    "---\n",
    "\n",
    "Given:\n",
    "\n",
    "- Loss function: $J(\\theta) = (\\theta - 3)^2$\n",
    "- Initial $\\theta = 0$\n",
    "- Learning rate $\\alpha = 0.1$\n",
    "- Gradient: $\\frac{d}{d\\theta} J(\\theta) = 2(\\theta - 3)$\n",
    "- Number of steps: 3\n",
    "\n",
    "---\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Compute Gradient | $2(0 - 3)$ | $-6$ |  \n",
    "| 2 | Update $\\theta$ | $0 - 0.1 \\times (-6)$ | $0.6$ |  \n",
    "| 3 | Compute New Gradient | $2(0.6 - 3)$ | $-4.8$ |  \n",
    "| 4 | Update $\\theta$ | $0.6 - 0.1 \\times (-4.8)$ | $1.08$ |  \n",
    "| 5 | Compute New Gradient | $2(1.08 - 3)$ | $-3.84$ |  \n",
    "| 6 | Update $\\theta$ | $1.08 - 0.1 \\times (-3.84)$ | $1.464$ |\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Result after 3 steps:   \n",
    "$\\theta \\approx 1.464$\n",
    "\n",
    "The marble started at 0,  \n",
    "**felt the slope, moved downhill, adjusting cautiously step-by-step, approaching the minimum at $\\theta = 3$.**\n",
    "\n",
    "--- \n",
    "\n",
    "## üî• **Theory Deepening** \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Socratic Breakdown** \n",
    "\n",
    "**Q1:** Why does the gradient tell us the \"fastest descent\" direction?\n",
    "\n",
    "**A1:** Because the gradient points toward the steepest slope, and stepping against it moves the model downward most efficiently ‚Äî like a marble naturally rolling where the hill is steepest.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2:** What happens if we pick a learning rate ($\\alpha$) that's too large?\n",
    "\n",
    "**A2:** The model can overshoot the minimum, bouncing back and forth without settling, or even diverging into infinite loss.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3:** Why must we update $\\theta$ iteratively instead of jumping to the minimum directly?\n",
    "\n",
    "**A3:** Because in complex landscapes, we often don't know where the minimum is ‚Äî only the immediate local slope is available at each step.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì **Test Your Knowledge: Gradient Descent (Single Variable)** \n",
    "\n",
    "**Scenario:**  \n",
    "You are training a model with single-variable gradient descent. Observed behavior: Loss is oscillating wildly, never settling.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**Learning rate too large** ‚Üí model is overshooting.\n",
    "\n",
    "2. **Action:**  \n",
    "**Decrease learning rate $\\alpha$** to allow smaller, more stable steps.\n",
    "\n",
    "3. **Calculation:**  \n",
    "If $\\alpha$ is reduced by 10x, oscillations typically calm, and descent becomes smooth.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **Gradient Descent** | Learning rate | High ($\\alpha$) | Loss oscillates wildly |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Large learning rate** ‚Üí Model can't stabilize.  \n",
    "2. **Lower $\\alpha$** ‚Üí Smaller steps, more gradual descent.  \n",
    "3. **Result** ‚Üí Smoother, more reliable convergence.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üåê **Cross-Concept Example** \n",
    "\n",
    "**For \"Gradient Descent\" in LLMs:**  \n",
    "\n",
    "**Scenario:**  \n",
    "During pretraining, early learning rates cause huge swings in token prediction loss.\n",
    "\n",
    "1. **Diagnosis:** Too large initial learning rate during warm-up phase.\n",
    "\n",
    "2. **Action:** Use **learning rate scheduler** (e.g., linear warmup).\n",
    "\n",
    "3. **Calculation:** Start $\\alpha$ near zero and slowly ramp it to stabilize gradients.\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Unstable early training** ‚Üí Divergent weights.  \n",
    "2. **Use learning rate scheduler** ‚Üí Gradual stabilization.  \n",
    "3. **Result** ‚Üí Smooth embedding space learning.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map** \n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Cauchy, 1847 | First gradient method | Origins of descent dynamics |  \n",
    "| Kingma & Ba, 2015 (Adam) | Momentum and learning rate adaptation | Evolution from basic gradient descent |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table** \n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular | Loss oscillates | Poor convergence | Learning rate too high |  \n",
    "| NLP | Diverging token embeddings | No stable meaning vectors | Bad optimization early on |  \n",
    "| CV | Jittery feature maps | No clear activation regions | Overshoot in weights |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan** \n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Increase $\\alpha$ by 2x | Check stability | Loss curve smoothness | Oscillation increases |  \n",
    "| Decrease $\\alpha$ by 10x | Check convergence | Steps to convergence | More steps but smoother |  \n",
    "| Start from better $\\theta$ | Check shortcut | Initial loss value | Lower start loss |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions** \n",
    "\n",
    "- **Can models self-tune learning rates dynamically during training without schedulers?**  \n",
    "  *Why hard: Requires real-time curvature estimation.*\n",
    "\n",
    "- **How to best detect imminent divergence before it happens?**  \n",
    "  *Why hard: Oscillation patterns are noisy early on.*\n",
    "\n",
    "- **How to handle non-differentiable points in modern descent algorithms?**  \n",
    "  *Why hard: Subgradients only partially solve it.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks** \n",
    "\n",
    "‚Ä¢ **Risk**: Poor convergence leads to biased models (especially early phase).  \n",
    "  *Mitigation: Early learning rate validation.*\n",
    "\n",
    "‚Ä¢ **Risk**: Divergent optimization hides minority pattern learning.  \n",
    "  *Mitigation: Monitor subgroup losses separately.*\n",
    "\n",
    "‚Ä¢ **Risk**: Oscillations hide convergence in noisy data.  \n",
    "  *Mitigation: Smoothed validation metrics.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise** \n",
    "\n",
    "> *\"Should we always prioritize convergence speed over stability in real-world ML applications?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**  \n",
    "  PyTorch `optim.SGD` without scheduler often diverges on deep nets ‚Äî always pair with decay scheduler.\n",
    "\n",
    "- **Scaling Limits**  \n",
    "  Pure gradient descent impractical for millions of parameters ‚Äî needs momentum or adaptive methods (Adam, RMSprop).\n",
    "\n",
    "- **Production Fixes**  \n",
    "  Visualize loss curves early ‚Äî oscillation patterns predict long-term instability.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Robotics | Minimize actuator error | Iterative control updates |  \n",
    "| Biology | Protein structure descent | Find minimal energy conformation |  \n",
    "| Finance | Risk minimization strategies | Descend in portfolio loss |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1847: Cauchy's Descent Principle\n",
    "‚Üí 1960s: Numerical Optimization Methods\n",
    "‚Üí 1990s: Stochastic Gradient Descent (SGD) introduced\n",
    "‚Üí 2010s: Adaptive optimizers like Adam, RMSProp\n",
    "‚Üí 2020s: Meta-learning optimizers dynamically adjust learning rates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Gradient Forecasting** ‚Üí Predict future gradients to adjust now.  \n",
    "- **Meta-Adaptive Step Sizes** ‚Üí Model learns its best learning rate on the fly.  \n",
    "- **Curvature-Aware Optimizers** ‚Üí Understand loss surface shape dynamically.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Full Simulation: Gradient Descent (Single Variable) + ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Setup\n",
    "# Define a simple convex function for the demo: J(Œ∏) = Œ∏¬≤\n",
    "# Goal: Minimize J(Œ∏)\n",
    "\n",
    "def generate_data():\n",
    "    # No input data needed for single-variable GD\n",
    "    return None\n",
    "\n",
    "# üîÅ Core Logic: Apply Gradient Descent\n",
    "def apply_concept(theta_init, learning_rate, epochs):\n",
    "    # Œ∏ initialization\n",
    "    theta = theta_init\n",
    "    history = {'theta': [], 'loss': []}\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Step 1: Forward pass\n",
    "        loss = theta**2  # J(Œ∏) = Œ∏¬≤\n",
    "        \n",
    "        # Step 2: Backward pass (Gradient)\n",
    "        grad = 2 * theta  # ‚àÇJ/‚àÇŒ∏ = 2Œ∏\n",
    "        \n",
    "        # Step 3: Update rule: Œ∏ := Œ∏ - Œ±‚àáJ(Œ∏)\n",
    "        theta = theta - learning_rate * grad\n",
    "\n",
    "        # Step 4: Record values\n",
    "        history['theta'].append(theta)\n",
    "        history['loss'].append(loss)\n",
    "\n",
    "    return history\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # üîπ Left: Œ∏ Trajectory\n",
    "    axs[0].plot(history['theta'], marker='o')\n",
    "    axs[0].set_title(\" Œ∏ Values Over Epochs\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Œ∏\")\n",
    "\n",
    "    # üîπ Right: Loss Curve\n",
    "    axs[1].plot(history['loss'], marker='o', color='red')\n",
    "    axs[1].set_title(\" Loss (J(Œ∏)) Over Epochs\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Simulator\n",
    "def interactive_sim(theta_init, learning_rate, epochs):\n",
    "    generate_data()\n",
    "    history = apply_concept(theta_init, learning_rate, epochs)\n",
    "    plot_results(history)\n",
    "\n",
    "# üß∞ Sliders for UI\n",
    "theta_slider = widgets.FloatSlider(\n",
    "    value=5.0,\n",
    "    min=-10.0,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Initial Œ∏:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "lr_slider = widgets.FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.001,\n",
    "    max=1.0,\n",
    "    step=0.001,\n",
    "    description='Learning Rate:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "epoch_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Epochs:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI to function\n",
    "ui = widgets.VBox([theta_slider, lr_slider, epoch_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'theta_init': theta_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'epochs': epoch_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f095bc3",
   "metadata": {},
   "source": [
    "# <a id=\"gd-multivariable\"></a>üßÆ Gradient Descent (Multivariable)  \n",
    "\n",
    "\n",
    "> *Gradient Descent in multivariable systems is like navigating through a twisting mountain valley ‚Äî adjusting every coordinate of your position step-by-step based on the steepness around you.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Necessary for optimizing models with multiple features.\n",
    "- **DL**: Powers weight updates across billions of parameters.\n",
    "- **LLMs**: Core optimization engine for giant embedding, attention, and feedforward layers.\n",
    "- **AGI**: Critical for learning from complex multi-sensory input streams.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mechanical Analogy**  \n",
    "Imagine a **mountaineer** standing in a **giant twisted mountain landscape**.  \n",
    "At each point, the mountain slopes differently **in every direction** ‚Äî  \n",
    "Gradient Descent reads the *local slopes* (gradients along each dimension),  \n",
    "and adjusts **all your position coordinates simultaneously** to slide down toward the valley of minimum energy. üå∏\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Ruder, 2016 ‚Äî *\"An overview of Gradient Descent Optimization Algorithms\"*  \n",
    "- Bottou et al., 2018 ‚Äî *\"Optimization Methods for Large-Scale Machine Learning\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Gradient Vector ($\\nabla_\\theta J(\\theta)$)**: Collection of all partial derivatives. *Analogous to slopes along all axes.*  \n",
    "‚Ä¢ **Learning Rate ($\\alpha$)**: How big each step is taken along gradient directions. *Analogous to stride size in the mountains.*  \n",
    "‚Ä¢ **Loss Surface**: The terrain shaped by $J(\\theta)$. *Analogous to mountain landscape.*  \n",
    "‚Ä¢ **Parameter Vector ($\\theta$)**: Full list of model parameters. *Analogous to mountaineer's coordinates.*  \n",
    "‚Ä¢ **Convergence**: Reaching the valley minimum. *Analogous to finding lowest altitude.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### 1. **Purpose**\n",
    "- Handle optimization with multiple interacting variables.\n",
    "- Update all parameters in parallel, not one-by-one.\n",
    "- Make efficient use of vectorized hardware (e.g., GPUs).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **When to Avoid**\n",
    "- Extremely rugged loss landscapes (risk of trapping in local minima).\n",
    "- Non-differentiable or chaotic optimization problems.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Origin Story**  \n",
    "Generalized from Cauchy's original descent method into **vector calculus**,  \n",
    "pushed further during the rise of **large-scale machine learning** in the 1990s and 2000s,  \n",
    "when matrix/vector operations became fast enough to enable full multivariable updates per iteration.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ASCII Flow Diagram**\n",
    "\n",
    "```plaintext\n",
    "Initialize Œ∏ vector randomly\n",
    "  ‚Üì\n",
    "Compute Full Gradient ‚àáŒ∏ J(Œ∏)\n",
    "  ‚Üì\n",
    "Update Œ∏ ‚Üê Œ∏ - Œ± ‚àáŒ∏ J(Œ∏)\n",
    "  ‚Üì\n",
    "Evaluate New Loss\n",
    "  ‚Üì\n",
    "Repeat until convergence\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Minimize multivariable functions |  \n",
    "| ML | Optimize weights for better predictions |  \n",
    "| DL | Update all neurons‚Äô parameters |  \n",
    "| LLM | Fine-tune massive parameter matrices |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "Update rule for parameter vector $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta \\in \\mathbb{R}^n$ is an $n$-dimensional parameter vector\n",
    "- $\\nabla_\\theta J(\\theta)$ is the gradient vector of partial derivatives\n",
    "\n",
    "Expanded:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\left[ \\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\dotsc, \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\right]^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üåü **Limit Cases**\n",
    "\n",
    "- $\\nabla_\\theta J(\\theta) = 0$ ‚Üí Model reached a critical point (minimum, maximum, or saddle).\n",
    "- Extremely small $\\alpha$ ‚Üí Tiny parameter updates ‚Üí Very slow convergence.\n",
    "- Extremely large $\\alpha$ ‚Üí Chaotic updates ‚Üí Divergence.\n",
    "\n",
    "**Physical Meaning**:  \n",
    "*Like trying to cross a mountain range with either tiny baby steps (too slow) or giant reckless leaps (falling everywhere).*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $\\theta$ | Current parameter vector | Mountaineer's coordinates | Drifting without guide if gradient ignored |  \n",
    "| $\\alpha$ | Step size | Stride length | Tiny steps or reckless jumps |  \n",
    "| $\\nabla_\\theta J(\\theta)$ | Gradient vector | Local slopes in each direction | Zero ‚Üí Flat ground |  \n",
    "| Update rule | Movement | New coordinates after step | Depends on all slopes |  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small gradients | Small updates across all parameters | Slow convergence |  \n",
    "| Large gradients | Big parameter jumps | Instability risk |  \n",
    "| Mixed gradients | Some coordinates flat, some steep | Directional adjustment needed |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Loss is differentiable | Needed to compute $\\nabla_\\theta J(\\theta)$ | Discontinuous loss |  \n",
    "| Learning rate tuned | Needed for stable updates | Divergence risk |  \n",
    "| Surface shape predictable | Helps descent logic | Chaotic surfaces trap model |\n",
    "\n",
    "---\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Differentiability | Can't compute gradient | Activation kinks | Subgradient methods |  \n",
    "| Learning rate | Oscillation or slow death | Too high or too low $\\alpha$ | Schedule $\\alpha$ |  \n",
    "| Predictable surface | Unstable updates | GAN training | Adaptive optimizers |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Instantaneous Loss | $J(\\theta)$ | Evaluate progress | Energy at current step |  \n",
    "| Gradient Norm | $\\|\\nabla_\\theta J(\\theta)\\|$ | Check steepness | Force of update |  \n",
    "| Loss Difference | $J(\\theta_{\\text{old}}) - J(\\theta_{\\text{new}})$ | Step progress | Energy drop |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Gradient computation | $O(n)$ | $O(n)$ | Linear in number of parameters |  \n",
    "| Parameter update | $O(n)$ | $O(n)$ | Easily scalable via vector ops |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy (PEP8 + Vectorized)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def multivar_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Multivariable gradient descent.\n",
    "\n",
    "    Args:\n",
    "        theta_init (np.ndarray): Initial parameter vector, shape (n,)\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function returning gradient vector.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Optimized parameter vector.\n",
    "    \"\"\"\n",
    "    theta = theta_init.copy()\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta -= alpha * grad\n",
    "    return theta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def multivar_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Multivariable gradient descent using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        theta_init (torch.Tensor): Initial parameter vector.\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function to compute gradient vector.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimized parameter vector.\n",
    "    \"\"\"\n",
    "    theta = theta_init.clone().detach()\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta = theta - alpha * grad\n",
    "    return theta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def multivar_gradient_descent(theta_init, alpha, grad_fn, num_iters):\n",
    "    \"\"\"\n",
    "    Multivariable gradient descent using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        theta_init (tf.Tensor): Initial parameter vector.\n",
    "        alpha (float): Learning rate.\n",
    "        grad_fn (callable): Function to compute gradient vector.\n",
    "        num_iters (int): Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Optimized parameter vector.\n",
    "    \"\"\"\n",
    "    theta = tf.Variable(theta_init, dtype=tf.float32)\n",
    "    for _ in range(num_iters):\n",
    "        grad = grad_fn(theta)\n",
    "        theta.assign_sub(alpha * grad)\n",
    "    return theta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example: Gradient Descent (Multivariable)** \n",
    "\n",
    "We will **atomic-break** a full multivariable gradient descent step-by-step ‚Äî  \n",
    "no math skips, **every tiny operation fully visible and physically executable.**  \n",
    "\n",
    "---\n",
    "\n",
    "**Given:**\n",
    "\n",
    "- Loss function:  \n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = (\\theta_0 + 2\\theta_1 - 4)^2\n",
    "$$\n",
    "\n",
    "- Initial parameters:  \n",
    "$$\n",
    "\\theta_0 = 0,\\quad \\theta_1 = 0\n",
    "$$\n",
    "\n",
    "- Learning rate:  \n",
    "$$\n",
    "\\alpha = 0.1\n",
    "$$\n",
    "\n",
    "- Gradient components:  \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_0} = 2(\\theta_0 + 2\\theta_1 - 4)\n",
    "$$  \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_1} = 4(\\theta_0 + 2\\theta_1 - 4)\n",
    "$$\n",
    "\n",
    "We will take **3 full steps** manually.  \n",
    "\n",
    "---\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:-----|:----------|:-----------------|:-------------|  \n",
    "| 1 | Compute $g_0$ | $2(0 + 2(0) - 4)$ | $-8$ |  \n",
    "| 2 | Compute $g_1$ | $4(0 + 2(0) - 4)$ | $-16$ |  \n",
    "| 3 | Update $\\theta_0$ | $0 - 0.1 \\times (-8)$ | $0.8$ |  \n",
    "| 4 | Update $\\theta_1$ | $0 - 0.1 \\times (-16)$ | $1.6$ |  \n",
    "| 5 | Compute $g_0$ (new) | $2(0.8 + 2(1.6) - 4)$ | $1.6$ |  \n",
    "| 6 | Compute $g_1$ (new) | $4(0.8 + 2(1.6) - 4)$ | $3.2$ |  \n",
    "| 7 | Update $\\theta_0$ | $0.8 - 0.1 \\times 1.6$ | $0.64$ |  \n",
    "| 8 | Update $\\theta_1$ | $1.6 - 0.1 \\times 3.2$ | $1.28$ |  \n",
    "| 9 | Compute $g_0$ (new) | $2(0.64 + 2(1.28) - 4)$ | $-2.56$ |  \n",
    "| 10 | Compute $g_1$ (new) | $4(0.64 + 2(1.28) - 4)$ | $-5.12$ |  \n",
    "| 11 | Update $\\theta_0$ | $0.64 - 0.1 \\times (-2.56)$ | $0.896$ |  \n",
    "| 12 | Update $\\theta_1$ | $1.28 - 0.1 \\times (-5.12)$ | $1.792$ |\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Values After 3 Steps:** \n",
    "\n",
    "- $\\theta_0 \\approx 0.896$  \n",
    "- $\\theta_1 \\approx 1.792$\n",
    "\n",
    "**The mountaineer (model)** started lost at $(0,0)$ ‚Äî   \n",
    "but **step-by-step, feeling every local slope, adjusting both legs at once,**  \n",
    "they moved closer to the valley where $J(\\theta_0, \\theta_1)$ is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• **Theory Deepening: Gradient Descent (Multivariable)** \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q1:** Why must we update **all parameters simultaneously** in multivariable gradient descent?\n",
    "\n",
    "**A1:** Because each parameter depends on the others ‚Äî the loss surface slopes differently along each axis, and ignoring any dimension would break the true downhill path.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2:** What happens if we ignore parameter scaling (different magnitudes across $\\theta$)?\n",
    "\n",
    "**A2:** Parameters with large scales dominate descent, leading to zig-zagging paths and slow convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3:** Why do we need vectorized operations in multivariable descent?\n",
    "\n",
    "**A3:** Vectorization allows simultaneous updates of all parameters, massively speeding up computation (especially on GPUs and TPUs).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Gradient Descent (Multivariable)**\n",
    "\n",
    "**Scenario:**  \n",
    "You are training a multivariable model and observe that some parameters converge much slower than others.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "**Feature scaling issue** ‚Üí Some features dominate gradients.\n",
    "\n",
    "2. **Action:**  \n",
    "**Apply feature normalization** to balance parameter updates.\n",
    "\n",
    "3. **Calculation:**  \n",
    "Standardize each feature:  \n",
    "$$\n",
    "x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$  \n",
    "to have zero mean and unit variance.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | CONCEPT | PARAMETER | BEHAVIOR |  \n",
    "|:--------|:--------|:----------|:---------|  \n",
    "| **Multivariable GD** | Feature scale | Unequal scaling | Zig-zag, slow convergence |\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Unequal feature scaling** ‚Üí Some steps too small, some too large.  \n",
    "2. **Standardize features** ‚Üí Equal gradient influence across dimensions.  \n",
    "3. **Result** ‚Üí Faster, more stable convergence.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "**For \"Multivariable Gradient Descent\" in LLMs:**  \n",
    "\n",
    "**Scenario:**  \n",
    "During LLM pretraining, attention layer weights update unevenly ‚Äî some heads dominate.\n",
    "\n",
    "1. **Diagnosis:** Unbalanced gradient flow across multi-head attention.\n",
    "\n",
    "2. **Action:** Apply gradient clipping or scale normalization across heads.\n",
    "\n",
    "3. **Calculation:** Bound each gradient norm to a maximum value (e.g., 1.0).\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answers**</summary>  \n",
    "\n",
    "1. **Gradient domination** ‚Üí Attention collapse.  \n",
    "2. **Gradient clipping** ‚Üí Balance updates.  \n",
    "3. **Result** ‚Üí Healthier, more expressive heads.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Bottou, 2010 | SGD for large-scale learning | Gradient updates in multivariable systems |  \n",
    "| Kingma & Ba, 2015 (Adam) | Adaptive scaling of gradients | Solution to uneven updates |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular | Zig-zag loss decrease | Financial risk modeling | Feature scale imbalance |  \n",
    "| NLP | Diverging embeddings | Overactive token heads | Uneven gradient norms |  \n",
    "| CV | Blurry convolutional filters | Dominant input channels | Scale imbalance |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Scale features | Smoother updates | Gradient norm variance | Decrease |  \n",
    "| Use different learning rates per parameter | Adapt faster | Steps to convergence | Decrease |  \n",
    "| Clip gradients | Prevent instability | Training loss curve | Smoother |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How can we detect feature imbalance dynamically during descent?**  \n",
    "  *Why hard: Feature influence shifts during training.*\n",
    "\n",
    "- **What is the best universal scaling method across domains (images, text, tabular)?**  \n",
    "  *Why hard: Each data type behaves differently.*\n",
    "\n",
    "- **Can attention mechanisms be optimized with parameter-specific learning rates?**  \n",
    "  *Why hard: Massive parameter counts and entanglement.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Improper feature scaling amplifies biases (e.g., overfitting common groups).  \n",
    "  *Mitigation: Normalize and monitor subgroup behaviors separately.*\n",
    "\n",
    "‚Ä¢ **Risk**: Model overfocuses on high-variance features (e.g., race, income).  \n",
    "  *Mitigation: Equalize feature contributions carefully.*\n",
    "\n",
    "‚Ä¢ **Risk**: Parameter explosion hides biased convergence paths.  \n",
    "  *Mitigation: Regular audits of parameter movement.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Should ML models adapt learning rates for sensitive features (e.g., gender, race) differently to ensure fairness?\"*\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "- **Deployment Gotchas**  \n",
    "  PyTorch optimizers like SGD expect **pre-scaled** features ‚Äî otherwise expect weird convergence rates.\n",
    "\n",
    "- **Scaling Limits**  \n",
    "  Vanilla multivariable descent struggles when $n \\gg m$ (more parameters than data points).\n",
    "\n",
    "- **Production Fixes**  \n",
    "  Always visualize parameter norms over time ‚Äî exploding ones warn of instability.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Optimize structural design | Multi-variable adjustments |  \n",
    "| Robotics | Arm movement learning | Simultaneous joint optimization |  \n",
    "| Finance | Risk-return optimization | Portfolio parameter tuning |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1847: Single-variable gradient descent\n",
    "‚Üí 1940s: Introduction of vector calculus in optimization\n",
    "‚Üí 1990s: Machine learning multivariable optimization\n",
    "‚Üí 2010s: Adaptive optimizers for deep learning\n",
    "‚Üí 2020s: Domain-specific gradient strategies (NLP, CV, Tabular)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Adaptive Feature Scaling** ‚Üí Dynamically normalize during training.  \n",
    "- **Curvature-Aware Multivariable Updates** ‚Üí Preconditioned descent methods.  \n",
    "- **Fair Gradient Descent** ‚Üí Bias-aware learning rate adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Full Simulation: Gradient Descent (Multivariable) + ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Setup\n",
    "# Minimize a simple quadratic cost: J(Œ∏‚ÇÄ, Œ∏‚ÇÅ) = Œ∏‚ÇÄ¬≤ + Œ∏‚ÇÅ¬≤\n",
    "# Global minimum at (Œ∏‚ÇÄ, Œ∏‚ÇÅ) = (0,0)\n",
    "\n",
    "def generate_data():\n",
    "    # No external data needed; the surface is implicit\n",
    "    return None\n",
    "\n",
    "# üîÅ Core logic: Apply Multivariable Gradient Descent\n",
    "def apply_concept(theta_init, learning_rate, epochs):\n",
    "    theta = np.array(theta_init, dtype=float)\n",
    "    history = {'theta0': [], 'theta1': [], 'loss': []}\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Step 1: Forward pass\n",
    "        loss = theta[0]**2 + theta[1]**2  # J(Œ∏) = Œ∏‚ÇÄ¬≤ + Œ∏‚ÇÅ¬≤\n",
    "\n",
    "        # Step 2: Compute Gradient\n",
    "        grad = np.array([2 * theta[0], 2 * theta[1]])  # ‚àáJ(Œ∏)\n",
    "\n",
    "        # Step 3: Update Parameters: Œ∏ := Œ∏ - Œ±‚àáJ(Œ∏)\n",
    "        theta -= learning_rate * grad\n",
    "\n",
    "        # Step 4: Log\n",
    "        history['theta0'].append(theta[0])\n",
    "        history['theta1'].append(theta[1])\n",
    "        history['loss'].append(loss)\n",
    "\n",
    "    return history\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # üîπ Left: Optimization path on contour\n",
    "    theta0_vals = np.linspace(-5, 5, 100)\n",
    "    theta1_vals = np.linspace(-5, 5, 100)\n",
    "    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "    Z = T0**2 + T1**2  # Cost function surface\n",
    "\n",
    "    axs[0].contour(T0, T1, Z, levels=30)\n",
    "    axs[0].plot(history['theta0'], history['theta1'], marker='o', color='red')\n",
    "    axs[0].set_title(\" Optimization Path on Loss Surface\")\n",
    "    axs[0].set_xlabel(\"Œ∏‚ÇÄ\")\n",
    "    axs[0].set_ylabel(\"Œ∏‚ÇÅ\")\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # üîπ Right: Loss over epochs\n",
    "    axs[1].plot(history['loss'], marker='o')\n",
    "    axs[1].set_title(\" Loss J(Œ∏) Over Epochs\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Simulator\n",
    "def interactive_sim(theta0_init, theta1_init, learning_rate, epochs):\n",
    "    generate_data()\n",
    "    theta_init = [theta0_init, theta1_init]\n",
    "    history = apply_concept(theta_init, learning_rate, epochs)\n",
    "    plot_results(history)\n",
    "\n",
    "# üß∞ Widgets\n",
    "theta0_slider = widgets.FloatSlider(\n",
    "    value=4.0,\n",
    "    min=-5.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Œ∏‚ÇÄ init:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "theta1_slider = widgets.FloatSlider(\n",
    "    value=4.0,\n",
    "    min=-5.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Œ∏‚ÇÅ init:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "lr_slider = widgets.FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.001,\n",
    "    max=1.0,\n",
    "    step=0.001,\n",
    "    description='Learning Rate:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "epoch_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Epochs:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI\n",
    "ui = widgets.VBox([theta0_slider, theta1_slider, lr_slider, epoch_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'theta0_init': theta0_slider,\n",
    "        'theta1_init': theta1_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'epochs': epoch_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be07706",
   "metadata": {},
   "source": [
    "# <a id=\"vectorization\"></a>‚ö° Vectorization for Speedup  \n",
    "\n",
    "> *Vectorization is rewriting operations to occur across entire arrays simultaneously, not element-by-element.*  \n",
    "> *Mechanical Analogy*: Like replacing a single hammer worker with a machine that hammers thousands at once.\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Needed for fast model training across large datasets.\n",
    "- **DL**: Neural networks require tensor ops for batch updates.\n",
    "- **LLMs**: Transformer operations on massive matrices depend on it.\n",
    "- **AGI**: Scaling up to brain-size models depends on vector ops.\n",
    "\n",
    "### 2. **Mechanical Analogy**\n",
    "Painting a wall ‚Äî  \n",
    "- Without vectorization: tiny brush, slow.  \n",
    "- With vectorization: spray gun painting meters at a time.\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Goodfellow et al., 2016 ‚Äî *Deep Learning* ‚Äî tensorization's criticality.\n",
    "- Ruder, 2017 ‚Äî *Optimization Algorithms* ‚Äî efficiency via vector ops.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Matrix multiplication |  \n",
    "| ML | Batch prediction computation |  \n",
    "| DL | Parallel forward pass |  \n",
    "| LLMs | Multi-token attention ops |  \n",
    "| Research/AGI | Simulate millions of agents |  \n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Vectorization**: Operating on whole arrays. *Analogous to spraying paint on a wall.*  \n",
    "‚Ä¢ **Matrix Multiplication**: Combining two arrays into one. *Analogous to mixing paints together.*  \n",
    "‚Ä¢ **Tensor Operation**: Multi-axis array math. *Analogous to folding many sheets at once.*  \n",
    "‚Ä¢ **Broadcasting**: Expand smaller arrays automatically. *Analogous to duplicating a part to fill a machine.*  \n",
    "‚Ä¢ **Batch Processing**: Parallel sample computation. *Analogous to baking multiple pizzas simultaneously.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### Purpose\n",
    "- Speeding up execution by operating on full arrays.\n",
    "- Parallel training across samples.\n",
    "- Reducing programming errors in manual loops.\n",
    "\n",
    "### When to Avoid\n",
    "- If memory can't fit full arrays (OOM errors).\n",
    "- When models are extremely small (tiny datasets).\n",
    "\n",
    "### Origin Story\n",
    "Matrix math optimization became prominent during 1960s supercomputing, expanded during 2000s with NumPy, and exploded in deep learning's GPU revolution (2012+).\n",
    "\n",
    "```plaintext\n",
    "Slow elementwise loops\n",
    "‚Üí Identify repeating patterns\n",
    "‚Üí Rewrite full array operations\n",
    "‚Üí Massive parallel execution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Faster large-scale calculations |  \n",
    "| ML | Process samples together |  \n",
    "| DL | Handle neuron outputs at once |  \n",
    "| LLM | Transform all tokens at once |  \n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "$$\n",
    "\\hat{y} = X\\theta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $X \\in \\mathbb{R}^{m \\times n}$\n",
    "- $\\theta \\in \\mathbb{R}^{n \\times 1}$\n",
    "- $\\hat{y} \\in \\mathbb{R}^{m \\times 1}$\n",
    "\n",
    "**Limit Cases**\n",
    "- $m = 1$: One sample.\n",
    "- $n = 1$: One feature.\n",
    "- $X = 0$: Always zero output.\n",
    "\n",
    "**Physical Meaning**\n",
    "Spray-painting an entire row instantly instead of brick-by-brick.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Dot product |  \n",
    "| ML | Vectorized loss calculation |  \n",
    "| DL | Tensor activations |  \n",
    "| LLMs | Embedding matrices |  \n",
    "| Research/AGI | Scaling brain simulations |  \n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $X$ | Feature matrix | Wall made of bricks | No bricks, no wall |  \n",
    "| $\\theta$ | Weight vector | Paint color | Bad color, bad wall |  \n",
    "| $X\\theta$ | Sum of influences | Final painted surface | Bad paint mix = bad output |  \n",
    "| $\\hat{y}$ | Predictions | Completed paint job | Blank if no data |  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Very small | Tiny updates | Convergence slow |  \n",
    "| Very large | Big jumps | Instability |  \n",
    "| Mixed | Some stuck, some overshoot | Requires tuning |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Matching shapes | Matrix math needs alignment | Error: incompatible dimensions |  \n",
    "| Enough memory | Vector ops can be memory-heavy | Crash during training |  \n",
    "| Hardware optimized | CPUs/GPUs expect parallel ops | Slowness otherwise |  \n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Shape mismatch | Code crash | Wrong tensor reshape | Validate shapes |  \n",
    "| OOM | Memory crash | BERT fine-tuning | Smaller batches |  \n",
    "| No hardware support | 10x slowdown | CPU fallback | Use cloud GPU |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Prediction error | $||\\hat{y} - y||$ | Accuracy measure | Lower = better fit |  \n",
    "| Gradient norm | $||\\nabla_\\theta J||$ | Training force | Should shrink |  \n",
    "| Loss change | $J_{\\text{old}} - J_{\\text{new}}$ | Progress tracking | Should be positive |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Dot product | $O(n)$ | $O(1)$ | Linear |  \n",
    "| Matrix-vector | $O(mn)$ | $O(m)$ | Batch processing |  \n",
    "| Tensor ops | $O(n^3)$ worst case | $O(n^2)$ | Needs optimization |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Vectorized prediction using NumPy.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): (m, n)\n",
    "        theta (np.ndarray): (n,)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (m,)\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2\n",
    "    assert theta.ndim == 1\n",
    "    return np.dot(X, theta)\n",
    "```\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Vectorized prediction using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): (m, n)\n",
    "        theta (torch.Tensor): (n,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (m,)\n",
    "    \"\"\"\n",
    "    assert X.dim() == 2\n",
    "    assert theta.dim() == 1\n",
    "    return torch.matmul(X, theta)\n",
    "```\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Vectorized prediction using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        X (tf.Tensor): (m, n)\n",
    "        theta (tf.Tensor): (n,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: (m,)\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(X, 2)\n",
    "    tf.debugging.assert_rank(theta, 1)\n",
    "    return tf.linalg.matvec(X, theta)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Debug & Fix Examples**\n",
    "\n",
    "| Symptom | Root Cause | Fix |  \n",
    "|:--------|:-----------|:----|  \n",
    "| Shape mismatch error | X rows vs theta cols mismatch | Align shapes |  \n",
    "| Out-of-memory crash | Arrays too big | Use batching |  \n",
    "| Slow loop execution | Manual Python loops | Use dot/matmul ops |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:----:|:----------|:-----------------|:------------:|  \n",
    "| 1 | Multiply 1st row, 1st col | $1 \\times 0.5$ | $0.5$ |  \n",
    "| 2 | Multiply 1st row, 2nd col | $2 \\times 1.0$ | $2.0$ |  \n",
    "| 3 | Sum row 1 results | $0.5 + 2.0$ | $2.5$ |  \n",
    "| 4 | Multiply 2nd row, 1st col | $3 \\times 0.5$ | $1.5$ |  \n",
    "| 5 | Multiply 2nd row, 2nd col | $4 \\times 1.0$ | $4.0$ |  \n",
    "| 6 | Sum row 2 results | $1.5 + 4.0$ | $5.5$ |  \n",
    "| 7 | Multiply 3rd row, 1st col | $5 \\times 0.5$ | $2.5$ |  \n",
    "| 8 | Multiply 3rd row, 2nd col | $6 \\times 1.0$ | $6.0$ |  \n",
    "| 9 | Sum row 3 results | $2.5 + 6.0$ | $8.5$ |\n",
    "\n",
    "Final output:\n",
    "$$\n",
    "\\hat{y} = \\begin{bmatrix} 2.5 \\\\ 5.5 \\\\ 8.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üî• **Theory Deepening**\n",
    "\n",
    "### ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q:** Why does vectorization require shape matching?  \n",
    "**A:** Misaligned arrays cannot multiply properly ‚Äî just like mismatched puzzle pieces.\n",
    "\n",
    "**Q:** Why does vectorization crash on huge arrays?  \n",
    "**A:** Memory explodes ‚Äî too many elements to store at once.\n",
    "\n",
    "**Q:** Why does GPU need vectorization?  \n",
    "**A:** GPUs operate on 1000s of data points at once ‚Äî no vector, no parallelism.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Vectorization**\n",
    "\n",
    "Scenario:  \n",
    "Training slowed 100x because you used manual loops.  \n",
    "Training accuracy okay, but epochs take hours.\n",
    "\n",
    "1. **Diagnosis**: Serial execution bottleneck.\n",
    "2. **Action**: Rewrite using batch matrix ops.\n",
    "3. **Calculation**: $5$ sec ‚Üí $500$ sec per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "Transformer self-attention with 512 vs 2048 tokens:  \n",
    "- Attention matrix grows 16x ($512^2$ ‚Üí $2048^2$).  \n",
    "- Vectorized sparse attention can fix memory blowup.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Goodfellow et al., 2016 | Tensorization critical for DL | NNs and layers vectorized |  \n",
    "| Vaswani et al., 2017 | Attention needs big matrix ops | Transformers use vectorization |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular | Training freeze | Financial modeling stuck | Manual loops |  \n",
    "| NLP | OOM error | Transformer crash | Full attention unoptimized |  \n",
    "| CV | Laggy loading | Image augmentation slow | Pixelwise loops |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Vectorized batches | Speed gain | Epoch time | Drops massively |  \n",
    "| Blocked attention | Less VRAM | Memory peak | Decreases |  \n",
    "| Manual loops | Slow | Epoch time | Increases |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **Detecting non-vectorized ops automatically?**  \n",
    "  *Why hard:* Python hides loops easily.\n",
    "\n",
    "- **Sparse vector ops for memory?**  \n",
    "  *Why hard:* Irregularity makes batching complex.\n",
    "\n",
    "- **Neuromorphic tensor ops?**  \n",
    "  *Why hard:* Brains aren't clean matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Overoptimized code hides bias. *Mitigation:* Careful audits.  \n",
    "‚Ä¢ **Risk**: GPU use = CO2 emissions. *Mitigation:* Efficient ops.  \n",
    "‚Ä¢ **Risk**: Speed leads to low-quality deployments. *Mitigation:* Validation checks.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Should ML training prioritize energy efficiency over raw speed?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "**Deployment Gotchas**  \n",
    "- TF expects `batch_first=True` layouts.\n",
    "\n",
    "**Scaling Limits**  \n",
    "- Full tensor products explode $n>10^5$ ‚Äî chunk.\n",
    "\n",
    "**Production Fixes**  \n",
    "- Compile CUDA kernels for repeated ops.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Simultaneous stress test simulations | Matrix multiplications |  \n",
    "| Robotics | Batch motion prediction | Tensor ops |  \n",
    "| Finance | Parallel portfolio computation | Vectorized returns |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1960s: Early matrix ops\n",
    "‚Üí 2000s: NumPy accelerations\n",
    "‚Üí 2010s: PyTorch, TensorFlow tensors\n",
    "‚Üí 2020s: TPU-based massive ops\n",
    "‚Üí 2030+: Neuromorphic parallel arrays\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- Auto-vectorizing compilers.\n",
    "- Sparse efficient matrix ops.\n",
    "- Neuromorphic memory + ops fusion.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8112997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Full Simulation: Vectorization vs Non-Vectorized Speedup + ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Setup\n",
    "# Generate synthetic data (X, theta)\n",
    "def generate_data(m=1000, n=50):\n",
    "    X = np.random.randn(m, n)\n",
    "    theta = np.random.randn(n)\n",
    "    return X, theta\n",
    "\n",
    "# üîÅ Core Logic: Apply vectorized and non-vectorized operations\n",
    "def apply_concept(X, theta, vectorized=True):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    if vectorized:\n",
    "        # üöÄ Vectorized: XŒ∏\n",
    "        start = time.time()\n",
    "        predictions = X @ theta\n",
    "        end = time.time()\n",
    "    else:\n",
    "        # üêå Non-vectorized: Loop over each row\n",
    "        start = time.time()\n",
    "        predictions = []\n",
    "        for i in range(m):\n",
    "            predictions.append(np.dot(X[i], theta))\n",
    "        predictions = np.array(predictions)\n",
    "        end = time.time()\n",
    "\n",
    "    elapsed_time = end - start\n",
    "    return predictions, elapsed_time\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(times):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    methods = ['Vectorized', 'Non-Vectorized']\n",
    "    ax.bar(methods, times, color=['green', 'red'])\n",
    "    ax.set_title(\" Vectorization vs  Non-Vectorized Computation Time\")\n",
    "    ax.set_ylabel(\"Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Simulator\n",
    "def interactive_sim(sample_size, feature_size):\n",
    "    X, theta = generate_data(m=sample_size, n=feature_size)\n",
    "    \n",
    "    _, vectorized_time = apply_concept(X, theta, vectorized=True)\n",
    "    _, non_vectorized_time = apply_concept(X, theta, vectorized=False)\n",
    "    \n",
    "    plot_results([vectorized_time, non_vectorized_time])\n",
    "\n",
    "# üß∞ Widgets\n",
    "sample_size_slider = widgets.IntSlider(\n",
    "    value=1000,\n",
    "    min=100,\n",
    "    max=10000,\n",
    "    step=100,\n",
    "    description='Sample Size (m):',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "feature_size_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=5,\n",
    "    max=500,\n",
    "    step=5,\n",
    "    description='Feature Size (n):',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI\n",
    "ui = widgets.VBox([sample_size_slider, feature_size_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'sample_size': sample_size_slider,\n",
    "        'feature_size': feature_size_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea8223",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä <a id=\"evaluation-interpretation\"></a>**3. Evaluation & Interpretation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbbfdb",
   "metadata": {},
   "source": [
    "# <a id=\"r2-score\"></a>üìà R¬≤ Score  \n",
    "\n",
    "> *R¬≤ Score measures how well a model's predictions match the true outputs ‚Äî 1.0 means perfect, 0.0 means useless.*  \n",
    "> *Mechanical Analogy*: Like checking how tightly your darts cluster around the bullseye on a dartboard.\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Evaluates regression model quality.\n",
    "- **DL**: Validates neural nets on continuous outputs.\n",
    "- **LLMs**: Less direct, but useful for evaluating numerical text generation tasks.\n",
    "- **AGI**: Critical for real-world numeric prediction reliability.\n",
    "\n",
    "### 2. **Mechanical Analogy**\n",
    "Imagine throwing darts:\n",
    "- If your darts **cluster close to bullseye**, R¬≤ ‚âà 1.\n",
    "- If your darts **scatter randomly**, R¬≤ ‚âà 0.\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- G√©ron, 2019 ‚Äî *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow* ‚Äî R¬≤ standard for regression.\n",
    "- James et al., 2021 ‚Äî *An Introduction to Statistical Learning* ‚Äî formal R¬≤ properties.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Variance reduction |  \n",
    "| ML | Regression model evaluation |  \n",
    "| DL | Regression network performance |  \n",
    "| LLMs | Numeric data prediction accuracy |  \n",
    "| Research/AGI | Prediction model validation |\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **$R^2$ Score**: Proportion of variance explained. *Analogous to bullseye clustering.*  \n",
    "‚Ä¢ **$y$**: True output values. *Analogous to target dots.*  \n",
    "‚Ä¢ **$\\hat{y}$**: Predicted outputs. *Analogous to thrown darts.*  \n",
    "‚Ä¢ **$SS_\\text{res}$**: Residual sum of squares. *Analogous to total dart error.*  \n",
    "‚Ä¢ **$SS_\\text{tot}$**: Total sum of squares. *Analogous to overall spread of darts.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### Purpose\n",
    "- Quantify prediction accuracy (better = closer to 1.0).\n",
    "- Identify overfitting/underfitting easily.\n",
    "- Compare models even if different scales.\n",
    "\n",
    "### When to Avoid\n",
    "- When modeling **nonlinear** problems blindly.\n",
    "- When target variable has **extremely low variance**.\n",
    "\n",
    "### Origin Story\n",
    "Originated in **statistical modeling** (early 1900s), adapted for ML to **quantify \"goodness of fit\"** of regression models.\n",
    "\n",
    "```plaintext\n",
    "Start with predictions\n",
    "‚Üí Compare to true outputs\n",
    "‚Üí Measure variance captured\n",
    "‚Üí Output normalized goodness score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Fraction of variance explained |  \n",
    "| ML | Regression performance score |  \n",
    "| DL | Validate continuous output nets |  \n",
    "| LLM | Test numeric output accuracy |  \n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- Residual Sum of Squares:  \n",
    "$$\n",
    "SS_\\text{res} = \\sum (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "- Total Sum of Squares:  \n",
    "$$\n",
    "SS_\\text{tot} = \\sum (y_i - \\bar{y})^2\n",
    "$$\n",
    "- $\\bar{y}$ = mean of $y$ values.\n",
    "\n",
    "**Limit Cases**\n",
    "- $R^2 = 1$ ‚Üí Perfect model.\n",
    "- $R^2 = 0$ ‚Üí Predicting just mean of targets.\n",
    "- $R^2 < 0$ ‚Üí Worse than mean prediction.\n",
    "\n",
    "**Physical Meaning**\n",
    "How much better are your predictions than just guessing the mean?\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Variance decomposition |  \n",
    "| ML | Regression score |  \n",
    "| DL | Forecasting models |  \n",
    "| LLMs | Numerical output quality |  \n",
    "| Research/AGI | System model evaluations |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $y_i$ | True value | Target dart location | None if missing |  \n",
    "| $\\hat{y}_i$ | Predicted value | Actual dart hit | Random if bad model |  \n",
    "| $SS_\\text{res}$ | Sum of squared errors | Total dart error | Zero if perfect |  \n",
    "| $SS_\\text{tot}$ | Spread of target | Wall spread without throwing darts | Zero if all targets same |  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small residuals | Low $SS_\\text{res}$ | High R¬≤ |  \n",
    "| Large residuals | High $SS_\\text{res}$ | Low R¬≤ |  \n",
    "| $SS_\\text{tot}$ near zero | R¬≤ undefined or unstable |  \n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Linear relationship | R¬≤ assumes linear error reduction | Bad for non-linear tasks |  \n",
    "| Nonzero variance | $SS_\\text{tot} \\neq 0$ needed | All targets same value |  \n",
    "| Homoscedasticity | Equal variance of errors | Biased R¬≤ if not |\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Nonlinear mapping | R¬≤ misleading | Tree regressors | Use adjusted R¬≤ |  \n",
    "| Zero variance | R¬≤ division error | Predicting constant | Skip R¬≤ use |  \n",
    "| Heteroscedasticity | Biased R¬≤ | Varied output spread | Use robust metrics |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Residuals | $y_i - \\hat{y}_i$ | Raw error | Model miss distance |  \n",
    "| Residual sum | $\\sum (y_i - \\hat{y}_i)^2$ | Total prediction error | Lower better |  \n",
    "| R¬≤ score | $1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}$ | Relative model strength | Closer to 1 = better |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Mean calculation | $O(n)$ | $O(1)$ | Linear |  \n",
    "| Residual sum | $O(n)$ | $O(1)$ | Linear |  \n",
    "| R¬≤ calculation | $O(1)$ | $O(1)$ | Cheap |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute R¬≤ Score using NumPy.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): (n,)\n",
    "        y_pred (np.ndarray): (n,)\n",
    "\n",
    "    Returns:\n",
    "        float: R¬≤ score\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute R¬≤ Score using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): (n,)\n",
    "        y_pred (torch.Tensor): (n,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar R¬≤ score\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute R¬≤ Score using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        y_true (tf.Tensor): (n,)\n",
    "        y_pred (tf.Tensor): (n,)\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Scalar R¬≤ score\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_equal(tf.shape(y_true), tf.shape(y_pred))\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / ss_tot\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:----:|:----------|:-----------------|:------------:|  \n",
    "| 1 | Compute mean of $y$ | $\\bar{y} = \\frac{3 + 5 + 7}{3}$ | $5.0$ |  \n",
    "| 2 | Compute residual 1 | $(3 - 2)^2$ | $1$ |  \n",
    "| 3 | Compute residual 2 | $(5 - 5)^2$ | $0$ |  \n",
    "| 4 | Compute residual 3 | $(7 - 8)^2$ | $1$ |  \n",
    "| 5 | Sum residuals | $1 + 0 + 1$ | $2$ |  \n",
    "| 6 | Compute total 1 | $(3 - 5)^2$ | $4$ |  \n",
    "| 7 | Compute total 2 | $(5 - 5)^2$ | $0$ |  \n",
    "| 8 | Compute total 3 | $(7 - 5)^2$ | $4$ |  \n",
    "| 9 | Sum totals | $4 + 0 + 4$ | $8$ |  \n",
    "| 10 | Compute R¬≤ Score | $1 - \\frac{2}{8}$ | $0.75$ |\n",
    "\n",
    "---\n",
    "\n",
    "**Inputs:**  \n",
    "- True values: $y = [3, 5, 7]$  \n",
    "- Predicted values: $\\hat{y} = [2, 5, 8]$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Output:**\n",
    "\n",
    "$$\n",
    "R^2 = 0.75\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation in words:**\n",
    "\n",
    "- The model captures **75%** of the total variance compared to simply predicting the mean.  \n",
    "- The closer $R^2$ is to $1$, the better the model fits the true data.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Fraction of explained variance |  \n",
    "| ML | Regression performance metric |  \n",
    "| DL | Output layer evaluation for continuous predictions |  \n",
    "| LLMs | Evaluate generated numeric values |  \n",
    "| Research/AGI | Model prediction calibration |\n",
    "\n",
    "---\n",
    "## üî• **Theory Deepening**\n",
    "\n",
    "### ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q:** Why can $R^2$ be negative even though it sounds like a \"goodness\" score?\n",
    "\n",
    "**A:**  \n",
    "Because if your model predictions are worse than just predicting the mean, $SS_\\text{res}$ becomes larger than $SS_\\text{tot}$, making $1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}$ negative.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** What does $R^2 = 0$ tell you about your model?\n",
    "\n",
    "**A:**  \n",
    "Your model is no better than just predicting the average target value every time ‚Äî it explains **zero variance**.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** Why is $R^2$ unstable when $SS_\\text{tot} = 0$?\n",
    "\n",
    "**A:**  \n",
    "If all true values $y_i$ are identical, their variance is zero, making $SS_\\text{tot}$ zero, leading to **division by zero** or undefined $R^2$.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Division instability with small denominators |  \n",
    "| ML | Misleading model evaluation on constant targets |  \n",
    "| DL | Output layer degenerate behavior |  \n",
    "| LLMs | Constant prediction error analysis |  \n",
    "| Research/AGI | Robustness against zero-variance scenarios |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: R¬≤ Score**\n",
    "\n",
    "**Scenario:**  \n",
    "You fit a regression model.  \n",
    "Training $R^2 = 0.99$, but Validation $R^2 = 0.30$.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "- Severe overfitting ‚Äî model memorizes training but generalizes poorly.\n",
    "\n",
    "2. **Action:**  \n",
    "- Regularize the model (e.g., Ridge/Lasso), or simplify it.\n",
    "\n",
    "3. **Calculation:**  \n",
    "- If regularization shrinks coefficients, Validation $R^2$ might improve from $0.30$ to $0.70$.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | R¬≤ Score | Parameter | Behavior |  \n",
    "|:--------|:---------|:----------|:---------|  \n",
    "| **Regularization** | Overfitting control | $\\lambda$ (penalty weight) | Smooths model coefficients |\n",
    "\n",
    "---\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Overfitting** ‚Üí Huge gap between train/val R¬≤  \n",
    "2. **Regularization** ‚Üí Shrinks unnecessary model weights  \n",
    "3. **Calculation** ‚Üí Validation R¬≤ improves significantly  \n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "Transformer language models predicting numeric answers (e.g., math questions).\n",
    "\n",
    "- Prediction close to ground-truth ‚Üí High $R^2$.\n",
    "- Random numeric guess ‚Üí Low or negative $R^2$.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| James et al., 2021 | Statistical basis of $R^2$ | Essential for regression model evaluation |  \n",
    "| G√©ron, 2019 | $R^2$ in practical ML tasks | Measuring model fit for real-world datasets |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular Data | Negative $R^2$ | Energy consumption prediction fails | Model worse than mean |  \n",
    "| NLP | Instability | Number prediction chaotic | Constant outputs |  \n",
    "| CV | Bad pixel regression | Blurry output | Non-linear pattern ignored |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Increase regularization | Reduce overfitting | Validation R¬≤ | Increase |  \n",
    "| Remove noisy features | Cleaner mapping | Residuals | Decrease |  \n",
    "| Use ensemble models | Stability across folds | R¬≤ standard deviation | Decrease |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How to create R¬≤ equivalents for structured data (trees, graphs)?**  \n",
    "  *Why hard:* No clear \"mean\" to compare against.\n",
    "\n",
    "- **How to modify R¬≤ to penalize overconfident wrong models?**  \n",
    "  *Why hard:* Standard R¬≤ only measures spread, not uncertainty.\n",
    "\n",
    "- **What happens to R¬≤ in extremely high-dimensional spaces?**  \n",
    "  *Why hard:* Curse of dimensionality distorts residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: High R¬≤ can hide bias in underrepresented groups. *Mitigation: Grouped R¬≤ audits.*  \n",
    "‚Ä¢ **Risk**: Overfitting on spurious correlations inflates R¬≤. *Mitigation: Proper cross-validation.*  \n",
    "‚Ä¢ **Risk**: Bad R¬≤ interpretation ‚Üí false claims of model strength. *Mitigation: Educate stakeholders.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Should R¬≤ be replaced by more robust metrics like MAE or RMSE in production systems?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "**Deployment Gotchas**  \n",
    "- R¬≤ for training only: Always compute on **validation** sets too.\n",
    "\n",
    "**Scaling Limits**  \n",
    "- In high-dimensional data, R¬≤ tends to overestimate performance ‚Äî prefer cross-validated R¬≤.\n",
    "\n",
    "**Production Fixes**  \n",
    "- Implement R¬≤ **plus** other metrics (MAE, RMSE) for balanced model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Predicting mechanical failure time | Model goodness-of-fit |  \n",
    "| Finance | Predicting stock returns | Regression evaluation |  \n",
    "| Robotics | Predicting future arm positions | Trajectory fitting quality |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "Early 1900s: R¬≤ emerges in basic statistics\n",
    "‚Üí 1950s: Linear regression models\n",
    "‚Üí 2000s: R¬≤ widely adopted in ML\n",
    "‚Üí 2020s: Extensions for complex systems (graphs, sequences)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Robust R¬≤** that accounts for uncertainty.\n",
    "- **Sparse R¬≤** for high-dimensional data.\n",
    "- **Domain-specific R¬≤ variants** (e.g., for structured text or graph outputs).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ Full Simulation: R¬≤ Score Visualization + ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Setup\n",
    "# Generate synthetic dataset\n",
    "def generate_data(samples=100, noise=0.1, nonlinear=False):\n",
    "    np.random.seed(0)\n",
    "    X = np.linspace(0, 10, samples)\n",
    "    \n",
    "    if nonlinear:\n",
    "        y = np.sin(X) + noise * np.random.randn(samples)\n",
    "    else:\n",
    "        y = 2 * X + 1 + noise * np.random.randn(samples)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# üîÅ Core logic: Calculate R¬≤ Score\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "# üî• Model Predictor\n",
    "def simple_model(X, mode=\"linear\"):\n",
    "    if mode == \"linear\":\n",
    "        return 2 * X + 1\n",
    "    elif mode == \"constant\":\n",
    "        return np.full_like(X, np.mean(X))\n",
    "    elif mode == \"random\":\n",
    "        return np.random.randn(len(X)) * 10\n",
    "    elif mode == \"sine\":\n",
    "        return np.sin(X)\n",
    "    else:\n",
    "        return np.zeros_like(X)\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(X, y_true, y_pred, r2_score):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(X, y_true, label=\"True Data\", color=\"blue\")\n",
    "    plt.plot(X, y_pred, label=f\"Predictions (R¬≤={r2_score:.2f})\", color=\"red\")\n",
    "    plt.title(\"üìà R¬≤ Score Visualization\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Simulator\n",
    "def interactive_sim(samples, noise, model_type, data_nonlinear):\n",
    "    X, y_true = generate_data(samples=samples, noise=noise, nonlinear=data_nonlinear)\n",
    "    y_pred = simple_model(X, mode=model_type)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    plot_results(X, y_true, y_pred, r2)\n",
    "\n",
    "# üß∞ Widgets\n",
    "sample_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=20,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Samples:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "noise_slider = widgets.FloatSlider(\n",
    "    value=0.2,\n",
    "    min=0.0,\n",
    "    max=2.0,\n",
    "    step=0.05,\n",
    "    description='Noise:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\"linear\", \"constant\", \"random\", \"sine\"],\n",
    "    value=\"linear\",\n",
    "    description='Model:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "data_type_toggle = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Nonlinear Data',\n",
    "    tooltip='Toggle between linear and nonlinear data generation',\n",
    "    icon='random'\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI\n",
    "ui = widgets.VBox([sample_slider, noise_slider, model_dropdown, data_type_toggle])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'samples': sample_slider,\n",
    "        'noise': noise_slider,\n",
    "        'model_type': model_dropdown,\n",
    "        'data_nonlinear': data_type_toggle\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3585d",
   "metadata": {},
   "source": [
    "# <a id=\"underfitting-diagnostics\"></a>ü©∫ Underfitting & Model Diagnostics  \n",
    "\n",
    "> *Underfitting happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance even on the training set.*  \n",
    "> *Mechanical Analogy*: Like using a straight ruler to trace a complicated spiral ‚Äî no matter how careful you are, it will always be wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Ensures models learn sufficient patterns without being too rigid.\n",
    "- **DL**: Detects when networks are too shallow or constrained.\n",
    "- **LLMs**: Guides model size, depth, and token prediction strategies.\n",
    "- **AGI**: Fundamental to ensuring sufficient \"capacity\" to capture complex realities.\n",
    "\n",
    "### 2. **Mechanical Analogy**\n",
    "Imagine trying to map mountain roads with a **straight line** ‚Äî  \n",
    "You miss all the curves, leading to terrible navigation maps (training and testing failures).\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Goodfellow et al., 2016 ‚Äî *Deep Learning* ‚Äî bias-variance tradeoff and underfitting.\n",
    "- Shalev-Shwartz and Ben-David, 2014 ‚Äî *Understanding Machine Learning* ‚Äî formal underfitting definitions.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Approximation theory |  \n",
    "| ML | Bias-variance decomposition |  \n",
    "| DL | Insufficient network depth |  \n",
    "| LLMs | Underpowered attention heads |  \n",
    "| Research/AGI | Limited generalization capacity |\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Underfitting**: Model too simple. *Analogous to using a straightedge to trace spirals.*  \n",
    "‚Ä¢ **Bias**: Error from wrong assumptions. *Analogous to badly drawn maps.*  \n",
    "‚Ä¢ **Variance**: Error from sensitivity to noise. *Analogous to shaky hands.*  \n",
    "‚Ä¢ **Capacity**: Model‚Äôs flexibility level. *Analogous to tool complexity.*  \n",
    "‚Ä¢ **Diagnostics**: Systematic model performance checks. *Analogous to car checkups.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### Purpose\n",
    "- Identify when a model is too simplistic.\n",
    "- Choose better architectures or features.\n",
    "- Debug poor training performance early.\n",
    "\n",
    "### When to Avoid\n",
    "- When models are known to need extreme regularization (rare).\n",
    "- When primary goal is interpretability over accuracy.\n",
    "\n",
    "### Origin Story\n",
    "Bias-variance analysis rose in **1970s statistics** and formalized into ML models during the **1990s** as part of early empirical risk minimization work.\n",
    "\n",
    "```plaintext\n",
    "Observe poor training accuracy\n",
    "‚Üí Hypothesize underfitting\n",
    "‚Üí Increase model complexity\n",
    "‚Üí Reevaluate on training + validation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Bias-dominant error |  \n",
    "| ML | Simple model failure detection |  \n",
    "| DL | Shallow net problems |  \n",
    "| LLM | Inadequate layer/width design |  \n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "Training and validation error decomposition:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- Bias: Error due to model assumptions.\n",
    "- Variance: Error due to model sensitivity.\n",
    "- Irreducible error: Noise inherent in data.\n",
    "\n",
    "**Limit Cases**\n",
    "- Bias $\\uparrow$, Variance $\\downarrow$ ‚Üí Underfitting.\n",
    "- Bias $\\downarrow$, Variance $\\uparrow$ ‚Üí Overfitting.\n",
    "- Both minimized ‚Üí Ideal learning.\n",
    "\n",
    "**Physical Meaning**\n",
    "Bias is like **rigidly using a wrong map** ‚Äî no matter how many times you redraw, you‚Äôll miss the path.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Approximation bias |  \n",
    "| ML | Simplistic model misspecification |  \n",
    "| DL | Insufficient model width/depth |  \n",
    "| LLMs | Tiny embedding size bottlenecks |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| Bias | Systematic error | Badly calibrated compass | Can‚Äôt fix by retraining |  \n",
    "| Variance | Noise sensitivity | Shaky measuring stick | Can reduce by ensemble |  \n",
    "| Model capacity | Learning flexibility | Size of tracing tool | Too small = stuck |  \n",
    "| Training error | Immediate signal | Wall crack detection | High = warning |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small gradients | Learning stalled | Bias likely too high |  \n",
    "| Large gradients | Model too noisy | Variance explosion |  \n",
    "| Mixed | Uneven learning | Local overfit/underfit zones |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Model sufficiently expressive | Otherwise bias stays | Linear model on spiral data |  \n",
    "| Enough training steps | Otherwise false diagnosis | Early stopping errors |  \n",
    "| Good data coverage | Sparse data misleads model | Biased sampling |\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Low capacity | Permanent underfit | Small neural net | Widen/deepen |  \n",
    "| Premature diagnosis | Missed late improvements | Early epoch dropout | Train longer |  \n",
    "| Sparse features | Phantom underfit | Few training examples | Augment data |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Bias | $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$ | Systematic miss | Always pointing wrong |  \n",
    "| Variance | $\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$ | Sensitivity | Wobble |  \n",
    "| Irreducible error | Noise variance | Intrinsic unpredictability | Data noise |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Model capacity check | $O(1)$ | $O(1)$ | Very cheap |  \n",
    "| Training loss tracking | $O(n)$ | $O(n)$ | Linear with data |  \n",
    "| Validation loss tracking | $O(n)$ | $O(n)$ | Necessary monitoring |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def training_validation_error(y_train, y_train_pred, y_val, y_val_pred):\n",
    "    \"\"\"\n",
    "    Compute training and validation errors.\n",
    "\n",
    "    Args:\n",
    "        y_train (np.ndarray): True train outputs\n",
    "        y_train_pred (np.ndarray): Predicted train outputs\n",
    "        y_val (np.ndarray): True validation outputs\n",
    "        y_val_pred (np.ndarray): Predicted validation outputs\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation MSE\n",
    "    \"\"\"\n",
    "    train_mse = np.mean((y_train - y_train_pred)**2)\n",
    "    val_mse = np.mean((y_val - y_val_pred)**2)\n",
    "    return {'train_mse': train_mse, 'val_mse': val_mse}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def training_validation_error(y_train, y_train_pred, y_val, y_val_pred):\n",
    "    \"\"\"\n",
    "    Compute training and validation errors in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        y_train (torch.Tensor): True train outputs\n",
    "        y_train_pred (torch.Tensor): Predicted train outputs\n",
    "        y_val (torch.Tensor): True validation outputs\n",
    "        y_val_pred (torch.Tensor): Predicted validation outputs\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation MSE\n",
    "    \"\"\"\n",
    "    train_mse = torch.mean((y_train - y_train_pred) ** 2)\n",
    "    val_mse = torch.mean((y_val - y_val_pred) ** 2)\n",
    "    return {'train_mse': train_mse.item(), 'val_mse': val_mse.item()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def training_validation_error(y_train, y_train_pred, y_val, y_val_pred):\n",
    "    \"\"\"\n",
    "    Compute training and validation errors using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        y_train (tf.Tensor): True train outputs\n",
    "        y_train_pred (tf.Tensor): Predicted train outputs\n",
    "        y_val (tf.Tensor): Predicted validation outputs\n",
    "        y_val_pred (tf.Tensor): Predicted validation outputs\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation MSE\n",
    "    \"\"\"\n",
    "    train_mse = tf.reduce_mean(tf.square(y_train - y_train_pred))\n",
    "    val_mse = tf.reduce_mean(tf.square(y_val - y_val_pred))\n",
    "    return {'train_mse': train_mse.numpy(), 'val_mse': val_mse.numpy()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example** \n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:----:|:----------|:-----------------|:------------:|  \n",
    "| 1 | Compute training prediction error 1 | $(3 - 2)^2$ | $1$ |  \n",
    "| 2 | Compute training prediction error 2 | $(5 - 4)^2$ | $1$ |  \n",
    "| 3 | Sum training errors | $1 + 1$ | $2$ |  \n",
    "| 4 | Mean training error | $\\frac{2}{2}$ | $1.0$ |  \n",
    "| 5 | Compute validation prediction error 1 | $(6 - 5)^2$ | $1$ |  \n",
    "| 6 | Compute validation prediction error 2 | $(8 - 7)^2$ | $1$ |  \n",
    "| 7 | Sum validation errors | $1 + 1$ | $2$ |  \n",
    "| 8 | Mean validation error | $\\frac{2}{2}$ | $1.0$ |\n",
    "\n",
    "---\n",
    "\n",
    "**Inputs:**\n",
    "- True training outputs: $y_\\text{train} = [3, 5]$\n",
    "- Predicted training outputs: $\\hat{y}_\\text{train} = [2, 4]$\n",
    "- True validation outputs: $y_\\text{val} = [6, 8]$\n",
    "- Predicted validation outputs: $\\hat{y}_\\text{val} = [5, 7]$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Output:**\n",
    "\n",
    "- Training MSE = $1.0$\n",
    "- Validation MSE = $1.0$\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation in words:**\n",
    "\n",
    "- High training MSE shows model cannot fit even known examples ‚Üí clear **underfitting**.  \n",
    "- Validation MSE similar to training MSE ‚Üí no overfitting yet; root cause is **model too simple**.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Residual calculation |  \n",
    "| ML | Diagnosing underfit |  \n",
    "| DL | Shallow network error analysis |  \n",
    "| LLMs | Tiny transformer capacity issues |  \n",
    "| Research/AGI | Insufficient function approximation |\n",
    "\n",
    "---                                                                                                                                                         ## üî• **Theory Deepening** \n",
    "\n",
    "### ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q:** What early warning signs indicate underfitting during model training?\n",
    "\n",
    "**A:**  \n",
    "High training error that does not decrease significantly even after multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** Why does simply adding more data not always fix underfitting?\n",
    "\n",
    "**A:**  \n",
    "Because if the model is too simple, no amount of data helps ‚Äî it fundamentally **can't represent** the underlying pattern.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** How does underfitting affect the bias-variance tradeoff?\n",
    "\n",
    "**A:**  \n",
    "It shows **high bias** (rigid assumptions) and **low variance** (not sensitive to different datasets).\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Poor polynomial fit |  \n",
    "| ML | Bias-dominated error |  \n",
    "| DL | Model too shallow |  \n",
    "| LLMs | Embedding size too small |  \n",
    "| Research/AGI | Limited representation capability |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Underfitting**\n",
    "\n",
    "**Scenario:**  \n",
    "A linear regression model is used on a dataset with a complex, spiral-shaped relationship.  \n",
    "Training MSE remains high even after extensive epochs.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "- Underfitting ‚Äî model too simple for the pattern.\n",
    "\n",
    "2. **Action:**  \n",
    "- Switch to a nonlinear model (e.g., polynomial regression, neural network).\n",
    "\n",
    "3. **Calculation:**  \n",
    "- If switching reduces Training MSE from $1.0$ to $0.1$, it confirms model complexity was the issue.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | Underfitting | Parameter | Behavior |  \n",
    "|:--------|:-------------|:----------|:---------|  \n",
    "| **Model complexity** | Flexibility | Degree/Depth | Higher = Better fit |\n",
    "\n",
    "---\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Diagnosis** ‚Üí Model unable to capture complex relationship.  \n",
    "2. **Action** ‚Üí Increase model expressivity.  \n",
    "3. **Calculation** ‚Üí Training error drop after complexity increase.  \n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "Transformer-based LLM fails to capture long-range dependencies if it has too few attention heads or layers, leading to **underfitting** for tasks like document summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Goodfellow et al., 2016 | Bias-variance tradeoff | Underfitting = high bias problem |  \n",
    "| Shalev-Shwartz and Ben-David, 2014 | Generalization analysis | Underfitting due to low model complexity |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular Data | High constant training error | Loan default prediction poor | Linear model too rigid |  \n",
    "| NLP | Repetitive summaries | Transformer too shallow | Capacity bottleneck |  \n",
    "| CV | Blurry image outputs | CNN too small | Low feature extraction power |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Increase model width | Better training fit | Training loss | Decrease |  \n",
    "| Add nonlinear transformations | More expressive features | Validation loss | Decrease |  \n",
    "| Train longer | Learning plateau detection | Loss curve | Stagnation confirms underfit |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How to auto-detect underfitting without training curves?**  \n",
    "  *Why hard:* Needs active pattern tracking inside layers.\n",
    "\n",
    "- **How to balance model size vs underfitting for tiny datasets?**  \n",
    "  *Why hard:* Larger models may memorize instead.\n",
    "\n",
    "- **Can underfitting occur subtly inside deep models (inner bottlenecks)?**  \n",
    "  *Why hard:* Hard to isolate specific low-capacity modules.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Underfitted models can completely miss minority patterns. *Mitigation: Ensure model expressivity evaluation.*  \n",
    "‚Ä¢ **Risk**: Underfitting hides system flaws by pretending outputs are random. *Mitigation: Layer-wise monitoring.*  \n",
    "‚Ä¢ **Risk**: Over-relying on training loss masks real-world error. *Mitigation: Always validate against diverse datasets.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Should we prefer slightly overfitted models to underfitted ones in safety-critical applications?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "**Deployment Gotchas**  \n",
    "- Always monitor **training loss vs validation loss gap** ‚Äî a **small gap + high errors** = underfitting warning.\n",
    "\n",
    "**Scaling Limits**  \n",
    "- Very deep models reduce underfitting but risk vanishing gradients ‚Äî balance depth carefully.\n",
    "\n",
    "**Production Fixes**  \n",
    "- Regularly **increase feature engineering richness** if underfitting persists after model upgrades.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Failure load prediction | Model underfitting risk |  \n",
    "| Finance | Predicting fraud patterns | Missing non-linear features |  \n",
    "| Robotics | Trajectory learning | Linear controllers fail on complex maneuvers |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1970s: Bias-variance conceptualization\n",
    "‚Üí 1990s: ML models formalizing underfitting risks\n",
    "‚Üí 2010s: Deep learning vs shallow nets debate\n",
    "‚Üí 2020s: AutoML systems detecting underfitting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Dynamic model expansion during training** (growing width/depth based on loss patterns).\n",
    "- **Adaptive data augmentation** to counter early underfitting signs.\n",
    "- **Self-diagnostic architectures** embedding underfit detection inside layers.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üß™ Setup\n",
    "# Generate synthetic data\n",
    "def generate_data(samples=50, noise=0.1):\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(-3, 3, samples).reshape(-1, 1)\n",
    "    y = np.sin(X) + noise * np.random.randn(samples, 1)\n",
    "    return X, y\n",
    "\n",
    "# üîÅ Core logic: Train a polynomial regression model\n",
    "def apply_concept(X_train, y_train, X_test, y_test, degree):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "    train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    return y_train_pred, y_test_pred, train_loss, test_loss\n",
    "\n",
    "# üìä Visualization\n",
    "def plot_results(X_train, y_train, X_test, y_test, y_train_pred, y_test_pred, train_loss, test_loss, degree):\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    # Scatter plot of true data\n",
    "    plt.scatter(X_train, y_train, label=\"Train Data\", color='blue', s=30)\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\", color='orange', s=30, alpha=0.7)\n",
    "\n",
    "    # Plot predictions\n",
    "    sorted_idx = np.argsort(X_test.flatten())\n",
    "    plt.plot(X_test[sorted_idx], y_test_pred[sorted_idx], color='red', label=f\"Model Prediction (Degree={degree})\")\n",
    "\n",
    "    plt.title(f\" Underfitting vs Overfitting (Train Loss: {train_loss:.3f}, Test Loss: {test_loss:.3f})\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive simulator\n",
    "def interactive_sim(degree, samples, noise):\n",
    "    # Split data into train/test\n",
    "    X, y = generate_data(samples=samples, noise=noise)\n",
    "    split_idx = int(0.7 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    # Apply concept\n",
    "    y_train_pred, y_test_pred, train_loss, test_loss = apply_concept(X_train, y_train, X_test, y_test, degree)\n",
    "    \n",
    "    # Plot\n",
    "    plot_results(X_train, y_train, X_test, y_test, y_train_pred, y_test_pred, train_loss, test_loss, degree)\n",
    "\n",
    "# üß∞ Widgets\n",
    "degree_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Polynomial Degree:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "samples_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=20,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Samples:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "noise_slider = widgets.FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Noise Level:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI\n",
    "ui = widgets.VBox([degree_slider, samples_slider, noise_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'degree': degree_slider,\n",
    "        'samples': samples_slider,\n",
    "        'noise': noise_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa956103",
   "metadata": {},
   "source": [
    "# <a id=\"cost-surface\"></a>üåÑ Visualizing Cost Surface  \n",
    "\n",
    "> *Visualizing the cost surface means plotting how the loss value changes across different model parameter values.*  \n",
    "> *Mechanical Analogy*: Like surveying mountains and valleys where valleys represent good models (low loss) and mountains represent bad models (high loss).\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Purpose & Relevance**\n",
    "\n",
    "### 1. **Why It Matters**\n",
    "- **ML**: Diagnoses how optimizers move during training.\n",
    "- **DL**: Reveals the landscape shape for network weights.\n",
    "- **LLMs**: Helps understand fine-tuning on complex objectives.\n",
    "- **AGI**: Guides design of scalable, stable learning architectures.\n",
    "\n",
    "### 2. **Mechanical Analogy**\n",
    "Imagine **mapping a landscape** by measuring ground height at every step.  \n",
    "- Valleys = good models with low loss.\n",
    "- Peaks = bad models with high loss.\n",
    "\n",
    "### 3. **2020+ Research Citations**\n",
    "- Goodfellow et al., 2014 ‚Äî *Qualitatively Characterizing Neural Network Optimization Problems*.  \n",
    "- Li et al., 2018 ‚Äî *Visualizing the Loss Landscape of Neural Nets*.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Multivariable function surfaces |  \n",
    "| ML | Loss curve inspection |  \n",
    "| DL | Neural network loss visualization |  \n",
    "| LLMs | Training dynamics observation |  \n",
    "| Research/AGI | Generalization landscape analysis |\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Key Terminology**\n",
    "\n",
    "‚Ä¢ **Cost Surface**: Loss plotted against parameters. *Analogous to landscape height map.*  \n",
    "‚Ä¢ **Contour Lines**: Curves of equal loss. *Analogous to topographic map lines.*  \n",
    "‚Ä¢ **Gradient**: Direction of steepest increase. *Analogous to climbing slope.*  \n",
    "‚Ä¢ **Saddle Point**: Flat along some axes, steep along others. *Analogous to a mountain pass.*  \n",
    "‚Ä¢ **Local Minimum**: Small valley not lowest globally. *Analogous to a false dip.*\n",
    "\n",
    "---\n",
    "\n",
    "## üå± **Conceptual Foundation**\n",
    "\n",
    "### Purpose\n",
    "- Reveal optimization difficulties (plateaus, cliffs).\n",
    "- Improve model initialization and optimizer selection.\n",
    "- Explain weird training behaviors (like getting stuck).\n",
    "\n",
    "### When to Avoid\n",
    "- High-dimensional models (can't plot all dimensions easily).\n",
    "- Real-time deployments (visualization adds overhead).\n",
    "\n",
    "### Origin Story\n",
    "Surface visualization originated in **1950s physics optimization problems**, later applied to **ML loss analysis** after 2010.\n",
    "\n",
    "```plaintext\n",
    "Pick two parameters\n",
    "    ‚Üì\n",
    "Sweep them over ranges\n",
    "    ‚Üì\n",
    "Evaluate loss at each combination\n",
    "    ‚Üì\n",
    "Build surface based on loss values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Deep Dive**\n",
    "\n",
    "### üîç **Core Concept Summary**\n",
    "\n",
    "| Field | Role |  \n",
    "|:------|:-----|  \n",
    "| Math | Study of multivariable surfaces |  \n",
    "| ML | Understand loss landscape |  \n",
    "| DL | Visualize optimization path |  \n",
    "| LLM | Analyze training plateaus |  \n",
    "\n",
    "### üìú **Canonical Formula**\n",
    "\n",
    "Example cost function:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = (\\theta_0 - 2)^2 + (\\theta_1 + 3)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_0$, $\\theta_1$ are model parameters.\n",
    "- $J(\\theta)$ measures loss at each coordinate.\n",
    "\n",
    "**Limit Cases**\n",
    "- Smooth convex bowl ‚Üí Easy descent.\n",
    "- Rugged jagged landscape ‚Üí Hard optimization.\n",
    "- Flat plateaus ‚Üí No descent direction.\n",
    "\n",
    "**Physical Meaning**\n",
    "Each point on the surface represents \"energy\" at that setting of model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Multivariate function graphs |  \n",
    "| ML | Loss visualization |  \n",
    "| DL | Saddle point detection |  \n",
    "| LLMs | Long flat training behavior |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Atomic Component Dissection**\n",
    "\n",
    "| Component | Math Role | Physical Analogy | Limit Behavior |  \n",
    "|:----------|:----------|:-----------------|:---------------|  \n",
    "| $\\theta_0$ | X-axis parameter | East-West movement | Fixed if frozen |  \n",
    "| $\\theta_1$ | Y-axis parameter | North-South movement | Fixed if frozen |  \n",
    "| $J(\\theta)$ | Z-axis (loss) | Altitude | Constant = flat |  \n",
    "| Contours | Constant loss | Map elevation lines | Tight in steep regions |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Gradient Behavior by Zones**\n",
    "\n",
    "| Condition | Gradient Value | Training Impact |  \n",
    "|:----------|:---------------|:----------------|  \n",
    "| Small gradient | Flat areas | Slow learning |  \n",
    "| Large gradient | Steep areas | Risk overshooting |  \n",
    "| Mixed gradients | Saddle zones | Complex navigation |\n",
    "\n",
    "---\n",
    "\n",
    "### üìú **Explicit Assumptions**\n",
    "\n",
    "| Assumption | Why Critical | Violation Example |  \n",
    "|:-----------|:-------------|:------------------|  \n",
    "| Low dimension (2D/3D) | Required for visualization | Billions of parameters in LLMs |  \n",
    "| Continuous surface | Required for gradient flow | Discrete jumps cause trouble |  \n",
    "| Reasonable grid | Memory manageable | Too fine grid = crash |\n",
    "\n",
    "### üõë **Assumption Violations Table**\n",
    "\n",
    "| Assumption | Breakage Effect | ML/DL/LLM Example | Fix |  \n",
    "|:-----------|:----------------|:-----------------|:---|  \n",
    "| Too high dimension | Cannot visualize | Large Transformer | Slice random subspaces |  \n",
    "| Discrete loss surface | Jagged optimization | Quantized models | Smooth approximations |  \n",
    "| Excessive grid | RAM overload | Fine sampling | Reduce resolution |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Unified Error Estimation**\n",
    "\n",
    "| Error Type | Formula | Purpose | Interpretation |  \n",
    "|:-----------|:--------|:--------|:---------------|  \n",
    "| Loss at point | $J(\\theta)$ | Measure model quality | Altitude |  \n",
    "| Local gradient | $\\nabla J(\\theta)$ | Find descent direction | Slope strength |  \n",
    "| Curvature | $\\nabla^2 J(\\theta)$ | Detect sharp valleys | Bend sharpness |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è≥ **Computational Complexity**\n",
    "\n",
    "| Operation | Time | Space | Scaling Impact |  \n",
    "|:----------|:-----|:------|:---------------|  \n",
    "| Grid creation | $O(n^2)$ | $O(n^2)$ | Fine grid = quadratic cost |  \n",
    "| Loss computation | $O(n^2)$ | $O(n^2)$ | Matching grid size |  \n",
    "| Full visualization | $O(n^2)$ | $O(n^2)$ | Bounded by grid limits |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Framework Implementations**\n",
    "\n",
    "### NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_cost_surface(theta0_vals, theta1_vals):\n",
    "    \"\"\"\n",
    "    Compute cost surface values using NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        theta0_vals (np.ndarray): 1D array of theta0 values\n",
    "        theta1_vals (np.ndarray): 1D array of theta1 values\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D array of cost values\n",
    "    \"\"\"\n",
    "    assert theta0_vals.ndim == 1, \"theta0_vals must be 1D\"\n",
    "    assert theta1_vals.ndim == 1, \"theta1_vals must be 1D\"\n",
    "    theta0, theta1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "    J = (theta0 - 2) ** 2 + (theta1 + 3) ** 2\n",
    "    return J\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def compute_cost_surface(theta0_vals, theta1_vals):\n",
    "    \"\"\"\n",
    "    Compute cost surface values using PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        theta0_vals (torch.Tensor): 1D tensor of theta0 values\n",
    "        theta1_vals (torch.Tensor): 1D tensor of theta1 values\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 2D tensor of cost values\n",
    "    \"\"\"\n",
    "    assert theta0_vals.dim() == 1, \"theta0_vals must be 1D\"\n",
    "    assert theta1_vals.dim() == 1, \"theta1_vals must be 1D\"\n",
    "    theta0, theta1 = torch.meshgrid(theta0_vals, theta1_vals, indexing='ij')\n",
    "    J = (theta0 - 2) ** 2 + (theta1 + 3) ** 2\n",
    "    return J\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def compute_cost_surface(theta0_vals, theta1_vals):\n",
    "    \"\"\"\n",
    "    Compute cost surface values using TensorFlow tensors.\n",
    "\n",
    "    Args:\n",
    "        theta0_vals (tf.Tensor): 1D tensor of theta0 values\n",
    "        theta1_vals (tf.Tensor): 1D tensor of theta1 values\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: 2D tensor of cost values\n",
    "    \"\"\"\n",
    "    tf.debugging.assert_rank(theta0_vals, 1)\n",
    "    tf.debugging.assert_rank(theta1_vals, 1)\n",
    "    theta0, theta1 = tf.meshgrid(theta0_vals, theta1_vals)\n",
    "    J = tf.square(theta0 - 2) + tf.square(theta1 + 3)\n",
    "    return J\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Debug & Fix Examples**\n",
    "\n",
    "| Symptom | Root Cause | Fix |  \n",
    "|:--------|:-----------|:----|  \n",
    "| Crash due to huge memory usage | Grid resolution too high | Use coarser grid (fewer points) |  \n",
    "| Flat cost surface everywhere | Incorrect cost function implemented | Verify formula correctness |  \n",
    "| Meshgrid shape mismatch | Non-1D theta inputs | Insert dimension assertions |\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ **Step-by-Step Numerical Example**\n",
    "\n",
    "---\n",
    "\n",
    "We will **atomically break down** how to compute a **small piece** of the cost surface manually.\n",
    "\n",
    "---\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "- True cost function:  \n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = (\\theta_0 - 2)^2 + (\\theta_1 + 3)^2\n",
    "$$\n",
    "\n",
    "- Choose $\\theta_0$ values:  \n",
    "$\\theta_0 \\in \\{1, 2\\}$\n",
    "\n",
    "- Choose $\\theta_1$ values:  \n",
    "$\\theta_1 \\in \\{-4, -3\\}$\n",
    "\n",
    "---\n",
    "\n",
    "| Step | Operation | Mini-Calculation | Micro-Result |  \n",
    "|:----:|:----------|:-----------------|:------------:|  \n",
    "| 1 | Evaluate $J(1, -4)$ | $(1-2)^2 + (-4+3)^2$ | $(‚àí1)^2 + (‚àí1)^2 = 1 + 1$ ‚Üí $2$ |  \n",
    "| 2 | Evaluate $J(1, -3)$ | $(1-2)^2 + (-3+3)^2$ | $(‚àí1)^2 + (0)^2 = 1 + 0$ ‚Üí $1$ |  \n",
    "| 3 | Evaluate $J(2, -4)$ | $(2-2)^2 + (-4+3)^2$ | $(0)^2 + (‚àí1)^2 = 0 + 1$ ‚Üí $1$ |  \n",
    "| 4 | Evaluate $J(2, -3)$ | $(2-2)^2 + (-3+3)^2$ | $(0)^2 + (0)^2 = 0 + 0$ ‚Üí $0$ |  \n",
    "\n",
    "---\n",
    "\n",
    "**Final Cost Surface Grid:**\n",
    "\n",
    "| $\\theta_0$ ‚Üí / $\\theta_1$ ‚Üì | 1 | 2 |  \n",
    "|:---------------------------:|:--:|:--:|  \n",
    "| -4 | 2 | 1 |  \n",
    "| -3 | 1 | 0 |  \n",
    "\n",
    "Or numerically as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation in words:**\n",
    "- As we move from $(1, -4)$ toward $(2, -3)$, the loss **decreases** steadily.\n",
    "- The lowest cost ($0$) is at $(\\theta_0=2, \\theta_1=-3)$ ‚Äî the **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Surface value sampling |  \n",
    "| ML | Grid-based loss evaluation |  \n",
    "| DL | Weight visualization |  \n",
    "| LLMs | Subspace loss analysis |  \n",
    "| Research/AGI | Local landscape mapping |\n",
    "\n",
    "---\n",
    "\n",
    "## üî• **Theory Deepening**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Socratic Breakdown**\n",
    "\n",
    "**Q:** Why can't we visualize the full cost surface of a real neural network?\n",
    "\n",
    "**A:**  \n",
    "Because real models (e.g., deep networks, LLMs) have **millions to billions** of parameters, and it's impossible to plot a space with so many dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** Why do saddle points make optimization harder?\n",
    "\n",
    "**A:**  \n",
    "At a saddle point, the gradient is close to zero but the point is not a minimum ‚Äî optimization can **stall or oscillate** there without making real progress.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** What does a \"flat\" cost surface region mean during training?\n",
    "\n",
    "**A:**  \n",
    "It means the loss doesn‚Äôt change much even with parameter updates, leading to **very slow learning** unless a momentum or adaptive optimizer is used.\n",
    "\n",
    "---\n",
    "\n",
    "| Realm | Example Concept |  \n",
    "|:------|:----------------|  \n",
    "| Pure Math | Critical points (minima, maxima, saddles) |  \n",
    "| ML | Plateaus in loss curves |  \n",
    "| DL | Weight decay towards saddle points |  \n",
    "| LLMs | Pretraining stability issues |  \n",
    "| Research/AGI | Energy landscape navigation |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **Test Your Knowledge: Visualizing Cost Surface**\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario:**  \n",
    "You visualize the cost surface of a small model and observe large flat plateaus surrounding sharp valleys.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Diagnosis:**  \n",
    "- Optimizer likely **struggles with slow progress** across flat regions, then sudden jumps at steep walls.\n",
    "\n",
    "2. **Action:**  \n",
    "- Switch from basic SGD to an adaptive optimizer like **Adam** to handle varied curvature better.\n",
    "\n",
    "3. **Calculation:**  \n",
    "- Adam adapts learning rates per parameter ‚Üí faster escape from plateaus, controlled descent into valleys.\n",
    "\n",
    "---\n",
    "\n",
    "| Concept | Visualizing Cost Surface | Parameter | Behavior |  \n",
    "|:--------|:-------------------------|:----------|:---------|  \n",
    "| **Optimizer type** | Gradient step adaptation | Momentum/Adam | Faster convergence |\n",
    "\n",
    "---\n",
    "\n",
    "<details>  \n",
    "<summary>üìù **Answer Key**</summary>  \n",
    "\n",
    "1. **Diagnosis** ‚Üí Slow learning in flats, jumps near cliffs.  \n",
    "2. **Action** ‚Üí Switch to curvature-aware optimizers.  \n",
    "3. **Calculation** ‚Üí Dynamic learning rates per parameter help stabilize training.  \n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Concept Example**\n",
    "\n",
    "In Transformer models, loss landscapes during fine-tuning can reveal **flat regions** indicating that **only a few weights** need significant updates.  \n",
    "Visualizing helps in designing **efficient transfer learning strategies**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú **Foundational Evidence Map**\n",
    "\n",
    "| Paper | Key Idea | Connection to Topic |  \n",
    "|:------|:---------|:--------------------|  \n",
    "| Goodfellow et al., 2014 | Smoothness and ruggedness of loss surfaces | Direct study of training dynamics |  \n",
    "| Li et al., 2018 | Mode connectivity and landscape geometry | Revealed surprising flatness between solutions |\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **Failure Scenario Table**\n",
    "\n",
    "| Scenario | General Output | Domain Output | Problem |  \n",
    "|:---------|:---------------|:--------------|:--------|  \n",
    "| Tabular Data | Stalled training | Poor loss convergence | Saddle points |  \n",
    "| NLP | Training instability | Random loss jumps | Curved valleys |  \n",
    "| CV | Slow feature extractor tuning | Flat regions | Insufficient gradient strength |\n",
    "\n",
    "---\n",
    "\n",
    "## üî≠ **What-If Experiments Plan**\n",
    "\n",
    "| Scenario | Hypothesis | Metric | Expected Outcome |  \n",
    "|:---------|:-----------|:-------|:-----------------|  \n",
    "| Switch SGD ‚Üí Adam | Faster escape from flats | Epochs to convergence | Decrease |  \n",
    "| Visualize smaller loss subspaces | Find smoother paths | Loss variability | Decrease |  \n",
    "| Train with noise injection | Shake stuck gradients loose | Training loss | Drops faster |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Open Research Questions**\n",
    "\n",
    "- **How to automatically detect saddle points during training?**  \n",
    "  *Why hard:* Gradient magnitude alone is ambiguous.\n",
    "\n",
    "- **Can models dynamically modify their optimization path if the surface is rugged?**  \n",
    "  *Why hard:* Requires real-time loss landscape sensing.\n",
    "\n",
    "- **Are flat minima truly better for generalization?**  \n",
    "  *Why hard:* Depends on domain and model type.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Ethical Lens & Bias Risks**\n",
    "\n",
    "‚Ä¢ **Risk**: Surface visualization can mislead if only local neighborhood analyzed. *Mitigation: Wide-area mapping needed.*  \n",
    "‚Ä¢ **Risk**: Smooth surfaces can hide dataset biases (low loss doesn‚Äôt mean fairness). *Mitigation: Fairness-specific probes.*  \n",
    "‚Ä¢ **Risk**: Visualization can falsely justify overcomplex models. *Mitigation: Occam‚Äôs razor applied after analysis.*\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Debate Prompt / Reflective Exercise**\n",
    "\n",
    "> *\"Is it better to aim for flat minima rather than steep sharp minima during training even if sharp minima give lower training loss?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† **Practical Engineering Tips**\n",
    "\n",
    "**Deployment Gotchas**  \n",
    "- Surface visualization results vary a lot depending on random seeds ‚Äî average across multiple runs.\n",
    "\n",
    "**Scaling Limits**  \n",
    "- Full cost surface visualization only possible for **‚â§3 parameters**.  \n",
    "- For bigger models, use **random low-dimensional projections**.\n",
    "\n",
    "**Production Fixes**  \n",
    "- Use **loss visualization** only for **model diagnosis** ‚Äî not for final model selection blindly.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Cross-Field Applications**\n",
    "\n",
    "| Field | Example | Mathematical Role |  \n",
    "|:------|:--------|:------------------|  \n",
    "| Engineering | Stress/strain field visualization | Surface mapping |  \n",
    "| Finance | Risk landscape analysis | Portfolio optimization surfaces |  \n",
    "| Robotics | Trajectory energy landscape plotting | Action cost fields |\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞Ô∏è **Historical Evolution**\n",
    "\n",
    "```plaintext\n",
    "1950s: Cost surface ideas from physics\n",
    "‚Üí 1990s: Small ML model loss visualizations\n",
    "‚Üí 2010s: Deep learning rugged landscapes studied\n",
    "‚Üí 2020s: Advanced high-dimensional projections in LLM training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Future Directions**\n",
    "\n",
    "- **Automatic surface smoothness detectors** during training.\n",
    "- **High-dimensional curvature mapping** for huge models.\n",
    "- **Surface-aware optimizers** adjusting dynamically to curvature.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dd4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a7a37ab5ea4430ad58d2f86f0bd2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=0.0, description='Œ∏‚ÇÄ init:', max=10.0, min=-10.0), FloatSlider(value=0.0, des‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7417b209964630935b82e71e61e378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# üì¶ Dynamic Gradient Descent on Cost Surface (with ipywidgets)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# üß™ Setup: Generate a simple toy dataset\n",
    "def generate_data():\n",
    "    X = np.array([[1, 1],\n",
    "                  [1, 2],\n",
    "                  [1, 3]])\n",
    "    y = np.array([1, 2, 3])\n",
    "    return X, y\n",
    "\n",
    "# üîÅ Core logic: Gradient Descent Step\n",
    "def compute_cost(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    predictions = X @ theta\n",
    "    loss = (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    predictions = X @ theta\n",
    "    grad = (1/m) * (X.T @ (predictions - y))\n",
    "    return grad\n",
    "\n",
    "def apply_gradient_descent(X, y, theta_init, learning_rate, epochs):\n",
    "    theta = np.array(theta_init, dtype=float)\n",
    "    path = [theta.copy()]\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        grad = compute_gradient(X, y, theta)\n",
    "        theta -= learning_rate * grad\n",
    "        path.append(theta.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# üìä Visualization: Plot surface + moving point\n",
    "def plot_results(X, y, path, theta0_range, theta1_range, resolution=50):\n",
    "    theta0_vals = np.linspace(theta0_range[0], theta0_range[1], resolution)\n",
    "    theta1_vals = np.linspace(theta1_range[0], theta1_range[1], resolution)\n",
    "    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "    \n",
    "    J_vals = np.zeros_like(T0)\n",
    "    \n",
    "    for i in range(T0.shape[0]):\n",
    "        for j in range(T0.shape[1]):\n",
    "            t = np.array([T0[i, j], T1[i, j]])\n",
    "            J_vals[i, j] = compute_cost(X, y, t)\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # 3D surface plot\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.8)\n",
    "    ax.plot(path[:, 0], path[:, 1], [compute_cost(X, y, p) for p in path],\n",
    "            marker='o', color='red')\n",
    "    ax.set_xlabel('Œ∏‚ÇÄ')\n",
    "    ax.set_ylabel('Œ∏‚ÇÅ')\n",
    "    ax.set_zlabel('Cost')\n",
    "    ax.set_title(' Cost Surface + Gradient Descent Path')\n",
    "    \n",
    "    # 2D contour plot\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    contour = ax2.contour(T0, T1, J_vals, levels=30, cmap='viridis')\n",
    "    ax2.plot(path[:, 0], path[:, 1], marker='o', color='red')\n",
    "    ax2.set_xlabel('Œ∏‚ÇÄ')\n",
    "    ax2.set_ylabel('Œ∏‚ÇÅ')\n",
    "    ax2.set_title(' Contour View of Gradient Descent')\n",
    "    plt.colorbar(contour)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive flow\n",
    "def interactive_sim(theta0_init, theta1_init, learning_rate, epochs):\n",
    "    X, y = generate_data()\n",
    "    theta_init = [theta0_init, theta1_init]\n",
    "    path = apply_gradient_descent(X, y, theta_init, learning_rate, epochs)\n",
    "    plot_results(X, y, path, theta0_range=(-10, 10), theta1_range=(-1, 5))\n",
    "\n",
    "# üß∞ Sliders\n",
    "theta0_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-10.0, max=10.0, step=0.1, description='Œ∏‚ÇÄ init:'\n",
    ")\n",
    "theta1_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-1.0, max=5.0, step=0.1, description='Œ∏‚ÇÅ init:'\n",
    ")\n",
    "lr_slider = widgets.FloatSlider(\n",
    "    value=0.1, min=0.001, max=1.0, step=0.001, description='Learning Rate:'\n",
    ")\n",
    "epoch_slider = widgets.IntSlider(\n",
    "    value=50, min=1, max=300, step=1, description='Epochs:'\n",
    ")\n",
    "\n",
    "# üîÅ Bind UI\n",
    "ui = widgets.VBox([theta0_slider, theta1_slider, lr_slider, epoch_slider])\n",
    "out = widgets.interactive_output(\n",
    "    interactive_sim,\n",
    "    {\n",
    "        'theta0_init': theta0_slider,\n",
    "        'theta1_init': theta1_slider,\n",
    "        'learning_rate': lr_slider,\n",
    "        'epochs': epoch_slider\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
