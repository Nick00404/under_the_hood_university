{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793f4425",
   "metadata": {},
   "source": [
    "- üß† [Understanding Logistic Regression](#logistic-understanding)\n",
    "  - üî¢ [Binary Classification Motivation](#binary-motivation)\n",
    "  - üìà [Sigmoid Function & Probability Output](#sigmoid-probability)\n",
    "  - üöß [Decision Boundary Interpretation](#decision-boundary)\n",
    "- üõ†Ô∏è [Training the Model](#logistic-training)\n",
    "  - üí∏ [Cost Function for Logistic Regression](#logistic-cost)\n",
    "  - üîÅ [Gradient Descent for Logistic Regression](#logistic-gd)\n",
    "  - ‚öñÔ∏è [Feature Scaling](#feature-scaling)\n",
    "- üìä [Evaluation & Performance](#logistic-evaluation)\n",
    "  - üìè [Accuracy, Precision, Recall, F1](#metrics)\n",
    "  - üßÆ [Confusion Matrix](#confusion-matrix)\n",
    "  - üìâ [ROC Curve & AUC](#roc-auc)\n",
    "  - üî• [Overfitting in Classification Models](#overfitting-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32491718",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† <a id=\"logistic-understanding\"></a>**1. Understanding Logistic Regression**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59124e4",
   "metadata": {},
   "source": [
    "# <a id=\"binary-motivation\"></a>üî¢ Binary Classification Motivation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6ea339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c0e76a8245478699bd9bc61e685157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='threshold', max=1.0, step=0.01), FloatSlider(value=0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_binary_classification(threshold=0.5, noise=0.2)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Binary Classification Motivation interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate a simple binary classification toy dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def plot_binary_classification(threshold=0.5, noise=0.2):\n",
    "    \"\"\"Plot binary classification toy dataset with live decision threshold.\"\"\"\n",
    "    X, y = generate_toy_dataset(noise=noise)\n",
    "    # True weights (simulated Œ∏ vector)\n",
    "    theta = np.array([1.0, 1.0])\n",
    "    bias = 0.0\n",
    "\n",
    "    logits = X @ theta + bias\n",
    "    probs = sigmoid(logits)\n",
    "    \n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=probs, cmap=\"coolwarm\", edgecolor='k', alpha=0.8)\n",
    "    plt.colorbar(scatter, label=\"Predicted Probability (hŒ∏(x))\")\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        plt.text(X[i,0], X[i,1]+0.1, f\"{probs[i]:.2f}\", fontsize=6, ha='center')\n",
    "\n",
    "    # Decision Boundary: Œ∏·µÄx + b = 0 (for thresholded version, manually adjusting)\n",
    "    x1 = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)\n",
    "    if theta[1] != 0:\n",
    "        adjusted_bias = np.log(threshold/(1-threshold))  # because sigmoid(Œ∏·µÄx+b)=threshold\n",
    "        x2 = -(theta[0]*x1 + adjusted_bias)/theta[1]\n",
    "        plt.plot(x1, x2, 'k--', label=f\"Decision Boundary at threshold={threshold:.2f}\")\n",
    "    else:\n",
    "        plt.axvline(x=-bias/theta[0], linestyle='--', color='k', label=\"Decision Boundary\")\n",
    "\n",
    "    plt.title(f\"Binary Classification Motivation\\nNoise={noise}, Threshold={threshold}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widget\n",
    "widgets.interact(\n",
    "    plot_binary_classification,\n",
    "    threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179d4af",
   "metadata": {},
   "source": [
    "# <a id=\"sigmoid-probability\"></a>üìà Sigmoid Function & Probability Output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feaa4e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5957dbb261204a2ead85f41ae7627c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='temperature', max=5.0, min=0.1), FloatSlider(value=0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_sigmoid_function(temperature=1.0, shift=0.0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Sigmoid Function & Probability Output interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def sigmoid(z, temperature=1.0):\n",
    "    \"\"\"Sigmoid function with temperature scaling.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z/temperature))\n",
    "\n",
    "def plot_sigmoid_function(temperature=1.0, shift=0.0):\n",
    "    \"\"\"Plot sigmoid function with adjustable temperature and shift.\"\"\"\n",
    "    z = np.linspace(-10, 10, 1000)\n",
    "    sigmoid_values = sigmoid(z - shift, temperature)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(z, sigmoid_values, color=\"purple\", lw=2)\n",
    "    plt.title(f\"Sigmoid Function\\nTemperature={temperature:.2f}, Shift={shift:.2f}\")\n",
    "    plt.xlabel(\"Input Logit (z)\")\n",
    "    plt.ylabel(\"Sigmoid Output œÉ(z)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Key points\n",
    "    plt.axhline(0.5, linestyle='--', color='gray')\n",
    "    plt.axvline(shift, linestyle='--', color='gray', label=f\"Center Shift: {shift:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_sigmoid_function,\n",
    "    temperature=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=1.0),\n",
    "    shift=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=0.0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823deca",
   "metadata": {},
   "source": [
    "# <a id=\"decision-boundary\"></a>üöß Decision Boundary Interpretation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba280a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cfcf1aea7f40a58eb6813f583e85f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='theta0', max=5.0, min=-5.0), FloatSlider(value=1.0, ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_decision_boundary(theta0=1.0, theta1=1.0, bias=0.0, noise=0.2)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Decision Boundary Interpretation interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate simple 2D binary classification toy data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def plot_decision_boundary(theta0=1.0, theta1=1.0, bias=0.0, noise=0.2):\n",
    "    \"\"\"Plot the binary classification dataset and live decision boundary.\"\"\"\n",
    "    X, y = generate_toy_dataset(noise=noise)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bwr\", edgecolor='k', alpha=0.7)\n",
    "\n",
    "    # Plot decision boundary: Œ∏‚ÇÄx‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ + bias = 0\n",
    "    x1_range = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100)\n",
    "    \n",
    "    if theta1 != 0:\n",
    "        x2_boundary = -(theta0 * x1_range + bias) / theta1\n",
    "        plt.plot(x1_range, x2_boundary, 'k--', label=f\"Boundary: Œ∏‚ÇÄ={theta0:.2f}, Œ∏‚ÇÅ={theta1:.2f}, bias={bias:.2f}\")\n",
    "    else:\n",
    "        plt.axvline(x=-bias/theta0, linestyle='--', color='k', label=f\"Vertical Boundary (Œ∏‚ÇÅ=0)\")\n",
    "\n",
    "    plt.title(\"Decision Boundary Interpretation (Live)\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_decision_boundary,\n",
    "    theta0=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=1.0),\n",
    "    theta1=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=1.0),\n",
    "    bias=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=0.0),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a95cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üõ†Ô∏è <a id=\"logistic-training\"></a>**2. Training the Model**\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33b201",
   "metadata": {},
   "source": [
    "# <a id=\"logistic-cost\"></a>üí∏ Cost Function for Logistic Regression  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5d3125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fd4400f52f463ebf0067b565bc1171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='theta0_val', max=5.0, min=-5.0), FloatSlider(value=1‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_loss_surface(theta0_val=1.0, theta1_val=1.0, bias=0.0, noise=0.2)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Cost Function for Logistic Regression interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate simple 2D binary classification toy data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(X, y, theta0, theta1, bias):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    logits = X[:,0] * theta0 + X[:,1] * theta1 + bias\n",
    "    probs = sigmoid(logits)\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(y * np.log(probs + epsilon) + (1 - y) * np.log(1 - probs + epsilon))\n",
    "    return loss\n",
    "\n",
    "def plot_loss_surface(theta0_val=1.0, theta1_val=1.0, bias=0.0, noise=0.2):\n",
    "    \"\"\"Plot the loss surface for logistic regression.\"\"\"\n",
    "    X, y = generate_toy_dataset(noise=noise)\n",
    "    \n",
    "    theta0_range = np.linspace(-5, 5, 50)\n",
    "    theta1_range = np.linspace(-5, 5, 50)\n",
    "    Theta0, Theta1 = np.meshgrid(theta0_range, theta1_range)\n",
    "    \n",
    "    Loss = np.zeros_like(Theta0)\n",
    "    for i in range(Theta0.shape[0]):\n",
    "        for j in range(Theta0.shape[1]):\n",
    "            Loss[i,j] = compute_loss(X, y, Theta0[i,j], Theta1[i,j], bias)\n",
    "\n",
    "    fig = plt.figure(figsize=(14,6))\n",
    "\n",
    "    # Subplot 1: 3D Surface\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d')\n",
    "    surf = ax.plot_surface(Theta0, Theta1, Loss, cmap=cm.viridis, alpha=0.8)\n",
    "    ax.scatter(theta0_val, theta1_val, compute_loss(X, y, theta0_val, theta1_val, bias), color='red', s=50)\n",
    "    ax.set_xlabel('Œ∏‚ÇÄ')\n",
    "    ax.set_ylabel('Œ∏‚ÇÅ')\n",
    "    ax.set_zlabel('Loss')\n",
    "    ax.set_title('Loss Surface (3D View)')\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "    # Subplot 2: Contour Plot\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    contour = ax2.contourf(Theta0, Theta1, Loss, 50, cmap=cm.viridis)\n",
    "    ax2.scatter(theta0_val, theta1_val, color='red', s=50, label=\"Current Œ∏\")\n",
    "    ax2.set_xlabel('Œ∏‚ÇÄ')\n",
    "    ax2.set_ylabel('Œ∏‚ÇÅ')\n",
    "    ax2.set_title('Loss Surface (Contour View)')\n",
    "    ax2.legend()\n",
    "    fig.colorbar(contour, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_loss_surface,\n",
    "    theta0_val=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=1.0),\n",
    "    theta1_val=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=1.0),\n",
    "    bias=widgets.FloatSlider(min=-5.0, max=5.0, step=0.1, value=0.0),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6d1a8",
   "metadata": {},
   "source": [
    "# <a id=\"logistic-gd\"></a>üîÅ Gradient Descent for Logistic Regression  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0ee89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04572cb506a54627be74f60974d9570a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='learning_rate', max=1.0, min=0.001, step=0.01), IntS‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gradient_descent_with_visualization(learning_rate=0.1, steps=50, noise=0.2)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Gradient Descent for Logistic Regression (Fixed Version)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate a simple 2D binary classification toy dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(X_aug, y, theta):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    logits = X_aug @ theta\n",
    "    probs = sigmoid(logits)\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(y * np.log(probs + epsilon) + (1 - y) * np.log(1 - probs + epsilon))\n",
    "    return loss\n",
    "\n",
    "def gradient_descent_with_visualization(learning_rate=0.1, steps=50, noise=0.2):\n",
    "    \"\"\"Perform gradient descent and plot live updates per step.\"\"\"\n",
    "    X, y = generate_toy_dataset(noise=noise)\n",
    "    m, n = X.shape\n",
    "    X_aug = np.hstack((X, np.ones((m, 1))))  # Add bias\n",
    "    theta = np.zeros(n + 1)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    x1_range = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100)\n",
    "\n",
    "    for step in range(steps):\n",
    "        # Forward pass\n",
    "        logits = X_aug @ theta\n",
    "        probs = sigmoid(logits)\n",
    "        \n",
    "        # Compute gradient and update parameters\n",
    "        gradient = (1/m) * (X_aug.T @ (probs - y))\n",
    "        theta -= learning_rate * gradient\n",
    "        \n",
    "        # Record loss\n",
    "        loss = compute_loss(X_aug, y, theta)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Plot every few steps (or all steps if few)\n",
    "        if step % (steps // 10 + 1) == 0 or step == steps-1:\n",
    "            clear_output(wait=True)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "            # Decision Boundary\n",
    "            ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bwr\", edgecolor='k', alpha=0.7)\n",
    "            if theta[1] != 0:\n",
    "                x2_boundary = -(theta[0]*x1_range + theta[2])/theta[1]\n",
    "                ax1.plot(x1_range, x2_boundary, 'k--')\n",
    "            else:\n",
    "                ax1.axvline(x=-theta[2]/theta[0], linestyle='--', color='k')\n",
    "\n",
    "            ax1.set_title(f\"Step {step+1}/{steps}\\nDecision Boundary Movement\")\n",
    "            ax1.set_xlabel(\"Feature 1\")\n",
    "            ax1.set_ylabel(\"Feature 2\")\n",
    "            ax1.grid(True)\n",
    "\n",
    "            # Loss Curve\n",
    "            ax2.plot(np.arange(1, len(loss_history)+1), loss_history, marker='o')\n",
    "            ax2.set_title(\"Loss Over Training Steps\")\n",
    "            ax2.set_xlabel(\"Step\")\n",
    "            ax2.set_ylabel(\"Loss\")\n",
    "            ax2.grid(True)\n",
    "            ax2.set_xlim(0, steps)\n",
    "            ax2.set_ylim(0, max(loss_history)+0.1)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    gradient_descent_with_visualization,\n",
    "    learning_rate=widgets.FloatSlider(min=0.001, max=1.0, step=0.01, value=0.1),\n",
    "    steps=widgets.IntSlider(min=10, max=200, step=10, value=50),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fc0bd",
   "metadata": {},
   "source": [
    "# <a id=\"feature-scaling\"></a>‚öñÔ∏è Feature Scaling  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94048e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285f3886559741db9035087204f494f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=20.0, description='feature_scale', max=50.0, min=1.0, step=1.0), Float‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_scaling_effects(feature_scale=20, noise=0.2, learning_rate=0.1, steps=100)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Feature Scaling effects interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, feature_scale=20, noise=0.2):\n",
    "    \"\"\"Generate 2D dataset with very different feature scales.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.randn(n_samples) * feature_scale  # Feature 1 stretched\n",
    "    X2 = np.random.randn(n_samples)  # Feature 2 normal\n",
    "    X = np.vstack((X1, X2)).T\n",
    "    y = (X1 + X2 > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def standardize_features(X):\n",
    "    \"\"\"Standardize features (zero mean, unit variance).\"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0) + 1e-8  # Prevent divide by zero\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(X_aug, y, theta):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    logits = X_aug @ theta\n",
    "    probs = sigmoid(logits)\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(y * np.log(probs + epsilon) + (1 - y) * np.log(1 - probs + epsilon))\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.1, steps=100):\n",
    "    \"\"\"Simple gradient descent optimizer.\"\"\"\n",
    "    m, n = X.shape\n",
    "    X_aug = np.hstack((X, np.ones((m,1))))  # Add bias\n",
    "    theta = np.zeros(n+1)\n",
    "    \n",
    "    loss_history = []\n",
    "    for _ in range(steps):\n",
    "        logits = X_aug @ theta\n",
    "        probs = sigmoid(logits)\n",
    "        gradient = (1/m) * (X_aug.T @ (probs - y))\n",
    "        theta -= learning_rate * gradient\n",
    "        loss_history.append(compute_loss(X_aug, y, theta))\n",
    "    return loss_history\n",
    "\n",
    "def plot_scaling_effects(feature_scale=20, noise=0.2, learning_rate=0.1, steps=100):\n",
    "    \"\"\"Compare convergence with and without feature scaling.\"\"\"\n",
    "    X, y = generate_toy_dataset(feature_scale=feature_scale, noise=noise)\n",
    "    X_scaled = standardize_features(X)\n",
    "    \n",
    "    loss_raw = gradient_descent(X, y, learning_rate, steps)\n",
    "    loss_scaled = gradient_descent(X_scaled, y, learning_rate, steps)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(loss_raw, label=\"Without Scaling\", marker='o')\n",
    "    plt.plot(loss_scaled, label=\"With Scaling\", marker='x')\n",
    "    plt.title(\"Gradient Descent Convergence: Scaling vs No Scaling\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_scaling_effects,\n",
    "    feature_scale=widgets.FloatSlider(min=1.0, max=50.0, step=1.0, value=20.0),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2),\n",
    "    learning_rate=widgets.FloatSlider(min=0.001, max=1.0, step=0.01, value=0.1),\n",
    "    steps=widgets.IntSlider(min=10, max=300, step=10, value=100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2543ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä <a id=\"logistic-evaluation\"></a>**3. Evaluation & Performance**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967ce6f",
   "metadata": {},
   "source": [
    "# <a id=\"metrics\"></a>üìè Accuracy, Precision, Recall, F1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84b189c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44942cedbaa4929aa2200913bfe2ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='noise', max=1.0, step=0.05), Output()), _dom_classes‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.animate_metrics(noise=0.2)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Accuracy, Precision, Recall, F1 interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate a simple binary classification toy dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.randn(n_samples)\n",
    "    X2 = np.random.randn(n_samples)\n",
    "    y = (X1 + X2 > 0).astype(int)\n",
    "    scores = X1 + X2 + noise * np.random.randn(n_samples)\n",
    "    return scores, y\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, F1 manually.\"\"\"\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def animate_metrics(noise=0.2):\n",
    "    \"\"\"Animate threshold moving and metrics changing.\"\"\"\n",
    "    scores, y_true = generate_toy_dataset(noise=noise)\n",
    "    probs = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(thresholds, accuracies, label=\"Accuracy\", marker='o')\n",
    "    plt.plot(thresholds, precisions, label=\"Precision\", marker='x')\n",
    "    plt.plot(thresholds, recalls, label=\"Recall\", marker='s')\n",
    "    plt.plot(thresholds, f1s, label=\"F1 Score\", marker='^')\n",
    "    \n",
    "    plt.title(f\"Metrics vs Threshold\\nNoise={noise}\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    animate_metrics,\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d540b",
   "metadata": {},
   "source": [
    "# <a id=\"confusion-matrix\"></a>üßÆ Confusion Matrix  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bc9632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b999ae4364604433b786ba058b6f1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='threshold', max=1.0, step=0.01), FloatSlider(value=0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_confusion(threshold=0.5, noise=0.2)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Confusion Matrix interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate simple binary classification toy dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.randn(n_samples)\n",
    "    X2 = np.random.randn(n_samples)\n",
    "    y = (X1 + X2 > 0).astype(int)\n",
    "    scores = X1 + X2 + noise * np.random.randn(n_samples)\n",
    "    return scores, y\n",
    "\n",
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Compute TP, TN, FP, FN manually.\"\"\"\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    return np.array([[TP, FN], [FP, TN]])\n",
    "\n",
    "def plot_confusion(threshold=0.5, noise=0.2):\n",
    "    \"\"\"Plot confusion matrix based on threshold.\"\"\"\n",
    "    scores, y_true = generate_toy_dataset(noise=noise)\n",
    "    probs = 1 / (1 + np.exp(-scores))\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    cm = compute_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Actual 1\", \"Actual 0\"],\n",
    "                yticklabels=[\"Predicted 1\", \"Predicted 0\"])\n",
    "    plt.title(f\"Confusion Matrix\\nThreshold={threshold:.2f}, Noise={noise}\")\n",
    "    plt.xlabel(\"Actual Label\")\n",
    "    plt.ylabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_confusion,\n",
    "    threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4fdbc",
   "metadata": {},
   "source": [
    "# <a id=\"roc-auc\"></a>üìâ ROC Curve & AUC  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe74830c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d468f7cd9a84ed0be2031c7bb44579b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='noise', max=1.0, step=0.05), Output()), _dom_classes‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_roc_curve(noise=0.2)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing ROC Curve & AUC interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=100, noise=0.2):\n",
    "    \"\"\"Generate simple binary classification toy data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X1 = np.random.randn(n_samples)\n",
    "    X2 = np.random.randn(n_samples)\n",
    "    y = (X1 + X2 > 0).astype(int)\n",
    "    scores = X1 + X2 + noise * np.random.randn(n_samples)\n",
    "    return scores, y\n",
    "\n",
    "def plot_roc_curve(noise=0.2):\n",
    "    \"\"\"Plot ROC curve and calculate AUC.\"\"\"\n",
    "    scores, y_true = generate_toy_dataset(noise=noise)\n",
    "    probs = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', lw=2, label=\"Random Classifier\")\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title(f\"ROC Curve\\nNoise={noise}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_roc_curve,\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50701c02",
   "metadata": {},
   "source": [
    "# <a id=\"overfitting-classification\"></a>üî• Overfitting in Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f4bcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529958c94aaf459a9456c37149a3f42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='degree', max=20, min=1), FloatSlider(value=0.2, descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_overfitting(degree=1, noise=0.2, complex_data=False)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulating and visualizing Overfitting in Classification Models interactively\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_toy_dataset(n_samples=300, noise=0.2, complex=False):\n",
    "    \"\"\"Generate a simple or complex binary classification dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    if complex:\n",
    "        y = (np.sin(1.5*X[:,0]) + np.cos(1.5*X[:,1]) > 0).astype(int)\n",
    "    else:\n",
    "        y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "    X += noise * np.random.randn(*X.shape)\n",
    "    return X, y\n",
    "\n",
    "def plot_overfitting(degree=1, noise=0.2, complex_data=False):\n",
    "    \"\"\"Plot decision boundary and train vs validation loss.\"\"\"\n",
    "    X, y = generate_toy_dataset(noise=noise, complex=complex_data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create polynomial logistic regression model\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        LogisticRegression(max_iter=1000)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    val_accuracy = model.score(X_val, y_val)\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    probs = model.predict_proba(grid)[:,1].reshape(xx.shape)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,6))\n",
    "    \n",
    "    # Decision Boundary Plot\n",
    "    ax1.contourf(xx, yy, probs, 25, cmap=\"bwr\", alpha=0.6)\n",
    "    ax1.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=\"bwr\", edgecolor='k', label='Train')\n",
    "    ax1.scatter(X_val[:,0], X_val[:,1], c=y_val, cmap=\"bwr\", marker='x', label='Validation')\n",
    "    ax1.set_title(f\"Decision Boundary\\nDegree={degree}, Complex={complex_data}, Noise={noise}\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Train vs Validation Accuracy\n",
    "    ax2.bar([\"Train Accuracy\", \"Validation Accuracy\"], [train_accuracy, val_accuracy], color=[\"green\", \"blue\"])\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_title(\"Train vs Validation Accuracy\")\n",
    "    for i, v in enumerate([train_accuracy, val_accuracy]):\n",
    "        ax2.text(i, v+0.02, f\"{v:.2f}\", ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# üïπÔ∏è Interactive Widgets\n",
    "widgets.interact(\n",
    "    plot_overfitting,\n",
    "    degree=widgets.IntSlider(min=1, max=20, step=1, value=1),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.2),\n",
    "    complex_data=widgets.Checkbox(value=False, description=\"Complex Data?\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
