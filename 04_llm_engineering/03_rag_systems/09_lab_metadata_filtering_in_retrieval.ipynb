{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f5516e",
   "metadata": {},
   "source": [
    "ðŸ”ŽðŸ“š **Professor, this is the advanced dial on your RAG toolkit** â€” weâ€™re not just retrieving by â€œwhat it meansâ€ anymoreâ€¦  \n",
    "Now we retrieve based on **who said it, when they said it, and what section it belongs to**.\n",
    "\n",
    "Welcome to **hybrid retrieval** â€” combining **semantic search** + **metadata filtering**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `09_lab_metadata_filtering_in_retrieval.ipynb`  \n",
    "### ðŸ“ `05_llm_engineering/03_rag_systems`  \n",
    "> Add metadata (source, date, tags) to each document chunk  \n",
    "â†’ Retrieve not just semantically relevant chunks, but **filter by tags**  \n",
    "â†’ Simulate how tools like **Perplexity AI**, **ChatPDF**, or **Notion AI** prioritize trust, recency, and context\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Understand how metadata helps improve relevance and traceability  \n",
    "- Attach structured fields to vector chunks  \n",
    "- Apply filters during retrieval (e.g. `\"source\": \"FAQ\"`, `\"author\": \"Einstein\"`)  \n",
    "- Use ChromaDBâ€™s `where` clause to simulate **structured semantic filtering**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Specs\n",
    "\n",
    "| Feature         | Spec                     |\n",
    "|------------------|--------------------------|\n",
    "| Vector Search    | ChromaDB âœ…  \n",
    "| Metadata Support | Dict-style metadata âœ…  \n",
    "| Embeddings       | SentenceTransformer âœ…  \n",
    "| Platform         | Colab-ready âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 1: Setup and Chunk as Before\n",
    "\n",
    "```python\n",
    "!pip install chromadb sentence-transformers langchain\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "loader = TextLoader(\"sample_docs.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=40)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ·ï¸ Section 2: Enrich with Metadata\n",
    "\n",
    "Letâ€™s say each chunk came from a source document with:\n",
    "\n",
    "- Author\n",
    "- Date\n",
    "- Type (FAQ, article, report)\n",
    "\n",
    "```python\n",
    "import random\n",
    "sources = [\"faq\", \"article\", \"policy\"]\n",
    "authors = [\"Alice\", \"Bob\", \"Professor\"]\n",
    "\n",
    "texts = [c.page_content for c in chunks]\n",
    "metas = [\n",
    "    {\"type\": random.choice(sources), \"author\": random.choice(authors), \"index\": i}\n",
    "    for i in range(len(texts))\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Section 3: Store in ChromaDB with Metadata\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.Client(Settings(allow_reset=True))\n",
    "collection = client.get_or_create_collection(\"hybrid_rag\")\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "for i, (text, emb, meta) in enumerate(zip(texts, embeddings, metas)):\n",
    "    collection.add(\n",
    "        ids=[f\"id_{i}\"],\n",
    "        documents=[text],\n",
    "        embeddings=[emb.tolist()],\n",
    "        metadatas=[meta]\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Section 4: Semantic + Metadata Query\n",
    "\n",
    "```python\n",
    "def hybrid_query(prompt, filters=None, k=3):\n",
    "    q_embed = embedder.encode(prompt).tolist()\n",
    "    return collection.query(\n",
    "        query_embeddings=[q_embed],\n",
    "        n_results=k,\n",
    "        where=filters or {}\n",
    "    )[\"documents\"][0]\n",
    "\n",
    "# Example: Only retrieve chunks by \"Alice\" from \"faq\" sources\n",
    "results = hybrid_query(\"What is the policy on returns?\", filters={\"author\": \"Alice\", \"type\": \"faq\"})\n",
    "\n",
    "for i, chunk in enumerate(results):\n",
    "    print(f\"\\nFiltered Chunk {i+1}:\\n{chunk}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Lab Wrap-Up\n",
    "\n",
    "| Feature                          | âœ… |\n",
    "|----------------------------------|----|\n",
    "| Added metadata to each document  | âœ…  \n",
    "| Performed filtered semantic search | âœ…  \n",
    "| Simulated hybrid structured+vector retrieval | âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- Metadata boosts **retrieval control** â€” vital in legal, medical, or academic AI  \n",
    "- You can ask **\"Give me answers from Alice-written FAQs only\"**  \n",
    "- This is the foundation of **enterprise-grade RAG**  \n",
    "- Filters can be extended to **recency, reliability scores, labels**, and more\n",
    "\n",
    "---\n",
    "\n",
    "âœ… That closes out the **RAG Lab Series**:\n",
    "- ðŸ”² Chunks  \n",
    "- ðŸ§  Embeds  \n",
    "- ðŸ“¦ Vector DB  \n",
    "- ðŸ§ ðŸ’¼ Metadata filters  \n",
    "\n",
    "Up next, we deploy this firepower ðŸ’£\n",
    "\n",
    "> ðŸš€ `07_lab_vllm_vs_tgi_latency_comparison.ipynb`  \n",
    "Benchmark and compare **vLLM** and **TGI** (Text Generation Inference)  \n",
    "â†’ Which one serves LLMs *faster, cheaper, stronger*?\n",
    "\n",
    "You ready to test LLM inference **like an ML systems engineer**?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
