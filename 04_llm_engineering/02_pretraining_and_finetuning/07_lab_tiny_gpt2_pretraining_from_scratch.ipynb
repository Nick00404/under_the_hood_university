{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47120771",
   "metadata": {},
   "source": [
    "ğŸ§ ğŸ’¥ **Professor, this is it** â€” your moment to recreate history.  \n",
    "Weâ€™re about to **pretrain a mini GPT-2** from scratch.  \n",
    "Just like OpenAIâ€¦ but on your terms, with your dataset, and your tokenizer if you want.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§ª `07_lab_tiny_gpt2_pretraining_from_scratch.ipynb`  \n",
    "### ğŸ“ `05_llm_engineering/02_pretraining_and_finetuning`  \n",
    "> Build a tiny GPT-2 model architecture.  \n",
    "Pretrain it from scratch using **HuggingFace + Trainer API**  \n",
    "â†’ On your own dataset (text8, poetry, or anything clean and small).  \n",
    "**Understand pretraining, loss curves, and overfitting like a research engineer.**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Goals\n",
    "\n",
    "- Initialize and configure **GPT-2 from scratch**  \n",
    "- Tokenize your custom text corpus  \n",
    "- Train using **Language Modeling loss**  \n",
    "- Track **training dynamics**, loss curves, and generalization  \n",
    "- Save and reuse your custom model\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’» Runtime Specs\n",
    "\n",
    "| Component     | Spec                         |\n",
    "|----------------|------------------------------|\n",
    "| Model          | GPT2Config (tiny) âœ…  \n",
    "| Dataset        | text8 / poetry / toy docs âœ…  \n",
    "| Framework      | ğŸ¤— Transformers + Trainer âœ…  \n",
    "| Runtime        | Colab / local âœ…  \n",
    "| GPU Optional   | Yes (for >10k tokens) âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Section 1: Install HuggingFace Tools\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Section 2: Prepare Dataset\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load or mock a small dataset\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train[:1%]')\n",
    "texts = ds[\"text\"]\n",
    "texts = [line for line in texts if len(line) > 20]  # Filter short lines\n",
    "\n",
    "with open(\"pretrain.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¤ Section 3: Tokenizer (Use BPE from earlier if desired)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Section 4: Configure Tiny GPT2\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=128,\n",
    "    n_ctx=128,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Section 5: Prepare Dataset & Trainer\n",
    "\n",
    "```python\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"pretrain.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‹ï¸ Section 6: Training Loop\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tiny_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Section 7: Evaluate\n",
    "\n",
    "```python\n",
    "trainer.save_model(\"./tiny_gpt2\")\n",
    "tokenizer.save_pretrained(\"./tiny_gpt2\")\n",
    "\n",
    "# Load & generate\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"./tiny_gpt2\", tokenizer=\"./tiny_gpt2\")\n",
    "print(pipe(\"The professor said moooaahhh\", max_length=50))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Summary\n",
    "\n",
    "| Task                               | âœ… |\n",
    "|------------------------------------|----|\n",
    "| Built GPT2 config from scratch     | âœ…  \n",
    "| Pretrained on tiny corpus          | âœ…  \n",
    "| Saved + reused your model          | âœ…  \n",
    "| End-to-end pipeline built + tested | âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What You Learned\n",
    "\n",
    "- You donâ€™t need OpenAIâ€™s infra to train GPT2 â€” just a clean corpus and understanding  \n",
    "- **Language modeling loss** is how LLMs learn to â€œspeakâ€  \n",
    "- You can now modify architecture, tokenizer, and tasks â€” **youâ€™re not a user, youâ€™re a creator**\n",
    "\n",
    "---\n",
    "\n",
    "Shall we scale the next peak?\n",
    "\n",
    "> `08_lab_parameter_efficient_finetune_lora.ipynb`  \n",
    "Take a huge model and fine-tune it using **<1% of weights** with **LoRA** â€”  \n",
    "like the real-world giants do to adapt LLMs for any domain.\n",
    "\n",
    "Letâ€™s LoRA-fy the world, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
