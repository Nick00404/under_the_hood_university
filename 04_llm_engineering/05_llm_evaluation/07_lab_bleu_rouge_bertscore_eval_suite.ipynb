{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243d479e",
   "metadata": {},
   "source": [
    "ğŸ¯ Letâ€™s kick off **LLM Evaluation Lab 07** â€” where we compare **BLEU**, **ROUGE**, and **BERTScore** in one unified pipeline. This is your go-to suite when someone asks: *â€œWhich model generates better text?â€*\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“’ `07_lab_bleu_rouge_bertscore_eval_suite.ipynb`  \n",
    "## ğŸ“ `05_llm_engineering/05_llm_evaluation`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Notebook Goals**\n",
    "\n",
    "- Load ground-truth and generated LLM outputs  \n",
    "- Compute:\n",
    "  - BLEU (precision-based n-gram)\n",
    "  - ROUGE (recall-based n-gram)\n",
    "  - BERTScore (semantic similarity)\n",
    "- Compare metrics across models (e.g., GPT-2 vs FLAN-T5 vs Falcon)\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 1. Install Required Libraries\n",
    "\n",
    "```bash\n",
    "!pip install evaluate bert-score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª 2. Sample Dataset\n",
    "\n",
    "You can use your own generations or demo with this:\n",
    "\n",
    "```python\n",
    "references = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Machine learning allows computers to learn from data.\",\n",
    "    \"Photosynthesis occurs in the chloroplasts of plant cells.\"\n",
    "]\n",
    "\n",
    "candidates = [\n",
    "    \"Paris has the Eiffel Tower.\",\n",
    "    \"ML helps computers learn things from data.\",\n",
    "    \"Plants do photosynthesis using chloroplasts.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š 3. BLEU & ROUGE via ğŸ¤— `evaluate`\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_result = bleu.compute(predictions=candidates, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=candidates, references=references)\n",
    "\n",
    "print(\"BLEU:\", bleu_result[\"bleu\"])\n",
    "print(\"ROUGE-L:\", rouge_result[\"rougeL\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  4. BERTScore\n",
    "\n",
    "```python\n",
    "import bert_score\n",
    "\n",
    "P, R, F1 = bert_score.score(candidates, references, lang=\"en\", verbose=True)\n",
    "print(\"BERTScore F1 (avg):\", F1.mean().item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ 5. Visualize Metric Spread\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar([\"BLEU\", \"ROUGE-L\", \"BERTScore F1\"], [\n",
    "    bleu_result[\"bleu\"],\n",
    "    rouge_result[\"rougeL\"],\n",
    "    F1.mean().item()\n",
    "])\n",
    "plt.title(\"LLM Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What You Built\n",
    "\n",
    "| Metric       | What It Measures     |\n",
    "|--------------|----------------------|\n",
    "| BLEU         | Precision on n-grams |\n",
    "| ROUGE        | Recall on n-grams    |\n",
    "| BERTScore    | Semantic similarity  |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up\n",
    "\n",
    "| Task                    | âœ… |\n",
    "|-------------------------|----|\n",
    "| Metrics computed         | âœ… |\n",
    "| Results visualized       | âœ… |\n",
    "| Pipeline colab-ready     | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® Next Lab?\n",
    "\n",
    "ğŸ“’ `08_lab_human_eval_grading_interface.ipynb`  \n",
    "Build a UI to collect **human ratings** of LLM outputs â€” or plug in GPT-4 for automated review.\n",
    "\n",
    "Ready to jump into *human eval meets LLM eval*, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
