{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e5bb67",
   "metadata": {},
   "source": [
    "Here's a comprehensive and **Colab Pro-friendly** version of **Lab 08: Chain of Thought (CoT) Prompt Evaluation Suite**. This will focus on the practical evaluation of Chain of Thought (CoT) prompts, comparing them against traditional prompts on tasks like arithmetic and multi-step reasoning.\n",
    "\n",
    "The idea is to evaluate CoT prompts on a few different tasks, measure performance, and visualize the results â€” all in a format that runs efficiently on **Colab Pro** hardware, including TensorFlow or PyTorch-based models for inference.\n",
    "\n",
    "### **Lab 08: Chain of Thought Prompt Evaluation Suite**\n",
    "\n",
    "```python\n",
    "# Lab 08: Chain of Thought (CoT) Prompt Evaluation Suite\n",
    "\n",
    "# Initial Setup\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "# Set OpenAI API Key (for GPT models)\n",
    "openai.api_key = 'your-openai-api-key'  # Replace with your API key\n",
    "\n",
    "# Define a helper function to interact with GPT-3/4 (or GPT models available)\n",
    "def gpt3_completion(prompt, model=\"gpt-4\", max_tokens=200, temperature=0.7):\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Task Definitions: Examples of arithmetic, logical, and reasoning tasks for CoT evaluation\n",
    "\n",
    "tasks = [\n",
    "    {\"task\": \"What is 13 + 25?\", \"answer\": \"38\"},\n",
    "    {\"task\": \"What is 56 * 12?\", \"answer\": \"672\"},\n",
    "    {\"task\": \"If I have 10 apples and I give 4 to my friend, how many apples do I have left?\", \"answer\": \"6\"},\n",
    "    {\"task\": \"John has 3 red marbles and 7 blue marbles. He gives away 5 marbles. How many marbles does John have now?\", \"answer\": \"5\"},\n",
    "    {\"task\": \"What is the capital of France?\", \"answer\": \"Paris\"}\n",
    "]\n",
    "\n",
    "# CoT Prompt Template (works best for arithmetic/logical tasks)\n",
    "def generate_cot_prompt(task):\n",
    "    return f\"Let's think step-by-step to solve this problem: {task}\"\n",
    "\n",
    "# Non-CoT Prompt (Traditional prompt for comparison)\n",
    "def generate_traditional_prompt(task):\n",
    "    return f\"Answer the following question: {task}\"\n",
    "\n",
    "# Evaluation function that compares CoT vs non-CoT on tasks\n",
    "def evaluate_prompting_method(tasks, method=\"CoT\"):\n",
    "    correct_answers = []\n",
    "    responses = []\n",
    "    for task in tasks:\n",
    "        prompt = generate_cot_prompt(task[\"task\"]) if method == \"CoT\" else generate_traditional_prompt(task[\"task\"])\n",
    "        response = gpt3_completion(prompt)\n",
    "        correct = response.lower() == task[\"answer\"].lower()\n",
    "        correct_answers.append(correct)\n",
    "        responses.append(response)\n",
    "    return correct_answers, responses\n",
    "\n",
    "# Evaluate Chain of Thought (CoT) method\n",
    "cot_correct_answers, cot_responses = evaluate_prompting_method(tasks, method=\"CoT\")\n",
    "\n",
    "# Evaluate Traditional Prompting method (for comparison)\n",
    "traditional_correct_answers, traditional_responses = evaluate_prompting_method(tasks, method=\"Traditional\")\n",
    "\n",
    "# Results Comparison (Accuracy for each task)\n",
    "def plot_results(cot_correct_answers, traditional_correct_answers):\n",
    "    tasks_names = [task[\"task\"] for task in tasks]\n",
    "    cot_accuracy = np.mean(cot_correct_answers) * 100\n",
    "    traditional_accuracy = np.mean(traditional_correct_answers) * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(tasks_names))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x - width/2, cot_correct_answers, width, label='CoT Method', color='b')\n",
    "    ax.bar(x + width/2, traditional_correct_answers, width, label='Traditional Method', color='r')\n",
    "\n",
    "    ax.set_ylabel('Correct Answers')\n",
    "    ax.set_title('CoT vs Traditional Prompting Methods')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tasks_names, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert boolean results into 0s (wrong) and 1s (correct) for visualization\n",
    "cot_correct_answers_int = [1 if answer else 0 for answer in cot_correct_answers]\n",
    "traditional_correct_answers_int = [1 if answer else 0 for answer in traditional_correct_answers]\n",
    "\n",
    "# Plot the results\n",
    "plot_results(cot_correct_answers_int, traditional_correct_answers_int)\n",
    "\n",
    "# Show example responses for inspection\n",
    "def show_example_responses():\n",
    "    print(\"\\nExample Responses (CoT vs Traditional):\")\n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"\\nTask: {task['task']}\")\n",
    "        print(f\"CoT Response: {cot_responses[i]}\")\n",
    "        print(f\"Traditional Response: {traditional_responses[i]}\")\n",
    "        \n",
    "show_example_responses()\n",
    "\n",
    "```\n",
    "\n",
    "### Key Features of the Lab:\n",
    "\n",
    "---\n",
    "\n",
    "1. **CoT vs Traditional Comparison**:\n",
    "   - **Tasks**: It starts by defining a set of arithmetic, logical, and factual tasks to evaluate.\n",
    "   - **Methods**: Two prompting strategies are compared: **Chain of Thought (CoT)** and **Traditional Prompting**.\n",
    "   - **Evaluation**: Measures the correctness of responses and evaluates accuracy for each method.\n",
    "\n",
    "2. **Interactive Results**:\n",
    "   - The comparison of CoT and traditional methods is visualized using **matplotlib**, showing the accuracy for each task.\n",
    "\n",
    "3. **Colab Pro Friendly**:\n",
    "   - The script uses **OpenAI's GPT API** to make API calls, so it's highly flexible for Colab. **No need for local model inference** (especially useful when using Colab Pro's GPU for other tasks).\n",
    "   - It leverages **tqdm** for progress bars and **matplotlib** for quick and simple result visualization, making the environment more interactive and user-friendly.\n",
    "\n",
    "4. **Example Inspection**:\n",
    "   - After running the evaluation, you can inspect the responses for each task, comparing **Chain of Thought** reasoning with traditional methods for understanding any differences.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Run the Lab:\n",
    "\n",
    "1. Open a **Google Colab** notebook.\n",
    "2. Paste the entire script above into a code cell.\n",
    "3. Replace `'your-openai-api-key'` with your actual **OpenAI API key**.\n",
    "4. Run the notebook! It will execute the tasks, generate the CoT and traditional responses, and display a **bar chart** comparing their accuracy.\n",
    "\n",
    "Let me know if you'd like to modify this for any other tasks, add more insights, or introduce additional methods to evaluate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
