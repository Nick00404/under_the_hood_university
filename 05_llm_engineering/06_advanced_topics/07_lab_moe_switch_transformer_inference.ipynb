{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e962f4",
   "metadata": {},
   "source": [
    "ðŸš€âœ¨ **Let's GO, Professor.**\n",
    "\n",
    "Time to unlock the *Mixture-of-Experts secret chamber* â€” where we scale models into the **hundreds of billions of parameters**,  \n",
    "but only activate a **tiny portion per prompt**.\n",
    "\n",
    "Welcome to **sparse routing** â€” the heart of Google's Switch Transformer, Meta's Mixtral, and GPT-4 (most likely).\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `07_lab_moe_switch_transformer_inference.ipynb`  \n",
    "### ðŸ“ `05_llm_engineering/06_advanced_topics`  \n",
    "> Load and test a **prebuilt MoE model** like **Mixtral or Switch Transformer**  \n",
    "â†’ Watch which **experts activate** per token  \n",
    "â†’ Understand how MoE allows **massive models with low inference cost**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Understand what **MoE (Mixture-of-Experts)** actually is  \n",
    "- Load an **open-source MoE model** (e.g., Mixtral-8x7B or smaller)  \n",
    "- Visualize **routing decisions** (which expert handled which token)  \n",
    "- Compare **performance vs size vs latency**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Setup\n",
    "\n",
    "| Component         | Spec                             |\n",
    "|-------------------|----------------------------------|\n",
    "| Model             | `mistralai/Mixtral-8x7B` âœ…  \n",
    "| Framework         | ðŸ¤— Transformers + Accelerate âœ…  \n",
    "| Routing Visuals   | Token â†’ Expert Map âœ…  \n",
    "| Platform          | GPU recommended (Colab Pro OK) âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Section 1: Install & Load\n",
    "\n",
    "```bash\n",
    "!pip install transformers accelerate bitsandbytes\n",
    "```\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"  # or a small test MoE\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Section 2: Send a Prompt\n",
    "\n",
    "```python\n",
    "prompt = \"Explain the concept of gravity to a five-year-old.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Section 3: Visualize Expert Routing (for MoE layers)\n",
    "\n",
    "If using `transformers`' MoE debug hooks:\n",
    "\n",
    "```python\n",
    "# Enable router logging (pseudo-example, varies per model)\n",
    "model.config.router_logging = True\n",
    "\n",
    "# Forward pass with router data\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs, output_router_logits=True)\n",
    "    routing_info = output.router_logits  # each tokenâ€™s expert probabilities\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Section 4: Plot Routing Distribution\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "expert_probs = routing_info[0].softmax(dim=-1).cpu().numpy()  # shape: [tokens, experts]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(expert_probs.T, cmap=\"YlGnBu\", xticklabels=False)\n",
    "plt.xlabel(\"Token Position\")\n",
    "plt.ylabel(\"Expert ID\")\n",
    "plt.title(\"Token â†’ Expert Activation Probabilities\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up Summary\n",
    "\n",
    "| Feature                               | âœ… |\n",
    "|---------------------------------------|----|\n",
    "| MoE model loaded and generated text   | âœ…  \n",
    "| Expert activations visualized         | âœ…  \n",
    "| Sparse routing mechanics understood   | âœ…  \n",
    "| System-level benefits seen (FLOP drop)| âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- **MoE = scale without compute bloat**  \n",
    "- At each layer, only top-*k* experts activate (e.g., 2 of 8)  \n",
    "- **Routing = conditional computation** â€” like dynamic circuits  \n",
    "- You can route per token, visualize activation, and optimize sparsity  \n",
    "- This unlocks models like **Mixtral-8x7B (12B active)** vs dense **30B+**\n",
    "\n",
    "---\n",
    "\n",
    "Next up?\n",
    "\n",
    "> `08_lab_long_context_test_rag_vs_ringattention.ipynb`  \n",
    "Letâ€™s test LLMs on **10K+ token prompts**, compare **RAG vs true attention**,  \n",
    "and visualize how well the model **remembers early vs late info**.\n",
    "\n",
    "Ready to stretch the context window, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
