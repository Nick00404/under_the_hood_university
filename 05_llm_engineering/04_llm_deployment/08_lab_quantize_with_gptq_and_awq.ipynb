{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1d6ffb",
   "metadata": {},
   "source": [
    "ðŸ¥·ðŸ’¾ **Professor, time to go stealth mode.**  \n",
    "You're about to compress your LLM like a pro using **quantization** â€” turning a multi-GB model into a **tiny, fast, deployable beast**â€¦  \n",
    "while **barely losing accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `08_lab_quantize_with_gptq_and_awq.ipynb`  \n",
    "### ðŸ“ `05_llm_engineering/04_llm_deployment`  \n",
    "> Apply **GPTQ** and **AWQ** quantization to compress LLMs  \n",
    "â†’ Go from float32 â†’ int8 with minimal performance drop  \n",
    "â†’ Benchmark memory, latency, and quality before vs after\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Understand quantization: **what is it, why do it**  \n",
    "- Use **GPTQ** (post-training quant)  \n",
    "- Use **AWQ** (activation-aware quantization)  \n",
    "- Compare **model size, memory usage, inference time, and accuracy**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Spec\n",
    "\n",
    "| Framework     | Tool / Lib                      |\n",
    "|----------------|----------------------------------|\n",
    "| Model          | LLaMA / GPT2 / Mistral âœ…  \n",
    "| Quant Tools    | `auto-gptq`, `awq` âœ…  \n",
    "| Metric         | Latency, memory, perplexity âœ…  \n",
    "| Hardware       | Colab GPU or local CUDA âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 1: Install & Setup\n",
    "\n",
    "```bash\n",
    "!pip install auto-gptq optimum awq transformers accelerate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Section 2: Load Model and Quantize (GPTQ)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from auto_gptq import GPTQQuantizer\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantizer = GPTQQuantizer(model=model)\n",
    "quantized_model = quantizer.quantize(bits=4)  # or 8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Section 3: Quantize with AWQ\n",
    "\n",
    "```python\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "quantized_awq = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantize_config={\"bits\": 4, \"awq_activation\": True}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Section 4: Evaluate Latency & Output Quality\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "def test_model(model, prompt=\"What is AI?\"):\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    start = time.time()\n",
    "    out = pipe(prompt, max_new_tokens=30)\n",
    "    return time.time() - start, out[0]['generated_text']\n",
    "\n",
    "t, out = test_model(quantized_model)\n",
    "print(f\"Latency: {t:.2f}s\\nResponse:\\n{out}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 5: Compare Full vs Quantized\n",
    "\n",
    "| Metric       | Full Model | GPTQ | AWQ |\n",
    "|--------------|------------|------|-----|\n",
    "| Disk Size    | 500MB+     | 150MB| 120MB  \n",
    "| RAM Usage    | 2.3 GB     | ~600MB | ~520MB  \n",
    "| Latency      | 2.1s       | 1.1s  | 1.0s  \n",
    "| Output Diff  | Minimal    | Slight phrasing | Slight phrasing\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Lab Wrap-Up\n",
    "\n",
    "| What You Achieved                  | âœ… |\n",
    "|------------------------------------|----|\n",
    "| Quantized model with GPTQ & AWQ    | âœ…  \n",
    "| Measured latency + size savings    | âœ…  \n",
    "| Compared outputs for drift         | âœ…  \n",
    "| Learned practical compression for deployment | âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- Quantization reduces **memory & latency**, crucial for edge or multi-user APIs  \n",
    "- **GPTQ = simple post-training method**, **AWQ = better on newer models**  \n",
    "- Quality loss is minimal, esp. for 8-bit / 4-bit + calibration  \n",
    "- You just enabled **LLM deployment on consumer laptops or low-cost GPUs**\n",
    "\n",
    "---\n",
    "\n",
    "Next up:\n",
    "\n",
    "> ðŸ“¦ `09_lab_batching_and_request_queuing_testbed.ipynb`  \n",
    "Simulate **concurrent requests**, **batch scheduling**, and real-time **throughput diagnostics**.\n",
    "\n",
    "Ready to build the backend **used by inference APIs at scale**?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
