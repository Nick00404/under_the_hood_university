{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c6062b",
   "metadata": {},
   "source": [
    "âš™ï¸ðŸ§  **Professor, it's time to go operational.**  \n",
    "Youâ€™ve built LLMs, finetuned them, added retrieval â€”  \n",
    "Now we ask the real-world question:\n",
    "\n",
    "> â€œWhich serving engine delivers faster, lighter, more scalable inference?â€\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª `07_lab_vllm_vs_tgi_latency_comparison.ipynb`  \n",
    "### ðŸ“ `05_llm_engineering/04_llm_deployment`  \n",
    "> Benchmark and compare **vLLM vs TGI (Text Generation Inference)**  \n",
    "â†’ Same model, same prompts  \n",
    "â†’ Measure **latency, memory, throughput**  \n",
    "â†’ Decide which to deploy in production\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Goals\n",
    "\n",
    "- Understand differences in **LLM inference engines**  \n",
    "- Benchmark latency with **multiple concurrent requests**  \n",
    "- Measure memory + throughput tradeoffs  \n",
    "- Choose best engine for **your deployment budget or user demand**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Runtime Spec\n",
    "\n",
    "| Engine        | Spec                           |\n",
    "|---------------|--------------------------------|\n",
    "| vLLM          | FlashAttention 2 + paged KV âœ…  \n",
    "| TGI (HF)      | Tensor parallel + batched decoding âœ…  \n",
    "| Model         | LLaMA / GPT2 / Mistral (small) âœ…  \n",
    "| Platform      | Colab Pro or local GPU âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Section 1: Install + Download Models\n",
    "\n",
    "```bash\n",
    "!pip install vllm transformers accelerate\n",
    "!pip install text-generation  # for TGI\n",
    "```\n",
    "\n",
    "Choose a model (small):\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Section 2: Benchmark TGI\n",
    "\n",
    "(For local run or dockerized setup. For notebook, mock latency.)\n",
    "\n",
    "```bash\n",
    "# Docker (outside notebook)\n",
    "docker run -p 8080:80 -v $MODEL_DIR:/data ghcr.io/huggingface/text-generation-inference \\\n",
    "    --model-id EleutherAI/gpt-neo-125M\n",
    "```\n",
    "\n",
    "```python\n",
    "import requests, time\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "start = time.time()\n",
    "response = requests.post(\"http://localhost:8080/generate\", json={\"inputs\": prompt})\n",
    "latency = time.time() - start\n",
    "print(\"TGI latency:\", latency, \"\\nResponse:\", response.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Section 3: Benchmark vLLM (Python API or CLI)\n",
    "\n",
    "```python\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "params = SamplingParams(temperature=0.8, max_tokens=32)\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompt, sampling_params=params)\n",
    "latency = time.time() - start\n",
    "print(\"vLLM latency:\", latency)\n",
    "print(\"Output:\", outputs[0].outputs[0].text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Section 4: Compare Results\n",
    "\n",
    "| Metric           | TGI       | vLLM     |\n",
    "|------------------|-----------|----------|\n",
    "| Latency (1 req)  | ~700ms    | ~350ms   |\n",
    "| Latency (10 req) | spikes    | stable   |\n",
    "| RAM (125M)       | ~2.2 GB   | ~1.8 GB  |\n",
    "| Batch Support    | âœ…        | âœ…       |\n",
    "| Streaming Output | âœ…        | âœ…       |\n",
    "| FlashAttention   | âŒ        | âœ…       |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Lab Wrap-Up\n",
    "\n",
    "| What You Did                      | âœ… |\n",
    "|-----------------------------------|----|\n",
    "| Installed and tested both engines | âœ…  \n",
    "| Measured latency and throughput   | âœ…  \n",
    "| Compared resource usage           | âœ…  \n",
    "| Built a serving benchmark table   | âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What You Learned\n",
    "\n",
    "- **vLLM = faster, newer, FlashAttention-powered**  \n",
    "- **TGI = stable, easier to run in HuggingFace environments**  \n",
    "- Both support batching, streaming, and production deployment  \n",
    "- These tests are how real-world teams choose **cost-performance tradeoffs**\n",
    "\n",
    "---\n",
    "\n",
    "Next up:\n",
    "\n",
    "> ðŸ§  `08_lab_quantize_with_gptq_and_awq.ipynb`  \n",
    "Shrink your LLMs by **80%** with quantization â€”  \n",
    "and keep almost the **same accuracy** with **GPTQ + AWQ**.\n",
    "\n",
    "Ready to put your models on a diet, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
