{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f2fd04",
   "metadata": {},
   "source": [
    "ğŸ§  Letâ€™s move into **Lab 08** â€” this oneâ€™s about **human evaluation**, the gold standard in LLM testing. Weâ€™ll build a clean UI for **rating generations**, plug in **GPT-4 as a reviewer**, and even prep for **crowdsourcing** if needed.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“’ `08_lab_human_eval_grading_interface.ipynb`  \n",
    "## ğŸ“ `05_llm_engineering/05_llm_evaluation`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Notebook Goals**\n",
    "\n",
    "- Create a **streamlit-style rating UI**\n",
    "- Let humans grade:\n",
    "  - Fluency âœ…\n",
    "  - Factual accuracy âœ…\n",
    "  - Relevance âœ…\n",
    "- Bonus: Plug in GPT-4 to simulate **automated human eval**\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 1. Sample Dataset: References vs Generations\n",
    "\n",
    "```python\n",
    "samples = [\n",
    "    {\n",
    "        \"prompt\": \"What is photosynthesis?\",\n",
    "        \"reference\": \"Photosynthesis is the process by which green plants use sunlight to synthesize food.\",\n",
    "        \"generation\": \"Photosynthesis is how plants turn light into energy for survival.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Where is the Eiffel Tower?\",\n",
    "        \"reference\": \"The Eiffel Tower is in Paris, France.\",\n",
    "        \"generation\": \"The Eiffel Tower is located in Berlin.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª 2. Build Grading Form (Colab Friendly)\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import widgets\n",
    "\n",
    "def create_grading_interface(sample):\n",
    "    display(Markdown(f\"### ğŸ§ª Prompt:\\n{sample['prompt']}\"))\n",
    "    display(Markdown(f\"**ğŸ“– Reference:** {sample['reference']}\"))\n",
    "    display(Markdown(f\"**ğŸ¤– LLM Output:** {sample['generation']}\"))\n",
    "\n",
    "    fluency = widgets.IntSlider(description=\"Fluency\", min=1, max=5, value=3)\n",
    "    factual = widgets.IntSlider(description=\"Factuality\", min=1, max=5, value=3)\n",
    "    relevance = widgets.IntSlider(description=\"Relevance\", min=1, max=5, value=3)\n",
    "    submit = widgets.Button(description=\"Submit Score\")\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_submit_clicked(_):\n",
    "        with output:\n",
    "            display(Markdown(\n",
    "                f\"âœ… **Scores Submitted:** Fluency: {fluency.value}, \"\n",
    "                f\"Factuality: {factual.value}, Relevance: {relevance.value}\"\n",
    "            ))\n",
    "\n",
    "    submit.on_click(on_submit_clicked)\n",
    "\n",
    "    display(fluency, factual, relevance, submit, output)\n",
    "\n",
    "for sample in samples:\n",
    "    create_grading_interface(sample)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  3. Simulate GPT-4 as Human Evaluator\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def gpt_evaluator(prompt, reference, generation):\n",
    "    system_prompt = \"You are a helpful evaluator who rates LLM outputs. Rate the fluency, factuality, and relevance on a scale from 1 to 5.\"\n",
    "    user_input = f\"\"\"\n",
    "    Prompt: {prompt}\n",
    "    Reference: {reference}\n",
    "    Output: {generation}\n",
    "    Rate each dimension (fluency, factuality, relevance) from 1 to 5.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Example\n",
    "#gpt_evaluator(**samples[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What You Built\n",
    "\n",
    "| Feature         | Role |\n",
    "|------------------|------|\n",
    "| Rating UI        | Collect human scores |\n",
    "| GPT-4 Auto Rater | Optional evaluator (for cost/time saving) |\n",
    "| Manual + Automated | âœ… Combined pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Wrap-Up\n",
    "\n",
    "| Task                          | âœ… |\n",
    "|-------------------------------|----|\n",
    "| Built LLM rating interface     | âœ… |\n",
    "| Collected fluency/factuality  | âœ… |\n",
    "| Plugged in GPT-4 eval optional| âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® Next Lab\n",
    "\n",
    "ğŸ“’ `09_lab_bias_and_toxicity_metrics_demo.ipynb`  \n",
    "Letâ€™s analyze **bias and toxicity** in LLM outputs â€” using open-source tools to detect stereotypes, slurs, or harmful completions.\n",
    "\n",
    "Ready to go ethical, Professor?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
